[
  {
    "metadata": {
      "title": "StarSling: Building Cursor for DevOps with Mastra",
      "publishedAt": "2025-07-01",
      "author": "Shreeda Segan",
      "summary": "How Netflix engineers Daniel Worku and Yonas Beshawred are building an AI-powered DevOps assistant using Mastra to automate the 20-30% of engineering work that happens outside the code editor.",
      "draft": false,
      "categories": ["case studies"]
    },
    "slug": "starsling",
    "content": "Daniel Worku was supporting a team of 3,000 engineers at Netflix when he realized something was broken. Despite building Netflix Console—the most-used internal tool that helped ship major features like ads, live streaming, and games in record time—engineers were still spending 20-30% of their time on tedious tasks outside their code editor. Bug fixes, deployments, performance issues, and production incidents consumed hours that could be spent building.\n\n\"We had this amazing tool for tracking services and pinging engineers, but it had no AI,\" Worku recalls. \"It was basically a manager's bookkeeping tool. The last project my team worked on was called Change Automation, but it was pre-AI—we were hand-coding templates.\"\n\nThat's when Worku reconnected with his longtime friend Yonas Beshawred, who had just sold his developer tools company StackShare to FOSSA. The two had met at a Bay Area hacker house a decade earlier, bonding over a shared love of Ethiopian culture at a party. Their friendship had grown over the years—Beshawred was even a groomsman at Worku's wedding in 2021.\n\n\"When we talked about building an internal developer portal for engineers with AI, Yonas was like, 'I'm down to do it again,'\" says Worku, who had always planned to return to startups after stints at Facebook and Netflix.\n\nNow part of Y Combinator's S25 batch, the StarSling team turned to Mastra in early 2025 to build their vision: an AI-powered system that brings the magic of Cursor to all the engineering work that happens outside the code editor.\n\n![Yonas Beshawred and Daniel Worku, co-founders of StarSling](/starslingfounders.png)\n\n<figcaption className=\"text-center text-sm text-gray-500 mt-2\">\n  Yonas Beshawred and Daniel Worku, co-founders of StarSling.\n</figcaption>\n\n\n## The Vision: Cursor for Everything Outside the IDE\n\nStarSling's mission is to revolutionize the 20-30% of engineering work that happens outside the code editor. While tools like Cursor have transformed how engineers write code, Worku describes stepping outside the IDE as \"walking back to the stone age.\"\n\n\"Let's say I'm done writing in the code editor. I need to deploy to production, optimize a slow database query, or debug an exception,\" Worku explains. \"That unplanned work—those annoying alerts that pop up—that's what we're building for.\"\n\nThe team is starting with a focused approach: a prioritized inbox that integrates GitHub, Linear, and Sentry. When an exception appears, engineers can click an \"Autofix\" button that launches an AI agent to debug the issue, find the root cause, and open a pull request—all without leaving their homepage.\n\n![StarSling Autofix feature in action](/starsling.png)\n\n<figcaption className=\"text-center text-sm text-gray-500 mt-2\">\n  StarSling&apos;s Autofix feature in action.\n</figcaption>\n\n\n\n*Screenshot: StarSling's Autofix feature in action*\n\n## Why Mastra Powers StarSling's Agent System\n\nAs StarSling began building their AI-powered DevOps assistant, they faced a critical architectural decision. They needed to create multiple specialized agents—each handling different aspects of the engineering workflow—while maintaining seamless integration across various developer tools.\n\n\"We evaluated LangGraph and considered building something custom,\" says Worku. \"But Mastra's modular agent framework and workflow engine were perfectly suited for what we needed.\"\n\nHere's how StarSling architected their solution with Mastra:\n\n**Frontend**: Like [Vetnio](https://mastra.ai/blog/vetnio), StarSling uses assistant-ui, a responsive frontend with deep Mastra integration that works seamlessly on desktop and mobile.\n\n**Agent Orchestration**: Mastra's workflow orchestrator intelligently routes tasks to specialized agents based on the type of engineering work—whether it's fixing a Sentry exception, updating a Linear task, or opening a GitHub PR.\n\n**Cross-Tool Integration**: The real power comes from Mastra enabling agents to work across multiple developer tools. \"The autofix feature requires getting data from Sentry, opening a GitHub PR, and potentially linking to a Linear task,\" Worku explains. \"Mastra makes this orchestration seamless.\"\n\n**Infrastructure**: Following their commitment to modern, scalable architecture, StarSling runs on serverless infrastructure using Vercel and Cloudflare. They're also big believers in open source—alongside Mastra, they use Better Auth for authentication and Inngest for data syncing.\n\n## Looking ahead\n\nThe StarSling team found Mastra's comprehensive documentation and responsive Slack support crucial for their rapid development. \"If you have questions, beyond getting a fast response from Abhi or Shane, you can look at the code yourself,\" notes Worku. \"Open source is a huge unlock.\"\n\nThis ease of integration let the team focus on their core mission instead of building infrastructure. While they're launching with the autofix feature, their vision extends to a comprehensive system of agents for all developer tasks outside the IDE. The team is rapidly growing; StarSling is [looking to hire their first engineers.](https://www.ycombinator.com/companies/starsling/jobs) \n\n\"Every engineer who has code in production does this type of work,\" Worku emphasizes. \"Especially when you're on-call, this 20-30% of your time becomes 60-70%. We're building Cursor for DevOps—bringing that same AI-powered experience to everything that happens after the code is written.\""
  },
  {
    "metadata": {
      "title": "Mastra Changelog 2025-06-27",
      "publishedAt": "2025-06-27",
      "author": "Shane Thomas",
      "summary": "Mastra Cloud public beta, agent network chat, memory improvements, workflow updates, and a new Mastra 101 lesson.",
      "draft": false,
      "categories": ["changelogs"]
    },
    "slug": "changelog-2025-06-27",
    "content": "Another busy week at Mastra. We're thrilled to announce the public beta of Mastra Cloud, plus some exciting new features that make building AI agents and workflows even more powerful.\n\nLet's dive into this week's updates.\n\n## Mastra Cloud\n\nWe're opening up **Mastra Cloud** to everyone—no waitlist, no invite, just head to [mastra.ai/cloud](https://mastra.ai/cloud) and start building.\n\nWith Mastra Cloud, you can:\n\n- Deploy and manage agents, workflows, MCP servers, and tools.\n- Test in playground\n- Monitor, observe, and debug your runs in real time.\n\nSign up and deploy your first agent in minutes at [mastra.ai/cloud](https://mastra.ai/cloud).\n\n\n## Agent Network\n\nYou can now chat with your agent network in playground — great for complex tasks like \"research London AND write an article about its weather\".\n\n![Agent Network Chat Interface](/agentnetworkvnext.png)\n\nWe currently have two chat modes: `loop` and `stream`.\n\nLoop lets your agent network keep iterating for a specified number of execution steps. You can tweak the max iterations directly in Playground.\n\n## Memory\n\nLast week we added per-resource semantic recall. This week, we added per-resource working memory across multiple Mastra packages.\n\nThis enables persistent user memory across multiple conversation threads. We've also included flexible scoping options.\n\n## Workflows\n\nWe added a new `bail` function to workflows. This method allows workflows to exit a step early.\n\nIt also updates the return types of `suspend()` and `bail()` to `any` so that their value can still be returned (without having to return something that looks like the step result type).\n\nWe also introduced a new async operation `createRunAsync()` that also stores a `pending` workflow state into storage. As such, the `createRun` method is deprecated and will `logger.warn()`.\n\n## Mastra 101\n\nWe added a new lesson to [Mastra 101](https://mastra.ai/course), our mcp-based course that you can take directly in your IDE (like Cursor or Windsurf).\n\n![Mastra 101 Lesson 4 - Building Powerful Workflows](/mastra101lesson4.png)\n\nThis fourth lesson teaches you how to build powerful workflows that orchestrate multiple AI agents and tools.\n\n\n\nThat's it for this week! As always, we'd love to hear your feedback and see what you're building with Mastra.\n\nHappy building! And remember: you can just deploy things. 🚀"
  },
  {
    "metadata": {
      "title": "Mastra Cloud Public Beta",
      "author": "Shane Thomas",
      "publishedAt": "2025-06-26",
      "summary": "Mastra Cloud is now in public beta — deploy, manage, and scale your AI agents and workflows.",
      "tags": "mastra-cloud",
      "categories": ["announcements"]
    },
    "slug": "mastra-cloud-public-beta",
    "content": "We're opening up **Mastra Cloud** to everyone—no waitlist, no invite, just head to [mastra.ai/cloud](https://mastra.ai/cloud) and start building.\n\n![Mastra Cloud MCP Interface](/cloudmcp.png)\n\n\n## What is Mastra Cloud?\n\nMastra Cloud is the easiest way to deploy your AI agents and workflows — no servers to manage, no infrastructure headaches. Just ship your code and go.\n\nWith Mastra Cloud, you can:\n\n- Deploy and manage agents. Build intelligent agents that execute tasks, access knowledge bases, and maintain memory persistently within threads. You can also monitor your workflows, MCP servers, and tools. \n- Test in playground. Mastra Cloud provides a UI and chat interface that allows you to interact with your agents and workflows to see how they are working in production.\n- Monitor, observe, and debug your runs in real time. See how your users interact with your agents and workflows.\n- Plug into the full Mastra ecosystem: agents, workflows, RAG, observability, and more\n\nWhether you're scaling something big or just getting started, Mastra Cloud handles infrastructure so you can focus on what matters most: building the best products possible for the AI-native future.\n\n## Try it out\n\nYou can sign up and deploy your first agent in minutes at [mastra.ai/cloud](https://mastra.ai/cloud).\n\nWe'd love to see what you build! Share your projects and feedback with us on [Discord](https://discord.gg/BTYqqHKUrf)."
  },
  {
    "metadata": {
      "title": "Mastra Changelog 2025-06-20",
      "publishedAt": "2025-06-20",
      "author": "Shane Thomas",
      "summary": "We added sleep methods to workflows, structured memory to agents, and a new Gladia STT provider.",
      "draft": false,
      "categories": ["changelogs"]
    },
    "slug": "changelog-2025-06-24",
    "content": "What a week at Mastra. Co-founders united in SF, we gave out books at YC Startup School, and the team keeps shipping…\n\nLet’s dive into this week’s updates.\n\n## Workflow Sleep Methods\n\nWe added `.sleep()` and `.sleepUntil()` methods to workflows, so you can pause execution for a specified duration or until a specific time: \n\n### `sleep`\n\n```jsx\nimport { createWorkflow, createStep } from '@mastra/core/workflows';\nimport { z } from 'zod';\n\nconst waitStep = createStep({\n  id: 'wait',\n  description: 'Waits for 5 seconds before proceeding',\n  inputSchema: z.object({}),\n  outputSchema: z.object({ done: z.boolean() }),\n  execute: async () => {\n    return { done: true };\n  },\n});\n\nconst workflow = createWorkflow({\n  id: 'sleepWorkflow',\n  inputSchema: z.object({}),\n  outputSchema: z.object({ done: z.boolean() }),\n  steps: [waitStep],\n})\n  .then(waitStep)\n  .sleep(5000)\n  .commit();\n```\n\n### `sleepUntil`\n\n```jsx\nimport { createWorkflow, createStep } from '@mastra/core/workflows';\nimport { z } from 'zod';\n\nconst stepOne = createStep({\n  id: 'step-one',\n  description: 'Return step',\n  inputSchema: z.object({}),\n  outputSchema: z.object({ step: z.string() }),\n  execute: async () => {\n    return { step: 'one' };\n  },\n});\n\nconst stepTwo = createStep({\n  id: 'step-two',\n  description: 'Return ste',\n  inputSchema: z.object({ step: z.string() }),\n  outputSchema: z.object({ success: z.boolean() }),\n  execute: async () => {\n    return { success: true };\n  },\n});\n\nconst workflow = createWorkflow({\n  id: 'sleepUntilWorkflow',\n  inputSchema: z.object({}),\n  outputSchema: z.object({ success: z.boolean() }),\n  steps: [stepOne, stepTwo],\n})\n  .then(stepOne)\n  .sleepUntil(new Date(Date.now() + 1000))\n  .then(stepTwo)\n  .commit();\n```\n\n### `waitForEvent`\n\n```jsx\nimport { createWorkflow, createStep } from '@mastra/core/workflows';\nimport { z } from 'zod';\n\nconst stepOne = createStep({\n  id: 'step-one',\n  description: 'Return step 1',\n  inputSchema: z.object({}),\n  outputSchema: z.object({ step: z.string() }),\n  execute: async () => {\n    return { step: 'one' };\n  },\n});\n\nconst stepTwo = createStep({\n  id: 'step-two',\n  description: 'Return step 2',\n  inputSchema: z.object({ step: z.string() }),\n  outputSchema: z.object({ step: z.string() }),\n  execute: async () => {\n    return { step: 'two' };\n  },\n});\n\nconst stepThree = createStep({\n  id: 'step-three',\n  description: 'Return success',\n  inputSchema: z.object({ step: z.string() }),\n  outputSchema: z.object({ success: z.boolean() }),\n  execute: async () => {\n    return { success: true };\n  },\n});\n\nconst workflow = createWorkflow({\n  id: 'waitForEventWorkflow',\n  inputSchema: z.object({}),\n  outputSchema: z.object({ success: z.boolean() }),\n  steps: [stepOne, stepTwo, stepThree],\n})\n  .then(stepOne)\n  .waitForEvent('my-event-name', stepTwo)\n  .then(stepThree)\n  .commit();\n```\n\nWhen executing the workflow the step waits for an event to be received. \n\n```jsx\nconst workflow = mastra.getWorkflow('waitForEventWorkflow')\n\nconst run = await workflow.createRun()\n\nrun.start({})\n\n// Elsewhere, you can execute sendEvent to activate the step\nsetTimeout(() => {\n    run.sendEvent('my-event-name', {\n        step: 'three',\n    })\n}, 5000)\n```\n\n## Structured Memory\n\nAgents can now store and retrieve objects / JSON in working memory, not just conversation messages. They can also store complex user characteristics and preferences: \n\n```jsx\nimport { Agent } from '@mastra/core/agent';\nimport { z } from 'zod';\n\nconst memory = new Memory({\n    options: {\n      workingMemory: {\n        enabled: true,\n        schema: z.object({ \n\t        theme: z.string(), \n\t        notifications: z.boolean().default(false),\n\t        language: z.string(),\n\t      })\n      },\n    },\n  })\n\nconst agent = new Agent({\n  name: 'MemoryAgent',\n  instructions: 'You remember user preferences and task states.',\n  memory: memory\n});\n\n// Example: Store user preferences\nawait agent.generate('I want my theme to be dark mode and en', {\n  resourceId: '123,\n  threadId: '234',\n});\n\n// Example: Retrieve user preferences\nconst prefs = await memory.getWorkingMemory({ threadId: '234', format: 'json' });\n\nconsole.log('User preferences:', prefs);\n// {\n//  theme: 'dark mode',\n//  language: 'en',\n//  notifications: false,\n// }\n```\n\n## Other Updates\n\n- **New Gladia STT provider.** Gladia is a French audio transcription API used by AI companies like 11x ([#4459](https://github.com/mastra-ai/mastra/releases#:~:text=a%20new%20feature.-,%234459,-%5BIMPORTANT%5D))\n- **Enhanced Agent Context**: We added thread metadata support to `agent.stream()` and ≈`agent.generate()` for better conversation management ([#5108](https://github.com/mastra-ai/mastra/releases#:~:text=for%20backward%20compatibility.-,%235108,-The%20type%20definition))\n- **LaTeX Document Support**: RAG now correctly chunks LaTeX documents with proper separator handling ([#4882](https://github.com/mastra-ai/mastra/releases#:~:text=in%20MDocument.chunk().-,%234882,-MCP))\n- **PostgreSQL 15 Compatibility**: We fixed memory retrieval issues with explicit query aliases ([#5182](https://github.com/mastra-ai/mastra/releases#:~:text=scope%20is%20resource.-,%235182,-Fix%20return%20value))\n- **Workflow Polling API**: We added a new API endpoint for checking workflow execution results programmatically ([#5061](https://github.com/mastra-ai/mastra/releases#:~:text=corresponding%20test%20updates.-,%235061,-Fix%20the%20final))\n- **MCP Connection Stability**: We shipped multiple improvements to streamable HTTP connections ([#5088](https://github.com/mastra-ai/mastra/releases#:~:text=over%20streamable%20HTTP.-,%235088,-Improve%20MCP%20connection), [#5089](https://github.com/mastra-ai/mastra/releases#:~:text=MCP%20connection%20handling.-,%235089,-Add%20MCP%20Bar))\n\nFind the full release log [here](https://github.com/mastra-ai/mastra/releases/tag/%40mastra%2Fcore%400.10.6)."
  },
  {
    "metadata": {
      "title": "Why PLAID Japan builds agents on their Google Cloud infrastructure with Mastra",
      "summary": "How PLAID Japan migrated from GUI-based AI tools to Mastra for better collaboration and productivity for their engineering team building on Google Cloud.",
      "author": "Sam Bhagwat",
      "publishedAt": "2025-06-15",
      "draft": false,
      "categories": ["case studies"]
    },
    "slug": "plaid-jpn-gcp-agents",
    "content": "Founded in 2011 by former Rakuten exec Kenta Kurahashi, PLAID analyzes website visitor behavior and emotions in real-time to enable personalized communications. Their software, called KARTE, tracks over 1 trillion yen in annual e-commerce transactions, and the company went public in December 2020 at a $500 million valuation.\n\n![PLAID KARTE website interface](/images/plaid-karte-website.png)\n\n\n## Stuck building workflows with GUI tools\n\nAI infrastructure became critical to PLAID's engineering roadmap because customers required increasingly sophisticated features like predictive user intent, dynamic content generation, and contextual recommendations that couldn't be built with rules-based logic.\n\nFor internal AI workflows, PLAID's engineering team had been using a combination of no-code tools Dify and n8n, which presented challenges for a team accustomed to code-first development.\n\n\"Dify would often break with updates,\" explains Kosuke Oya, a lead engineer at PLAID. \"And since it's managed through a GUI, it didn't align well with our product development flow.\"\n\nFor customer-facing products, the team faced similar issues with existing TypeScript AI frameworks. They tried using Langchain.js, but found that its Typescript type support was lacking. They used provider SDKs directly, but found that switching between LLMs wasn't straightforward.\n\n## Finding Mastra\n\nPLAID's engineering team discovered Mastra on GitHub in mid-February, when Mastra went viral on Github Trending. \"It became a hot topic internally when the GitHub star count grew rapidly,\" Oya recalls.\n\n\"It's a TypeScript framework, which is a great fit for us since most of our products are written in TypeScript,\" Oya explains. \"We don't have in-house expertise in Python, and I believe TypeScript is a better language for long-term maintenance.\"\n\nTakanori Koga, another engineer on the team, felt the same way: \"Because it's built with TypeScript, we could quickly build things like agents and integrate with MCP servers.\"\n\n## Implementation on Google Cloud infrastructure\n\nPLAID implemented within their existing Google Cloud infrastructure, running on Google Cloud Run and integrating it through their Backend-for-Frontend (BFF) layer. They have Node.js servers specifically to talk to both the frontend web app side as well as the backend infrastructure services.\n\n\"Our infrastructure is on GKE, and we've configured it to use the Mastra API Server from our BFF,\" Koga explains. \n\nThey use Mastra's agentic and MCP capabilities, as well as putting the Mastra client library in their frontend. \n\n\"Building an agent and connecting it remotely to an MCP server was easy,\" Koga says. \"We integrate with external tools using the Mastra Client—we were able to integrate with a Slack App very quickly.\"\n\n\"Being able to manage it as code makes it easy to handle, which has increased our development productivity,\" Oya reports. \n\nThe shift from GUI tools to code-first development internally has made it easier for the engineering teams to collaborate. \n\n\"Because we can understand what's happening through the code, I believe it has improved the common understanding and ease of collaboration among engineers,\" Koga explains.\n\n\"Thanks to Mastra, AI Agents have become more accessible to us because we can develop and customize them ourselves,\" Koga notes. \n\nThe playground in particular is a team favorite. \"The playground makes it easy to adjust prompts and troubleshoot with traces, which has boosted development productivity,\" Oya adds.\n\n## Building advanced agents for KARTE\n\n![PLAID KARTE platform interface](/images/plaid-karte.png)\n\n\nThe next step for PLAID is to move from internal into customer-facing agents.\n\n\"We plan to build advanced agents that can perform operations on the KARTE admin screen or conduct analysis on our behalf using tools, as well as multi-agents that can execute tasks across multiple products within KARTE,\" Oya explains.\n\nThey're also developing a new database called the Context Database. This provides the agent with behavioral data on demand, enabling it to understand website visitor behaviors and emotions to create better customer experiences.\n\n![PLAID Context Database architecture diagram](/images/plaid-context-db.png)\n\nThe team is also working on streamlining internal processes: \"We're planning to streamline our internal inquiry response process and create agents to automate routine tasks,\" Koga adds.\n\nAnd, he adds, the team is continuously monitoring Mastra for the stream of new features.\n\n\"The update speed is extremely fast, so it's best to check on it continuously so you don't miss anything,\" he laughs."
  },
  {
    "metadata": {
      "title": "Mastra Changelog 2025-06-13",
      "publishedAt": "2025-06-13",
      "summary": "Cross-thread memory recall, universal schema support, and enhanced workflow observability.",
      "author": "Shane Thomas",
      "draft": false,
      "categories": ["changelogs"]
    },
    "slug": "changelog-2025-06-13",
    "content": "Another busy week at Mastra. The team has been largely heads down, making moves on some big projects (big updates coming very soon 👀.) But that doesn't mean we've slowed down on shipping core platform improvements.\n\nLet's dig in…\n\n## Cross-Thread Memory Recall\n\nWe added cross-thread semantic recall functionality to Mastra Memory, allowing memory retrieval across different threads belonging to the same resource, via a new `scope` key that can be set to `resource`.\n\n```typescript {5-7}\nconst agent = new Agent({\n  memory: new Memory({\n    options: {\n      semanticRecall: {\n        topK: 3, // Retrieve 3 most similar messages\n        messageRange: 2, // Include 2 messages before and after each match\n        scope: 'resource', // Search across all threads for this user\n      },\n    },\n  }),\n});\n```\n\nAgents can now access memories from previous threads when working with the same user or resource. The 'resource' scope is currently supported by LibSQL, Postgres, and Upstash storage adapters.\n\n## Output Schema Support for Agents\n\nWe previously sanitized input schemas with the `@mastra/schema-compat` utility. Now, we've extended that to support output schemas as well. \n\nIn addition, we've enabled support for any Zod schema (not just Zod objects) when you stream or generate objects. For example, you could generate an array. \n\n## Other Updates\n\n- **Enhanced Workflow Observability**: Added log filtering with level-based filtering (debug, info, error) and infinite scrolling for workflow execution logs in the local playground ([#4706](https://github.com/mastra-ai/mastra/pull/4706), [#4768](https://github.com/mastra-ai/mastra/pull/4768)).\n- **Unified Developer UI Experience**: We're pulling some of the things we've done in Mastra cloud into the local playground for better consistency between Playground and Cloud UI. This includes simplified navigation structure and enhanced agent page layouts ([#4524](https://github.com/mastra-ai/mastra/pull/4524)).\n- **MCP Prompt Support**: Added support for MCP prompts to both `MCPServer` and `MCPClient` ([#4566](https://github.com/mastra-ai/mastra/pull/4566)).\n- **Storage API Pagination**: Implemented pagination support across PostgreSQL, MongoDB, and Upstash, with new paginated `getEvals` method ([#4652](https://github.com/mastra-ai/mastra/pull/4652), [#4631](https://github.com/mastra-ai/mastra/pull/4631)).\n- **LanceDB Vector Integration**: Added LanceDB as a new vector database provider option with `MastraVector` and `MastraStorage` interfaces ([#3324](https://github.com/mastra-ai/mastra/pull/3324)).\n\nFind the full release log [here](https://github.com/mastra-ai/mastra/releases/tag/%40mastra%2Fcore%400.10.4)."
  },
  {
    "metadata": {
      "title": "Why Vetnio powers their AI veterinary technician with Mastra",
      "publishedAt": "2025-06-11",
      "author": "Shreeda Segan",
      "summary": "How Vetnio uses Mastra's workflow orchestrator to build specialized veterinary AI assistants",
      "draft": false,
      "categories": ["case studies"]
    },
    "slug": "vetnio",
    "content": "Arman Karégar was taking his 12-year-old mixed breed dog to the vet in Stockholm when he had an idea. He noticed that he had to keep repeating basic medical information during the visits and wondered if any of this information was truly documented. It seemed like veterinarians spent excessive time taking notes but still managed to miss critical details about his pet. \n\nSo Karégar, a seasoned technologist, teamed up with college friend Max Xie to build Vetnio, a tool that records conversations between veterinary professionals and pet owners and generates clinical notes for the meetings. In one practice, Vetnio saved veterinarians approximately 40% of their time. \n\n![Vetnio cofounders](/vetnio.png)\n\n> From left to right: CTO Max Xie, CEO Arman Karegar, founding engineer Emil Kranzell.\n\n\nCurrently, they operate across the UK, Germany, and several other European countries, and are quickly expanding in the US. On top of building their AI clinical notetaker, they wanted to build an assistant that could help with common veterinary administrative tasks. But this involved needing to build not just one but several agents — a booking agent, a reminder agent, a clinical-info agent. Each agent also needed to maintain its own context and tools, like a calendar API and medical database.\n\n\nIn February 2025, as they were building out this functionality, they turned to Mastra.\n\n\n“We liked how Mastra handled complex workflows,” Karégar says. “We needed a flexible, extensible foundation for a veterinary-focused chatbot, and Mastra’s modular agents and workflow engine fit that need perfectly.”\n\n\n\n## How Mastra helps Vetnio go above and beyond transcription \n\n\nNow a team of 9, Vetnio currently utilizes Mastra to deploy chatbots which assist with common veterinary administrative tasks, like scheduling appointments, booking follow-ups, sending appointment reminders, and answering common client inquiries. \n\n\nThe team was able to use Mastra to spin up specialized agents for each type of task and iterate quickly based on vet feedback. It’s often difficult to build a single agent that specializes in multiple types of tasks but they’ve found that Mastra makes it easy for them to build multi-agent systems.\n\n\n\nHere’s a high-level breakdown of how they built the chatbot: \n\n\n**Frontend:** Vetnio uses [assistant-ui](https://assistant-ui.com/), an agentic chat frontend with a strong Mastra integration. Assistant-UI is responsive, so vets and patients can access the chatbot on both desktop and mobile.\n\n\n**Orchestration:** Vetnio has set up a routing system where a Mastra workflow orchestrator routes each user request to the appropriate agent (booking, reminder, clinical info, etc). Each agent maintains its own context and tools, like a calendar API or a medical database.\n\n\n**Hosting:** The Vetnio team is all-in on the serverless model, which ensures auto-scaling, low maintenance, and predictable costs.\n\n\n“There were very few obstacles,” says founding engineer Emil Franzell. “Mastra’s documentation and examples covered most of what we needed, so setup and integration were straightforward. Agents and workflows could be added or modified without major rewrites, which kept development smooth.”\n\n\nThe team initially planned to build the chatbot from scratch. But after finding Mastra, making the switch was an obvious choice. \n\n\nAt the end of the day, the team is happy that Mastra helps them focus on what’s most important to them: building the best product experience for veterinary practices."
  },
  {
    "metadata": {
      "title": "Why Fireworks uses Mastra in their agentic runtime",
      "summary": "Why Fireworks AI decided to ship AIML, a prompt-based agent framework, using Mastra.",
      "author": "Sam Bhagwat",
      "publishedAt": "2025-06-10",
      "draft": false,
      "categories": ["case-studies"]
    },
    "slug": "fireworks-xml-prompting",
    "content": "Fireworks AI is an inference platform that runs open-source models and processes over 140 billion tokens daily for companies like Uber, DoorDash, and Cursor. Founded in October 2022 by former Meta engineers Lin Qiao and Dmytro Dzhulgakov who built and ran PyTorch, Fireworks recently closed a $52M Series B at a valuation over $500 million.\n\n## Rolling their own framework\n\nIn September 2024 Matt Apperson, a staff software engineer at Fireworks, was tasked with figuring out an agentic vision for Fireworks. The goal was to build a hosted agentic system with the control and programmability requested by Fireworks' customers.\n\n\"I'm a big fan of using state machines for agentic flows,\" Matt explains. \"I think they make a lot of sense, but they're not quite perfect, they're a little verbose.\" The team was prototyping a three-layer architecture: a state machine foundation, an agentic runtime on top of that, then an API layer.\n\nBut finding the right foundation proved challenging. \"There wasn't really a lot out there that was not Python based,\" Matt recalls. He felt that existing solutions like LangChain were \"either too much or too little,\" and they didn't allow other pieces of the TypeScript ecosystem.\n\nSo Fireworks started building their own. Matt was 75% of the way through implementing a custom state machine system when he found Mastra.\n\n## Discovering Mastra\n\nMatt discovered Mastra while browsing GitHub to get a lay of the land. He found Mastra and was immediately impressed: \"You guys were already using XState, which we were too.\"\n\nThe timing couldn't have been better. \"Mastra basically handed us that first layer in a nice neat little bow,\" Matt explains. When he found Mastra, he threw out the state machine he'd written.\n\nUsing Mastra workflows as their foundation, the Fireworks team [built AIML](https://aiml.fireworks.ai/), an open source framework allowing developers to build multi-step agentic systems using only prompts with special XML tags.\n\n\"Instead of sending just text as a system prompt to Fireworks API, you can send these AIML style prompts,\" Matt explains. \"It just uses declarative XML tags within a prompt just like you use XML normally in a prompt, just now they are more functional.\"\n\n![AIML workflow visualization showing state graph with Incoming Request, Think, Answer, and Stream Response nodes](/images/casestudy-fireworks-1.png)\n\n\nThe system works by parsing XML-tagged prompts, breaking them down into document order flows based on SCXML state graphs, then dynamically composing Mastra workflows to execute the logic. The result: complex agentic workflows that can be built and iterated on without writing code.\n\n\"The problem we're solving is that product teams often work in TypeScript while AI experts work in Python,\" Matt explains. \"With AIML, you don't need to convert between languages. It's a whole lot easier to hand a prompt back and forth than it is to take a Python notebook and convert that to TypeScript.\"\n\n![AIML code examples showing Simple control flow, Real-time streaming, and State & Context features](/images/casestudy-fireworks-2.png)\n\nThe Mastra team worked closely with Matt during development. \"The biggest pain point was the concept around being able to pass values from one step to the next,\" Matt said. The Fireworks team had wrapped Mastra workflows in a class to automatically pass values between steps, and shared this feedback with the Mastra team, who \"implemented it quickly, but not just like, slapping it together, but really kind of thoughtfully applying the feedback from a first principles perspective.\"\n\n## Early traction and focus shift\n\nPost-launch, some of Fireworks' largest customers have begun using AIML, and using Mastra allowed the team to shift focus from infrastructure to user experience. \n\n\"It allowed us to take a step back and stop worrying about the internals,\" Matt reflects.\n\nThe team is currently working on real-time tracing capabilities and preparing for broader production use."
  },
  {
    "metadata": {
      "title": "Mastra Changelog 2025-06-06",
      "publishedAt": "2025-06-06",
      "summary": "Mastra 101, Mastra Auth, and more",
      "author": "Shane Thomas",
      "draft": false,
      "categories": ["changelogs"]
    },
    "slug": "changelog-2025-06-06",
    "content": "It’s been a packed week at Mastra. We’re at the AI Engineer’s World Fair giving away fresh copies of Principles of Building AI Agents, 2nd edition. We also released a new course. Let’s dig in…\n\n## Mastra 101\n\nWe released the first ever MCP-based, IDE-native course: Mastra 101. \n\nNow you can learn all about agents, workflows, MCP, etc., all through your agentic code editor — no videos, books, websites, or apps needed.\n\n![Mastra 101 course demo](/mastracourse.gif)\n\n \nJust hook up the Mastra MCP docs server to your code editor and type `Start Mastra course` to get going.\n\n[More instructions and details here](https://mastra.ai/blog/mastra-101). \n\n## Mastra Auth\n\nWe released Mastra Auth with support for Auth0, WorkOS, Clerk, Firebase, and Supabase.\n\n![Mastra Auth demo](/weather-chat.gif)\n\nThe new `@mastra/auth` package implements JWT authentication with core utilities and provider classes.\n\nWe also introduces an abstract `MastraAuthProvider` class. This class establishes a foundation for implementing custom authentication providers and managing protected routes.\n\n## Workflows\n\nWe introduced a handful of improvements to workflow this week:\n\n- You can now display the step status for a multi-step nested workflows ([#4505](https://github.com/mastra-ai/mastra/pull/4505)).\n- You can directly link and preselect traces from a workflow step by clicking on it ([#4496](https://github.com/mastra-ai/mastra/pull/4496)).\n\nWe are hopeful these upgrades will make it easier to design, debug, and iterate on complex workflows. \n\n## Memory\n\nThere are two big memory updates this week.\n\nFirst, we fixed image handling in the playground. Now your agents can interpret and engage with image data. This fix was handled by modifying how attachments and messages are processed and ensuring proper display of images retrieved from memory.\n\n![Memory image handling](/memoryimage.png)\n\nSecond, we refactored our message processing logic to make it easier to store richer internal representations of messages. We then output that representation into UIMessage format. This change lays the groundwork for AI SDK V5 support.\n\n## Other updates\n\n- We added `RuntimeContext` support to tools and agents in workflow steps ([#4539](https://github.com/mastra-ai/mastra/pull/4539)).\n- We released a new feature that allows users to set a `runtimeContext` in AGUI when using CopilotKit. The feature also updates the documentation accordingly ([#4501](https://github.com/mastra-ai/mastra/pull/4501)).\n- We now support  database-specific configuration in `createVectorQueryTool` for RAG, allowing both static and runtime overrides ([#4552](https://github.com/mastra-ai/mastra/pull/4552)).\n- We built-in backward compatibility logic to all storage adapters for public storage APIs. This  addresses typecheck issues introduced by recent changes to UIMessages handling ([#4530](https://github.com/mastra-ai/mastra/pull/4530)).\n\nFind the full release log [here](https://github.com/mastra-ai/mastra/releases/tag/%40mastra%2Fcore%400.10.2)."
  },
  {
    "metadata": {
      "title": "Building an AI Research Assistant  with vNext Workflows",
      "publishedAt": "2025-06-06",
      "summary": "A deep dive into building MastraManus, an AI research assistant powered by Mastra's vNext workflows.",
      "author": "Sam Bhagwat",
      "draft": false,
      "categories": ["examples"]
    },
    "slug": "research-assistant",
    "content": "# Introducing MastraManus, an AI Research Assistant\n\nWe recently built an AI-powered research assistant called MastraManus. We wanted MastraManus to be able to handle complex research tasks while keeping humans in the control loop. Here's a video of it in action:\n<br/>\n<iframe\n  width=\"560\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/e_D7aQOHqxY?si=mNcuUOeCYrO8usnJ\"\n  title=\"MastraManus Demo\"\n  frameBorder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n  allowFullScreen\n></iframe>\n\nThis project was also the perfect opportunity to explore Mastra's vNext workflows capabilities.\n\nIn this post, we'll walk through how we built MastraManus using:\n\n- Nested workflows to encapsulate repeatable logic\n- Human-in-the-loop interactions using vNext's `suspend` and `resume` mechanism\n- `doWhile` loops for conditional workflow execution\n- Exa API for high-quality web search results\n\nLet's dive in!\n\n## The Architecture: A Workflow Within a Workflow\n\nThe most interesting aspect of our application is its nested workflow architecture. MastraManus has two workflows:\n\n1. **Research Workflow**: Handles user query collection, research execution, and approval\n2. **Main Workflow**: Orchestrates the research workflow and report generation\n\nThis approach gives us a clean separation of tasks, making the code more maintainable and even reusable for other applications.\n\n```tsx\n// Main workflow orchestrates everything\nexport const mainWorkflow = createWorkflow({\n  id: \"main-workflow\",\n  steps: [researchWorkflow, processResearchResultStep],\n  inputSchema: z.object({}),\n  outputSchema: z.object({\n    reportPath: z.string().optional(),\n    completed: z.boolean(),\n  }),\n});\n\n// The key pattern: using doWhile to conditionally repeat the research workflow\nmainWorkflow\n  .dowhile(researchWorkflow, async ({ inputData }) => {\n    const isCompleted = inputData.approved;\n    return isCompleted !== true;\n  })\n  .then(processResearchResultStep)\n  .commit();\n```\n\n## Human-in-the-Loop with Suspend and Resume\n\nOne of the most helpful features of vNext workflows is its built-in support for suspend and resume operations. This enabled us to create intuitive human-in-the-loop interactions (minus the complexity of state management).\n\nHere's how we implemented the user query step:\n\n```tsx\nconst getUserQueryStep = createStep({\n  id: \"get-user-query\",\n  // Schemas defined for input, output, resume, and suspend\n  execute: async ({ resumeData, suspend }) => {\n    if (resumeData) {\n      return {\n        ...resumeData,\n        query: resumeData.query || \"\",\n        depth: resumeData.depth || 2,\n        breadth: resumeData.breadth || 2,\n      };\n    }\n\n    await suspend({\n      message: {\n        query: \"What would you like to research?\",\n        depth: \"Please provide the depth of the research [1-3] (default: 2): \",\n        breadth:\n          \"Please provide the breadth of the research [1-3] (default: 2): \",\n      },\n    });\n\n    // Unreachable but needed\n    return {\n      query: \"\",\n      depth: 2,\n      breadth: 2,\n    };\n  },\n});\n```\n\nAnd in our main application, we handle this suspension with Node's readline:\n\n```tsx\n// Handle user query step\nif (result.suspended[0].includes(\"get-user-query\")) {\n  const suspendData = getSuspendData(result, \"research-workflow\");\n\n  const message =\n    suspendData.message?.query || \"What would you like to research?\";\n  const depthPrompt =\n    suspendData.message?.depth || \"Research depth (1-3, default: 2):\";\n  const breadthPrompt =\n    suspendData.message?.breadth || \"Research breadth (1-5, default: 2):\";\n\n  const userQuery = await question(message + \" \");\n  const depth = await question(depthPrompt + \" \");\n  const breadth = await question(breadthPrompt + \" \");\n\n  console.log(\n    \"\\\\nStarting research process. This may take a minute or two...\\\\n\",\n  );\n\n  result = await run.resume({\n    step: [\"research-workflow\", \"get-user-query\"],\n    resumeData: {\n      query: userQuery,\n      depth: parseInt(depth) || 2,\n      breadth: parseInt(breadth) || 2,\n    },\n  });\n}\n```\n\nThis gives us flexibility. The workflow itself doesn't care about how the user interaction happens — which could be via a command line, a web form, or even a voice assistant. The workflow simply suspends and the application layer handles the appropriate UI interaction.\n\n## Enhancing Research Quality with Exa API\n\nTo give our assistant high-quality search capabilities, we integrated MastraManus with the Exa API - a search engine designed specifically for AI applications. The implementation was straightforward:\n\n```tsx\nimport { createTool } from \"@mastra/core/tools\";\nimport { z } from \"zod\";\nimport Exa from \"exa-js\";\n\n// Initialize Exa client\nconst exa = new Exa(process.env.EXA_API_KEY);\n\nexport const webSearchTool = createTool({\n  id: \"web-search\",\n  description: \"Search the web for information on a specific query\",\n  inputSchema: z.object({\n    query: z.string().describe(\"The search query to run\"),\n  }),\n  execute: async ({ context }) => {\n    const { query } = context;\n\n    // ... error handling\n\n    const { results } = await exa.searchAndContents(query, {\n      livecrawl: \"always\", // Important for fresh results\n      numResults: 5,\n    });\n\n    // ... process and return results\n  },\n});\n```\n\nThe Exa API was a perfect fit for our application as it:\n\n- Provides up-to-date information from across the web\n- Returns the full content of pages, not just snippets\n- Supports live crawling to ensure we have the most recent information\n\n## The Feedback Loop: `doWhile` in Action\n\nAnother pattern we implemented was using vNext's `doWhile` loop to enable iterative research refinement:\n\n```tsx\nmainWorkflow\n  .dowhile(researchWorkflow, async ({ inputData }) => {\n    const isCompleted = inputData.approved;\n    return isCompleted !== true;\n  })\n  .then(processResearchResultStep)\n  .commit();\n```\n\nThis solution solves a tricky workflow problem: allowing users to keep refining their research until they're satisfied with the results. The `doWhile` loop keeps executing the research workflow until the user approves the results.\n\n## What's Next?\n\nWe have several ideas for future improvements:\n\n- Adding memory to remember past research sessions\n- Implementing cross-device persistence using Mastra's storage capabilities\n- Creating a web UI for even more user-friendly interaction\n\nFor now, you can check out MastraManus on [Github](https://github.com/mastra-ai/aie-feb-25-starter-mastra)."
  },
  {
    "metadata": {
      "title": "Introducing Mastra 101: The experimental course led by MCP, right in your IDE",
      "publishedAt": "2025-06-04",
      "summary": "The newest (and perhaps coolest) way to learn Mastra",
      "author": "Shane Thomas",
      "draft": false,
      "categories": ["announcements"]
    },
    "slug": "mastra-101",
    "content": "MCP has been getting a ton of hype this year and for good reason. It’s very fun to equip AI agents with the tools to take real-world actions. But that use of MCP merely scratches the surface of what’s possible. MCP has the potential to change the nature of education itself.\n\nIntroducing [Mastra 101](https://mastra.ai/course): the very first course (as far as we can tell) led by MCP. \n\nThat’s right: instead of reading a book, tutorial, or watching a video, you can learn how to use Mastra right in your agentic code editor (whether that’s Cursor, Windsurf, or VSCode.)\n\n![Mastra 101 course demo](/mastracourse.gif)\n\nPreviously, we launched the [Mastra MCP docs server](https://mastra.ai/blog/introducing-mastra-mcp), which allows you to ask your agent questions about Mastra and enables it to write syntactically-correct code. \n\nNow the MCP docs server contains the information to walk you through a step-by-step course of using Mastra as well. It’s literally never been easier to remake yourself into an AI engineer.\n\nOnce you’ve added the Mastra MCP docs server to your agentic code editor, simply prompt it to `Start Mastra course` and begin. \n\nWhile the course might be fun to speedrun, we know that many of you will want to break it into pieces and/or revisit parts later. That’s why you can start a new chat at any time, ask it where you are in the course, and pick up right where you left off.\n\n![Mastra course tracker demo](/mastracoursetracker.gif)\n\nThe agent will also direct you to a customized course progress page. This is where you can track your progress and explore the syllabus.\n\nSimilarly, if you’d like to jump around parts or skip ahead, you can ask your agent to do so.\n\nThe course is currently “experimental.” As you probably know by now, we try to keep things at Mastra fast because the field of AI moves even faster. We hope to ship updates based off your feedback.\n\nAnd if you really like videos, here's an introduction to Mastra 101:\n\n<iframe\n  width=\"600\"\n  height=\"400\"\n  src=\"https://www.youtube.com/embed/xh09d4zijsc?si=oJTcCKq5Mo5cABrD\"\n  title=\"Mastra 101 Introduction\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n  allowFullScreen\n  className=\"rounded-lg\"\n></iframe>\n\n\n\n\nBut for now… happy learning-slash-building 🚀"
  },
  {
    "metadata": {
      "title": "Announcing the second edition of Principles of Building AI Agents",
      "publishedAt": "2025-06-02",
      "summary": "We're excited to announce the release of v2 of Principles of Building AI Agents.",
      "author": "Sam Bhagwat",
      "draft": false,
      "categories": ["announcements"]
    },
    "slug": "principlesv2",
    "content": "Two months is a short time to write a new edition of a book, but life moves fast in AI.\n\nThis edition has new content on MCP, image gen, voice, A2A, web browsing and computer use, workflow streaming, code generation, agentic RAG, and deployment.\n\n![Principles of Building AI Agents v2 cover](/principlesv2.png)\n\n\nAI engineering continues to get hotter and hotter. Mastra’s weekly downloads have doubled each of the last two months. At a typical SF AI evening meetup, I give away a hundred copies of this book.\n\nThen two days ago, a popular developer newsletter tweeted about this book and 3,500 people (!) downloaded a digital copy (available for free at [mastra.ai/book](http://mastra.ai/book) if you are reading a paper copy).\n\nSo yes, 2025 is truly the year of agents. Thanks for reading, and happy building!\n\n## **Getting the book**\n\nThe book is [available as a download to everyone online](https://mastra.ai/book). You can also purchase a print copy on Amazon."
  },
  {
    "metadata": {
      "title": "Lua: From Worrying About Infrastructure to Focusing on Growing Partnerships",
      "summary": "How Lua switched from custom agent infrastructure to Mastra Cloud for rapid deployment of 100+ business partners across African markets.",
      "author": "Sam Bhagwat",
      "publishedAt": "2025-05-30",
      "draft": false,
      "categories": ["case studies"]
    },
    "slug": "lua-scaling",
    "content": "Lua is building a chat bot and assistant marketplace that connects businesses with consumers. They allow brands to respond to users via WhatsApp to answer questions about the company, and provide tailored commerce experiences.\n\nFounded in 2024 with a distributed team across Africa and Europe, Lua recently closed seed funding and has launched across three African markets. In these markets, WhatsApp has become the operating system for daily life—used by close to 200 million Africans who often bypass traditional banking entirely, conducting everything from government services to P2P commerce; in Nigeria, 95% of the population are daily active users. \n\n![Lua orchestrating a food order through WhatsApp, connecting users to restaurant partners like ChowPadi through simple chat](/images/casestudy-lua.png)\n\n\nCo-founders Stefan Kruger and Lorcan O Cathain have lived and built in Africa for their entire professional lives. Lorcan (CEO) grew up in Ireland but lived in East Africa for most of a decade, advising the Rwandan government on trade policy, then building and running an African fintech for both financial and insurance products in Kenya. Stefan (CTO) was VP of Engineering at Nigerian YC (W '16) fintech Paystack and then served in the same capacity at Stripe after acquisition, connecting Stripe with the African continent. \n\n\n## Building the initial systems\n\nThe Lua team needed to stand up core infrastructure to support 500k+ users at launch, and Stefan spent over 9 months building the core systems —  agent orchestration, monitoring systems, workflow management, and evaluation frameworks — to support over 100 business partners.\n\nThe team was already serving 4,500 daily users in pre-launch while quickly approaching an April 1st launch. Stefan had been wrestling with performance optimization—disabling semantic searches under load due to memory and CPU constraints, fine-tuning the balance between cost and latency for their WhatsApp integration where response times are critical, and debugging complex stack traces through wrapped tool implementations. \n\n\n## Tossing their internal framework, moving to Mastra\n\nStefan's discovery of Mastra marked a turning point. Mastra provided exactly what they'd been building from scratch: workflows, agents, RAG, integrations, and evals—all in TypeScript, matching Lua's existing stack.\n\n\"I've been building since November last year, spending a lot of time figuring out just the foundational side of things—how to build agents very quickly, how do we monitor and how do we keep it solid,\" Stefan recalls. \"I discovered your platform, I was like, holy, that would have saved me a ton of time.\" Stefan made a quick decision to move on from months of work on their own internal agent framework in favor of using Mastra's off the shelf primitives\n\nThe migration was fast. Stefan quickly got Lua's prototype fully running on Mastra, with agent orchestrations replicated quickly. Using Mastra's workflow primitives, the team was able to rapidly test complex orchestration scenarios, including hotel and flight booking integrations with end-to-end authentication flows.\n\nAs Stefan put it in March: \"I wish I found you guys three months ago. I would have had less gray hair.\"\n\n\n## Lua's deployment architecture\n\nLua currently runs Mastra agents inside Hono servers, bundled into Docker containers within a Kubernetes cluster deployed on AWS EKS, with Neon handling their serverless Postgres backend. They're now running a parallel deployment on Mastra Cloud to test performance and reliability before fully migrating their business partners and potentially thousands of agents to eliminate infrastructure management overhead.\n\n\"We have 100 partners. I don't want to really manage 100 infrastructure components,\" Stefan explains. The built-in observability was particularly attractive: \"Even the tracing and monitoring, like even running a separate stack for the monitoring side for us is just an overkill if you already have that all in there.\"\n\nStefan noted the improvements to Lua's development velocity. \"Now we can just focus on the business side, which is more exciting for us. Just getting these businesses out, partnering up and getting it out to consumers.\" \n\n\n## Growth after launch\n\nThe launch was a huge success, and right now, the Lua team is running quickly to keep up with demand both in the Middle East as well as East Africa. It's still early, but at one point in May, they were getting 200 new users every 10 minutes. At the last check-in, infrastructure and tooling were not fires they were fighting, or had recently fought."
  },
  {
    "metadata": {
      "title": "Medusa builds e-commerce agents with Mastra",
      "summary": "How MedusaJS used Mastra Cloud to prototype and deploy agent architectures for automated e-commerce feature generation.",
      "author": "Sam Bhagwat",
      "publishedAt": "2025-05-30",
      "draft": false,
      "categories": ["case studies"]
    },
    "slug": "medusa-ecommerce",
    "content": "MedusaJS is the most popular open-source Javascript e-commerce framework, with over 29K stars on Github, and helps companies build custom online stores. Founded in 2021 and headquartered in Copenhagen, Denmark, the 15 person team has raised $9M across two funding rounds, including an $8M seed round in 2022 led by Dawn Capital.\n\n\n## Choosing Mastra to iterate quickly\n\nCo-founders Oli & Seb wanted to quickly test different agent architectures for a new \"AI solutions engineer\" that would allow users to ask an agent to build features into stores end-to-end. Because they were a Typescript framework, Mastra was the natural choice.\n\nMastra's built-in eval framework and local development environment allowed the team to prototype different agent configurations, while the workflow visualization helped them understand how different architectures performed in practice\n\n\"Mastra's TypeScript framework enabled us to quickly experiment with different architectures using the primitives and the dashboard gave us easy visibility into execution state and memory persistence. We used multiple eval frameworks to determine which configurations best handled feature generation requests.\"\n\n![Senior Software Engineer Riqwan Thamir demonstrates the MedusaJS AI solutions engineer agent in the Mastra Cloud](/images/casestudy-medusa.png)\n\n## First approach: multi-agent\n\nUsing Mastra's workflow primitives, the team built their first architecture with separate agents handling different parts of the e-commerce feature pipeline\n\n\"We started out with a fleet of agents because we thought, well, for every component that we need to do, let's have an agent that does it,\" said Oli Juhl, co-founder and CTO. Mastra made it simple to set up different agent configurations and test coordination patterns. Medusa built separate agents for database models, API endpoints, frontend components, and business logic.\n\nThe initial results were mixed. \"We were having some problems with the accuracy and the transferable knowledge between the agents and the latency between all the agents and the different tools they were using,\" explained Senior Software Engineer Adrien De Peretti.\n\nThe agents couldn't maintain context about how different parts of the system connected. An agent generating a database model didn't understand how frontend components would consume the data, leading to mismatched interfaces and integration problems.\n\n\n## The successful approach: single-agent architecture\n\nBut because Mastra has single-agent, multi–agent, and workflow primitives, when multi-agent didn't work as expected, the team could pivot quickly to test a completely different architecture with only minor code changes.\n\n\"The best results we have seen have come from using one agent…with the entire code base fed into the context,\" Juhl explained. \"Both the storefront and the entire Medusa repo into the context using Gemini.\"\n\nPutting the whole codebase in context made an immediate difference. \"If we provide everything like the entire code base of both projects, we are able to build end to end features with some pretty strict guardrails included in the prompt,\" Juhl noted.\n\nThe single-agent approach generated code that imported existing modules correctly, followed established patterns, and integrated with database schemas. \"Even the simplest prompts actually give very good results,\" Juhl observed. \"We can write something as simple as 'build me a product review feature for my e-commerce store' and it builds it end-to-end.\"\n\n\"The developer experience is pretty fantastic,\" Juhl noted. \"We were able to set up and test quickly with your framework which I really love\" and quickly made the decision to release this agent as an experimental feature to certain customers.\n\n## A successful rollout\n\nOne month after shipping to customers, Medusa has seen validation of their AI solutions engineer. The team has successfully merged PRs generated by the agent, with customers actively using the feature.\n\nThe rollout has been selective, with Medusa choosing six companies to work with initially with their own engineering teams. Through \"AI Tickets\" in their Medusa Cloud dashboard, these customers submit prompts for features they want built, with Medusa manually running those prompts through the agent before opening PRs. \"The people we've chosen are the ones that usually when they have problems or they need something, they are capable of giving enough details and digging enough for us to be able to do something about it instead of just saying it doesn't work,\" explains De Peretti. \"Users seem pretty happy for now. There's a lot of excitement around the feature. We're moving forward with a greater rollout as we speak.\"\n\nMembers of the internal development team have been converted, too. \"I don't use Cursor anymore,\" says Thamir. \"I use this agent for my local development flows and it's so much better than Cursor... I build entire features just using the agent.\"\n\nThe team is now working through scaling challenges—primarily rate limiting from API providers and context management for long conversations that can reach 4 to 5 million tokens—while preparing to expand access to more developers and eventually non-technical users like e-commerce managers."
  },
  {
    "metadata": {
      "title": "Mastra Changelog 2025-05-29",
      "publishedAt": "2025-05-29",
      "summary": "MCP Server updates, streaming support, and more.",
      "author": "Shane Thomas",
      "draft": false,
      "categories": ["changelogs"]
    },
    "slug": "changelog-2025-05-29",
    "content": "It’s been an eventful past few weeks at Mastra. Last week we shipped [Mastra 0.10](https://mastra.ai/blog/mastra-0.10). Yesterday we publicly announced the [hackathon winners](https://mastra.ai/blog/mastra-build-recap). And today we’re back with more updates.\n\nLet’s dig in…\n\n## Pass agents into MCPServer\n\nAgents can now be directly passed into the MCPServer class, automatically creating tools from them (with the naming convention `ask_<agentId>`.) The agent description is converted into a tool description and the tool will call `agent.generate()`.\n\n![MCP Server with agents example](/mcpserver1.png)\n\n## Pass workflows into MCPServer\n\nSimilarly, you can also pass workflow instances into the MCPServer class. This gives you a tool for each workflow. The workflow `inputSchema` and description are used as the tool `inputSchema` and description. Calling the tool will start the workflow.\n\n![MCP Server with workflows example](/mcpserver2.png)\n\n## Stream workflows\n\nWe added streaming support (`stream`) for workflows so users can see exactly what step an executed workflow is on.\n\n```tsx\nconst { stream } = run.stream({inputData: {}))\nfor await (const data of stream) {\n  if (data.type === 'step-suspended') {\n    // make it async to show that execution is not blocked\n    setImmediate(() => {\n      const resumeData = { stepId: 'promptAgent', context: { userInput: 'test input for resumption' } };\n      run.resume({ resumeData: resumeData as any, step: promptAgent });\n    });\n  }\n}\n```\n\nThis works similarly to the client-js watch SDK.\n\n## Other updates\n\n- [We now support MongoDB as a storage backend](https://github.com/mastra-ai/mastra/pull/3841). [You can also pass options to MongoDBStore, similar to how options are passed in the Vector case](https://github.com/mastra-ai/mastra/pull/4414).\n- [We added configuration for the `mastra` package in the Verdaccio registry setup for E2E testing environment](https://github.com/mastra-ai/mastra/pull/4393).\n- [We added runtimeContext to evals.](https://github.com/mastra-ai/mastra/pull/4422).\n\nThat’s all for this week. Happy building 🚀"
  },
  {
    "metadata": {
      "title": "Reducing tool calling error rates from 15% to 3% for OpenAI, Anthropic, and Google Gemini models",
      "publishedAt": "2025-05-29",
      "summary": "We recently built a tool compatibility layer that reduced tool calling error rates from 15% to 3% for 12 OpenAI, Anthropic, and Google Gemini models, across a set of 30 property types and constraints.",
      "author": "Daniel Lew",
      "draft": false,
      "categories": ["foundations"]
    },
    "slug": "mcp-tool-compatibility-layer",
    "content": "We recently built a tool compatibility layer that reduced tool calling error rates from 15% to 3% for 12 OpenAI, Anthropic, and Google Gemini models, across a set of 30 property types and constraints. \n\n![Tool Compatibility Error Rate by Provider](/tool-compat-error-rate-by-provider.png)\n\n## Background\n\nMastra is a TypeScript agent framework. When a user is doing tool calls that accepts some inputs they need to pass either a Zod schema (think `z.string().min(5)`) or a JSON schema.\n\nWe then transform the schema from Zod to JSON and then feed it into the model (for examples of Zod constraints, see the Appendix).\n\nPeople in the Mastra community were raising issues that they were trying to use a specific model with a tool with an MCP server. The tool calls would fail with some models and succeed with others. This made us think something was happening on the model side.\n\nWe saw this quite often with the OpenAI reasoning models, but various errors would happen with other model providers as well. \n\n## Scoping the problem\n\nTo better understand the problem, we started by making a test suite with a bunch of schema properties and constraints (like unions, nullable, array length), and ran it for each of our providers and models. We ended up with essentially a green / yellow / red grid of models vs constraints.\n\nSome things we noticed:\n\n- OpenAI models, if they didn't support a property, would throw an error with a message like \"invalid schema for tool X\"\n- Google Gemini models wouldn't explicitly fail, but they would silently ignore properties like the length of a string, or minimum length of an array.\n- Anthropic models performed quite well, the majority of the ones we tested did not error at all.\n- DeepSeek and Llama weren't the best at tool usage and would occasionally refuse to call the tool (Even with small extremely explicit prompts: \"YOUR ONLY JOB IS TO CALL TOOL X, PLEASE PLEASE DO IT\"). They would also sometimes pass a value that did not respect the schema.\n\nThe things that seemed within our control to fix were schema errors and ignoring the schema constraints. Improving tool call and usage would be up the provider itself or for the client to implement some retry functionality.\n\n## Testing approaches to feeding the models schema constraints\n\nWe started thinking about how to solve this problem. There are only a few options for transforming tool definitions. We could feed information to the LLM: the tool description, the input schema, or the LLM prompt. \n\nWe started with the input schema: sometimes we could transform the input schema into a shape the LLM would accept (for example changing nullable fields to optional). This helped, but only worked for a few edge cases. There are also various types of JSON schema formats that helped. For example, a JSON schema output target can be selected while transforming between JSON Schema (used by MCP servers) and Zod (used in our framework) to choose between different versions of the JSON Schema spec that are more compatible with certain models.\n\nThe next thing we tried was injecting the tool schema contraints into to the LLM prompt in cases where the tool call was likely going to fail. This worked, and our tests for the popular model providers (OpenAI, Google, Anthropic) passed!\n\n## The solution\n\nOur problem was now fixed, but our worry was that this felt like an impedance mismatch — the prompt just didn't feel like the right place to put it. We also tested with short prompts and we somewhat worried about what would happen with longer prompts.\n\nBut it did work, so we gained confidence that this was a solvable problem. We continued experimenting and next tried passing schema constraints into tool instructions. Just like with agent prompt injection, it turned out that it worked well too — now our entire test suite for OpenAI, Anthropic, and Google models was properly following the tool schema constraints.\n\nWe tried to go a level deeper after this, we added it directly to the property description. This felt like the perfect place as the description shouldn't leak outside of the property itself.\n\nThis was much more appropriate as everything was contained in the tool property itself and nothing was spilling over to the tool or agent definitions.\n\nTake for example a string that you need to be a url. In the JSONSchema spec you can specify a string to be of `uri` format, which you would define as `z.string().url()` using Zod. Before our changes, this is the payload that would get sent to a model like o3-mini.\n\n```json\n{\n  \"parameters\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"stringUrl\": {\n        \"type\": \"string\",\n        \"format\": \"uri\"\n      }\n    },\n    \"required\": [\n      \"stringUrl\"\n    ],\n  }\n}\n```\n\nThe problem is that o3-mini, like many other models, will ignore or error if given the `format` property. \n\nAfter our changes, this is the payload that we send:\n\n```json\n{\n  \"parameters\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"stringUrl\": {\n        \"type\": \"string\",\n        \"description\": \"{\\\"url\\\":true}\"\n      }\n    },\n    \"required\": [\n      \"stringUrl\"\n    ],\n  },\n}\n```\n\n## Why this matters\n\nWe all remember the bad old days of web develop where browser compatibility was historically a major concern (IE8 😅). \n\nOur hope is that the framework layer will provide a shim for model tool interoperability so that teams don't have to refactor their entire codebase if they want to switch (or just test out!) a different model provider. \n\nEven though we saw this because of users wanting better MCP support, it actually works for all tools — not just ones being pulled in from an MCP server.\n\n## Before and after\n\nWe tested 30 extensive property types and constraints on those types and passed a tool with that input schema to the LLM to see if it would handle it properly. Here are the results of that test\n\nNOTES: \n- For DeepSeek and Meta models, the tests would perform fairly inconsistently. Sometimes it would be a lot better, sometimes it would be a lot worse. Adding in retries for the calls would greatly improve performance with or without the compatability layer.\n- Some Zod properties like `z.never()`, `z.undefined()`, and `z.tuple()` were omitted from this test as they either don't make sense for the schema of a tool or are extremely rare. The majority of models don't handle these properties. As a result we throw a clear a error for when the model doesn't handle these field types.\n\nHere are the results of our tests before and after the MCP Tool Compat layer was applied.\n\n![Tool Compatibility Error Rate by Model](/tool-compat-error-rate-by-model.png)\n\n| PROVIDER       | MODEL                         | Before          | After           |\n|----------------|-------------------------------|--------------------|-------------------|\n| Anthropic      | `claude-3.5-haiku`              | 96.67%             | 100%              |\n| Anthropic      | `claude-3.5-sonnet`             | 100%               | 100%              |\n| Anthropic      | `claude-3.7-sonnet`             | 100%               | 100%              |\n| Anthropic      | `claude-opus-4`                 | 100%               | 100%              |\n| Anthropic      | `claude-sonnet-4`               | 100%               | 100%              |\n| DeepSeek       | `deepseek-chat-v3-0324`         | 60.00%             | 86.67%            |\n| Google         | `gemini-2.0-flash-lite-001`     | 73.33%             | 96.67%            |\n| Google         | `gemini-2.5-flash-preview`      | 86.67%             | 96.67%            |\n| Google         | `gemini-2.5-pro-preview`        | 90.00%             | 96.67%            |\n| Meta Llama     | `llama-4-maverick`              | 86.67%             | 90.00%            |\n| OpenAI         | `gpt-4.1`                       | 96.67%             | 100%              |\n| OpenAI         | `gpt-4.1-mini`                  | 96.67%             | 100%              |\n| OpenAI         | `gpt-4o`                        | 96.67%             | 100%              |\n| OpenAI         | `gpt-4o-mini`                   | 93.33%             | 100%              |\n| OpenAI         | `o3-mini`                       | 73.33%             | 100%              |\n| OpenAI         | `o4-mini`                       | 73.33%             | 100%              |\n\n## Using it\n\nThe MCP Tool Compatibility Layer is now available in Mastra in all versions after 0.9.4 (including our 0.10.0 release). If we're missing a model, we've made it really easy to add new model provider coverage, check out the source code on [GitHub for the OpenAI layer](https://github.com/mastra-ai/mastra/blob/main/packages/core/src/tools/tool-compatibility/provider-compats/openai.ts) for an example.\n\nPlease drop us a Github issue or drop into our Discord if you have questions or comments about how we're doing this, want to extend it, or want to implement something in a project not using Mastra.\n\n[Here is a link to the implementation](https://github.com/mastra-ai/mastra/pull/4027)\n\nWe can't wait to see what you'll build!\n\n## Appendix\n\n[Google sheet with comprehensive test results](https://docs.google.com/spreadsheets/d/1VqBdj7liJHQmiU0rwC44nAfNpHChJCmNe1mSWrMfko8/edit?usp=sharing). The models that did not error were not included in this. We hand selected popular models, it's likely that there is a wide range of performance from the models we did not include here.\n\n| Property Name   | Zod Definition                                                                                          |\n|-----------------|---------------------------------------------------------------------------------------------------------|\n| **String types**|                                                                                                         |\n| string          | `z.string()`                                                                                            |\n| stringMin       | `z.string().min(5)`                                                                                     |\n| stringMax       | `z.string().max(10)`                                                                                    |\n| stringEmail     | `z.string().email()`                                                                                    |\n| stringEmoji     | `z.string().emoji()`                                                                                    |\n| stringUrl       | `z.string().url()`                                                                                      |\n| stringUuid      | `z.string().uuid()`                                                                                     |\n| stringCuid      | `z.string().cuid()`                                                                                     |\n| stringRegex     | `z.string().regex(/^test-/)`                                                                            |\n| **Number types**|                                                                                                         |\n| number          | `z.number()`                                                                                            |\n| numberGt        | `z.number().gt(3)`                                                                                      |\n| numberLt        | `z.number().lt(1)`                                                                                      |\n| numberGte       | `z.number().gte(5)`                                                                                     |\n| numberLte       | `z.number().lte(1)`                                                                                     |\n| numberMultipleOf| `z.number().multipleOf(2)`                                                                              |\n| numberInt       | `z.number().int()`                                                                                      |\n| **Array types** |                                                                                                         |\n| array           | `z.array(z.string())`                                                                                   |\n| arrayMin        | `z.array(z.string()).min(5)`                                                                            |\n| arrayMax        | `z.array(z.string()).max(5)`                                                                            |\n| **Object types**|                                                                                                         |\n| object          | `z.object({ foo: z.string(), bar: z.number() })`                                                        |\n| objectNested    | `z.object({ user: z.object({ name: z.string().min(5), age: z.number().gte(18) }) })`                    |\n| **Optional and nullable**|                                                                                                |\n| optional        | `z.string().optional()`                                                                                 |\n| nullable        | `z.string().nullable()`                                                                                 |\n| **Enums**       |                                                                                                         |\n| enum            | `z.enum(['A', 'B', 'C'])`                                                                               |\n| nativeEnum      | `z.nativeEnum(TestEnum)`                                                                                |\n| **Union types** |                                                                                                         |\n| unionPrimitives | `z.union([z.string(), z.number()])`                                                                     |\n| unionObjects    | `z.union([ z.object({ amount: z.number(), name: z.string() }), z.object({ type: z.string(), permissions: z.array(z.string()) }) ])` |\n| **Uncategorized types**|                                                                                                  |\n| default         | `z.string().default('test')`                                                                            |\n| anyOptional     | `z.any().optional()`                                                                                    |\n| any             | `z.any()`                                                                                               |\n| **Unsupported types**|                                                                                                    |\n| intersection    | `z.intersection(z.string().min(1), z.string().max(4))`                                                  |\n| never           | `z.never() as any`                                                                                      |\n| null            | `z.null()`                                                                                              |\n| tuple           | `z.tuple([z.string(), z.number(), z.boolean()])`                                                        |\n| undefined       | `z.undefined()`                                                                                         |\n\nUnsupported types were omitted from the test. But they are included in the tool compatibility layer as a way to throw a clear error if the model does not support the type."
  },
  {
    "metadata": {
      "title": "MASTRA.BUILD Recap & Winner Announcement",
      "description": "A recap of Mastra's very first hackathon",
      "publishedAt": "2025-05-27",
      "summary": "The results of MASTRA.BUILD, our inaugural hackathon.",
      "author": "Shane Thomas",
      "draft": false,
      "categories": ["announcements"]
    },
    "slug": "mastra-build-recap",
    "content": "We're thrilled to share the results of MASTRA.BUILD, our inaugural hackathon that brought together developers and tinkerers from all over the world to build with Mastra. We had a great time judging the submissions and are excited to publish the winners here.\n\n## Event Highlights\n\n- Over 300 participants signed up for the event\n- A full week of building, office hours, and collaboration\n- ~100 projects submitted across all categories\n- ~$3000 in cash prizes, $1000 in Smithery credits, and other prizes awarded to the winners\n\n## Category Winners\n\n### B2B SaaS Category\n\nIn first place, we have Chameleon, a tool that lets you chat with your invoices in order to fill them out.\n\n<iframe\n  width=\"560\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/Q3AWXcudeIo\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  referrerpolicy=\"strict-origin-when-cross-origin\"\n  allowfullscreen\n></iframe>\n\nSecond place goes to [Ragnet](https://drive.google.com/file/d/1qyWpCkhUv1kIOiOxwMnPVP3IDA3rrC8s/view), an AI recruiter.\n\nThird place goes to [Ava](https://www.loom.com/share/c5b6bf353fbb4e7890614ffbf9856c9e?sid=3e7a9ab2-82b9-4b26-8a9b-97e999e1e723), an AI virtual assistant that you interact with via voice.\n\n### Art & Creative Category\n\nIn first place, we have Family Storybooks, an app that chats with your family members to create a storybook of their lives.\n\n<iframe\n  width=\"560\"\n  height=\"315\"\n  src=\"https://www.loom.com/embed/4345144ec1ab4ee79d846cefe39cd338?sid=ec7f9914-1d38-46f9-ad64-a74149ca829c\"\n  title=\"Loom video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  referrerpolicy=\"strict-origin-when-cross-origin\"\n  allowfullscreen\n></iframe>\n\nSecond place goes to [Ninja Chef](https://ninja-meal-master.lovable.app/), a tool that takes stock of your pantry and fridge and suggests recipes.\n\nThird place goes to [ChessAgine](https://chessagineweb.vercel.app/), an AI powered chess companion that analyzes your moves and gives you feedback.\n\n### Other Category\n\nIn first place, we have MCP Lens: postman for MCP servers.\n\n<iframe\n  width=\"560\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/0oyhl7Hfxtk\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  referrerpolicy=\"strict-origin-when-cross-origin\"\n  allowfullscreen\n></iframe>\n\nSecond place goes to [Ekudi financial assistant](https://www.loom.com/share/cb3e06f468ea4b60abcd390837e512c9?sid=ba8b2e5c-63ff-4104-b98c-89fb54927913).\n\nWe didn't award a third place prize this time. Instead, we gave an honorary prize to [PerfAgent](https://agent.perflab.io/), a tool that helps you optimize your website's performance.\n\n## What's Next?\n\nThe success of MASTRA.BUILD has inspired us to make this a regular event. We're already planning the next hackathon with even more categories, prizes, and opportunities for developers to showcase their creativity.\n\nThanks to everyone who participated, and a special thanks to our sponsors:\n\n- [Smithery](https://smithery.com)\n- [MCP Runner](https://mcp.dev)\n- [MongoDB](https://mongodb.com)\n\nStay tuned for more updates and announcements about future Mastra events!"
  },
  {
    "metadata": {
      "title": "Full-Stack TypeScript Agents with Mastra and Copilot Kit",
      "publishedAt": "2025-05-23",
      "summary": "Build a full-stack TypeScript agent with Mastra and Copilot Kit.",
      "author": "Shane Thomas",
      "draft": false,
      "categories": ["examples"]
    },
    "slug": "fullstack-typescript-agents-with-mastra-and-copilotkit",
    "content": "We recently teamed up with CoPilot Kit for a [joint workshop](https://www.youtube.com/live/wzneIfcgXvM) on building agents that run entirely in the browser using TypeScript. Copilot Kit is an open-source framework for embedding AI copilots directly into your app UI, enabling them to perform real actions like submitting forms, triggering UI updates, and calling APIs. They bring the same rigor to frontend-agent interaction that Mastra brings to agent runtime.\n\nPairing Mastra with Copilot Kit lets JS engineers build full-stack AI agents that persist state, handle real inputs, and run directly in the browser, with no backend required.\n\n<div className=\"h-[400px] mt-5\">\n<VideoPlayer \n  src=\"https://youtu.be/wzneIfcgXvM?t=1126\" \n  className=\"max-w-full rounded-lg\"\n  showThumbnail={false}\n/>\n</div>\n\nThis demo shows a real-world, multi-step workflow where a user-facing copilot could:\n\n- Query backend APIs\n- Parse and transform user data\n- Trigger UI changes in response to reasoning steps\n- Maintain session-aware memory across interactions\n\nAnd all of it was handled in TypeScript-with no model lock-in, no proprietary DSL, and no backend roundtrips.\nA phrase that stuck with the workshop audience: \"Shipping full-stack, production-ready agents—auth, context persistence, long-tails included.\"\n\nDevelopers could step through the logic, tweak behavior live, and inspect memory without leaving the devtools. No special tooling like bespoke agent orchestration CLIs or closed-source SDKs. No custom language like YAML flows or internal DSLs. Just plain TypeScript—functions, promises, async flows—wired together with your own fetches, UI components, and toolchains. Code that runs in-browser, debugs in devtools, and integrates cleanly with your existing frontend stack.\n\n## Why Most Agent Stacks Break Down for JS Engineers\n\nBuilding AI agents typically means hopping between languages and abstractions. You write your core logic in Python, use YAML or DSLs for orchestration, and try to glue it all into a JavaScript frontend. That's cognitively expensive and brittle in practice.\n\nToo many agent SDKs feel like this:\n\n- You install a CLI that scaffolds a mystery box.\n- You write YAML or a new DSL to describe flows.\n- You pray it works, then backtrack to the frontend to wire up UI.\n- It&apos;s disjointed. Slow. Hard to debug.\n\n\"I changed the agent logic and saw the UI update without a single refresh or build step.\" —Harsh Makadia (Copilot Kit)\n\nThat's what happens when your agent runs in the browser. Mastra and Copilot Kit make it feel like just another TypeScript module.\n\n## What \"Full-Stack TypeScript Agents\" Actually Means\n\n### 1. Single Language, Full Surface Area\n\nTypeScript defines everything: agent logic, UI behaviors, execution steps, memory. No context switching. Just `tsc` and go.\n\n```typescript showLineNumbers copy\n// Basic step definition\nconst steps = [\n  async () => await showForm({ fields: [\"email\", \"intent\"] }),\n  async (data) => await fetch(`/api/enrich?email=${data.email}`),\n  async (resp) => await showResult(resp),\n];\n```\n\n### 2. In-Browser Execution\n\nWith `WebAssembly` and `SharedArrayBuffers`, agents run client-side. That means faster iteration, less latency, and fewer backend calls.\n\n\"This isn't just rendering UI in the browser—your agents live and think there too.\" —Harsh Makadia (Copilot Kit)\n\n### 3. Composable Agent+UI Workflows\n\nAgents can call `await showForm(), await selectFromList()`, or trigger modals inline. Every UI interaction becomes a first-class step in the agent's reasoning loop.\n\n```typescript showLineNumbers copy\nawait agent.run(async (ctx) => {\n  const userInput = await ctx.tools.showForm({ fields: [\"searchTerm\"] });\n  const results = await searchDocs(userInput.searchTerm);\n  return await ctx.tools.selectFromList(results);\n});\n```\n\n### 4. Explicit State + Control\n\nYou own the agent plan, memory, and tools. No magic methods. No vendor lock-in. Just TypeScript functions and object literals.\n\n\"It&apos;s just TypeScript functions. No new DSL. No ceremony.\" —Shane Thomas (Mastra)\n\n\n## Production-Ready Means the Boring Stuff Works\n\n\"Production-ready\" doesn&apos;t mean perfect. It means predictable, testable, ship-fast-with-confidence.\n\n- Auth flows: Your agents can securely track users, pass tokens, and restrict access.\n- Context that persists: Memory survives across sessions, tabs, and reloads. CRM assistant? Agent-powered dashboard? All covered.\n- Edge-case handling: Custom tools. API retries. User disambiguation. It all works because you own the control logic.\n\n\n\"Let&apos;s stop romanticizing agents and start productizing them.\" —Shane Thomas (Mastra)\n\n## Why This Architecture Works for Engineering Leaders\n\nIf you're making architecture decisions, full-stack TypeScript agents solve a common pain: fragmentation between teams. When your ML logic is in Python, your UI is in React, and your workflow glue is in YAML, you get slower onboarding, more integration bugs, and hard-to-trace failures.\n\nThis model fixes that by collapsing logic and interface into a single, testable language.\n\n- One language = fewer silos\n\n- Runs in browser = easier onboarding\n\n- Devtools-native = faster debugging\n\n- Open-core = modifiable, forkable, inspectable\n\nIt&apos;s about owning the whole experience, with tools your team already knows.\n\nThanks again to the CopilotKit crew for the joint workshop. [Watch the full replay here](https://youtube.com/live/wzneIfcgXvM)"
  },
  {
    "metadata": {
      "title": "An Honest Skeptic Meets Mastra: Matt Pocock's Live Review",
      "publishedAt": "2025-05-23",
      "summary": "Typescript leader Matt Pocock builds, breaks, and reacts to Mastra in real time.",
      "author": "Abhi Aiyer",
      "draft": false,
      "categories": ["foundations"]
    },
    "slug": "matt-pocock-meets-mastra",
    "content": "Matt Pocock is a leading voice in the TypeScript world. He's known for asking tough questions and demanding clarity from developer tools. And he's skeptical of AI agent frameworks. Most feel like overhyped abstractions to him.\n\nSo we brought him into a Mastra workshop. No prep. Just build, break, and react in real time.\n\nHere's a recap of the highlights in case you missed it.\n\n## First Contact with Mastra\n\nMatt showed up with questions. \"What even is an agent? How is it different from a function?\" He called out naming choices, default settings, and edge-case handling.\n\nBut when we showed a typed agent with clear input/output and schema validation, his tone shifted. \"Okay, this I can work with,\" he said. Strong TypeScript support didn’t hurt.\n\n<iframe\n  width=\"560\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/w-aNPblqg94?si=5o7zDMdqQwZXquzg\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  referrerpolicy=\"strict-origin-when-cross-origin\"\n  allowfullscreen\n  className=\"max-w-full rounded-lg\"\n></iframe>\n\n## Primitives That Pass the Smell Test\n\nWe broke down the core primitives: inputs, steps, tools, output checks. Matt saw familiar patterns. Not magic, just structured logic.\n\n\"You're not claiming the AI does everything. You're giving me a framework I can reason about. That’s good.\"\n\nHe liked the analogy: agents as interns who need supervision. \"Managing AI interns with types. I can live with that.\"\n\n<iframe\n  width=\"560\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/uD_Cs0KE1Ns?si=p-5Ksj2h09S6vBKw\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  referrerpolicy=\"strict-origin-when-cross-origin\"\n  allowfullscreen\n  className=\"max-w-full rounded-lg border border-border-1 w-full\"\n></iframe>\n\n## Evals: Skeptical, Then Sold\n\nMatt didn&apos;t hide it: \"Evals are usually fluff. No one shows how to make them useful.\"\n\nSo we showed a working eval setup inside a live workflow. A test agent runs tasks. A judge agent scores output. The score controls retries or path changes.\n\nHe tested edge cases. The system held up. \"Okay, this is interesting,\" Matt said. \"You&apos;re checking behavior, not just execution.\"\n\n<iframe\n  width=\"560\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/rmc0v4Fy5pI?si=YTeOkYlqPPxIVWxn\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  referrerpolicy=\"strict-origin-when-cross-origin\"\n  allowfullscreen\n  className=\"max-w-full rounded-lg border border-border-1 w-full\"\n></iframe>\n\n## Healthy Skepticism Meets Healthy Patterns\n\n![Matt Pocock meets Mastra review](/images/blog/matt-pocock-meets-mastra/matt-pocock-mastra-review.png)\n\nMatt summed it up: \"You're not selling snake oil. You're selling tools. And tools I can debug, extend, and trust? That&apos;s a win.\"\n\nShoutout to Matt for being a good sport and asking us the tough questions."
  },
  {
    "metadata": {
      "title": "Announcing Mastra 0.10",
      "publishedAt": "2025-05-20",
      "summary": "Deploy Mastra anywhere",
      "author": "Sam Bhagwat",
      "draft": false,
      "categories": ["announcements"]
    },
    "slug": "mastra-0.10",
    "content": "Breaking changes merit breaking news. We are shipping a new version of Mastra with a handful of changes to simplify things, adjust naming conventions, and ensure that people can deploy Mastra anywhere. Announcing Mastra 0.10.\n\nThe following changes are going out today (May 21st) and are not backwards-compatible with previous Mastra releases (hence 'breaking' changes). Let’s dig in…\n\n## vNext workflows\n\nWe&apos;ve given workflows [a major overhaul](https://github.com/mastra-ai/mastra/tree/main/packages/core/src/workflows/vNext) based on user feedback. vNext will feature several improvements that largely address 3 big needs: stronger control flow, better type safety, and multi-engine support. (We also wrote a whole blogpost on vNext [here](https://mastra.ai/blog/vNext-workflows).)\n\n#### Before\n<hr/>\n\nPreviously you&apos;d get less type safety and have to access `inputValue` through the context, casting types for IDE support\n\n```tsx\nconst logCatName = new Step({\n  id: \"logCatName\",\n  outputSchema: z.object({\n    rawText: z.string(),\n  }),\n  execute: async ({ context }) => {\n    const name = context?.getStepResult<{ name: string }>(\"trigger\")?.name;\n    console.log(`Hello, ${name} 🐈`);\n    return { rawText: `Hello ${name}` };\n  },\n});\n\nexport const logCatWorkflow = new Workflow({\n  name: \"log-cat-workflow\",\n  triggerSchema: z.object({\n    name: z.string(),\n  }),\n});\n\nlogCatWorkflow.step(logCatName).commit();\n```\n\n#### After\n\n<hr/>\n\nNow you get strong typing and can access `inputData` easily, which means better IDE support, stronger control flow and an improved dev experience\n\n```tsx\nconst logCatName = createStep({\n  id: \"logCatName\",\n  inputSchema: z.object({\n    name: z.string(),\n  }),\n  outputSchema: z.object({\n    rawText: z.string(),\n  }),\n  execute: async ({ inputData }) => {\n    console.log(`Hello, ${inputData.name} 🐈`);\n    return { rawText: `Hello ${inputData.name}` };\n  },\n});\n\nexport const logCatWorkflow = createWorkflow({\n  id: \"log-cat-workflow\",\n  inputSchema: z.object({\n    name: z.string(),\n  }),\n  outputSchema: z.object({\n    rawText: z.string(),\n  }),\n  steps: [logCatName],\n})\n  .then(logCatName)\n  .commit();\n```\n\n## Vector Store Changes\n\nIf your application uses any of our vector stores, you'll need to update your code to match the new API. These changes are designed to make the API more consistent, improve type safety, and provide better developer experience.\n\n### Positional Arguments\n\nAll vector stores that previously used positional arguments now use object parameters instead for public facing vector functions. This change makes the API more consistent and less error-prone.\n\n#### Before\n\n<hr/>\n<br/>\n\n```tsx\nawait vectorDB.createIndex(indexName2, 3, \"cosine\");\nawait vectorDB.upsert(indexName, [[1, 2, 3]], [{ test: \"data\" }]);\nawait vectorDB.query(indexName, [1, 2, 3], 5);\n```\n\n#### After\n\n<hr/>\n<br/>\n\n```tsx\nawait vectorDB.createIndex({\n  indexName: indexName2,\n  dimension: 3,\n  metric: \"cosine\",\n});\n\nawait vectorDB.upsert({\n  indexName,\n  vectors: [[1, 2, 3]],\n  metadata: [{ test: \"data\" }],\n});\n\nawait vectorDB.query({\n  indexName,\n  queryVector: [1, 2, 3],\n  topK: 5,\n});\n```\n\n### Method Renaming\n\nThe methods `updateIndexById` and `deleteIndexById` have been renamed to `updateVector` and `deleteVector` respectively to better reflect their purpose.\n\n#### Before\n\n<hr/>\n<br/>\n\n```tsx\nawait vectorDB.updateIndexById(testIndexName, idToBeUpdated, update);\n```\n\n#### After\n\n<hr/>\n<br/>\n```tsx\nawait vectorDB.updateVector({\n  indexName: testIndexName,\n  id: idToBeUpdated,\n  update,\n});\n```\n\n## PG Vector Specific Changes\n\n### Constructor Changes\n\nThe string constructor for PGVector has been removed in favor of object parameters, providing a more consistent and type-safe API.\n\n#### Before\n\n<hr/>\n<br/>\n\n```tsx\nconst pgVector = new PgVector(process.env.POSTGRES_CONNECTION_STRING!);\n```\n\n#### After\n\n<hr/>\n<br/>\n```tsx\nconst pgVector = new PgVector({\n  connectionString: process.env.POSTGRES_CONNECTION_STRING,\n});\n```\n\n### Removed Methods\n\nThe `defineIndex` method has been removed from PGVector.\n\n#### Before\n\n<hr/>\n<br/>\n```tsx\nawait vectorDB.defineIndex(indexName, \"cosine\", { type: \"flat\" });\n```\n\n#### After\n\n<hr/>\n<br/>\n```tsx\nawait vectorDB.buildIndex({\n  indexName: indexName,\n  metric: \"cosine\",\n  indexConfig: { type: \"flat\" },\n});\n```\n\n### PG Storage Specific Changes\n\nThe `schema` parameter has been replaced with `schemaName` in the constructor for better clarity.\n\n#### Before\n\n<hr/>\n<br/>\n```tsx\nconst pgStore = new PostgresStore({\n  connectionString: process.env.POSTGRES_CONNECTION_STRING,\n  schema: customSchema,\n});\n```\n\n#### After\n\n<hr/>\n\n```tsx\nconst pgStore = new PostgresStore({\n  connectionString: process.env.POSTGRES_CONNECTION_STRING,\n  schemaName: customSchema,\n});\n```\n\n## `@mastra/memory 0.3.x -> 0.10.0`\n\nIf your application relies on the previous default Memory configuration, uses semantic recall, or utilizes working memory, you'll need to update your code. The goal of these changes is to reduce package size, make configuration more explicit, and lower resource usage and latency.\n\n### Default Embedder\n\nThe default embedder previously included in `@mastra/memory` has been moved to a separate package, `@mastra/fastembed`. This change reduces package size and addresses issues with deploying to platforms like Vercel and Cloudflare due to the built-in embedder's large dependencies.\n\nIf you were relying on the default embedder, you now need to explicitly configure an embedder.\n\n#### Before\n\n<hr/>\n<br/>\n```tsx\nimport { Memory } from \"@mastra/memory\";\nconst memory = new Memory({\n  // this was the implicit default\n  // embedder: fastembed\n});\n```\n\n#### After\n<hr/>\nTo continue using the FastEmbed model, first install the package `npm install @mastra/fastembed`, then add it to your code:\n\n  ```tsx\nimport { Memory } from \"@mastra/memory\";\nimport { fastembed } from \"@mastra/fastembed\";\n\nconst memory = new Memory({\n  embedder: fastembed,\n});\n```\n\nOr use [another embedder](https://ai-sdk.dev/docs/foundations/overview#embedding-models) like OpenAI or Gemini:\n\n```tsx\nimport { Memory } from \"@mastra/memory\";\nimport { openai } from \"@ai-sdk/openai\";\n\nconst memory = new Memory({\n  embedder: openai.embedding(\"text-embedding-3-small\"),\n});\n```\n\n### Default Storage and Vector Adapters\n\nDefault storage and vector adapters have been removed from Memory. You now need to configure these explicitly. Memory will also inherit the storage configuration from your Mastra instance if it's omitted. This change makes the configuration more explicit and helps avoid unexpected behavior when deploying to platforms like Cloudflare.\n\n#### Before\n\n<hr/>\n\nLibSQL was previously used for both storage and vector adapters by default:\n\n```tsx\nimport { Memory } from \"@mastra/memory\";\nconst memory = new Memory({\n  // these were added by default\n  // storage: new LibSQLStore(),\n  // vector: new LibSQLVector(),\n});\n```\n\n#### After\n\n<hr/>\n\nNow you need to explicitly define them:\n\n```tsx\nimport { Memory } from \"@mastra/memory\";\nimport { LibSQLStore, LibSQLVector } from \"@mastra/libsql\";\n\n// Option 1: Set storage and vector directly on memory\nconst memory = new Memory({\n  storage: new LibSQLStore(),\n  vector: new LibSQLVector(),\n  options: {\n    semanticRecall: true,\n  },\n});\n\n// Option 2: Let memory inherit storage from the Mastra instance, but configure vector explicitly if you're using semanticRecall\nimport { Mastra } from \"@mastra/core\";\n\nconst mastra = new Mastra({\n  storage: new LibSQLStore(),\n});\n\nconst memory = new Memory({\n  vector: new LibSQLVector(),\n  options: {\n    semanticRecall: true,\n  },\n});\n```\n\nIf you're not using `semanticRecall` you don't need to configure a vector store.\n\n### Working Memory\n\nWorking memory now uses tool calls by default instead of adding content to the text response. This change improves compatibility with data streaming and provides a more structured way for models to update working memory. It's especially important if you're using `toDataStream()` as the previous `text-stream` mode was difficult to use with data streaming.\n\n#### Before\n\n<hr/>\n<br/>\n```tsx\nimport { Memory } from \"@mastra/memory\";\n\nconst memory = new Memory({\n  options: {\n    workingMemory: {\n      enabled: true,\n      use: \"text-stream\", // when working memory was enabled this was the default\n      template: `...`,\n    },\n  },\n});\n```\n\n#### After\n\n<hr/>\nNow when `workingMemory` is enabled, the only option is the previous \"use: `tool-calls`\" behaviour.\n\n```tsx\nimport { Memory } from \"@mastra/memory\";\nconst memory = new Memory({\n  options: {\n    workingMemory: {\n      enabled: true,\n      template: `...`,\n    },\n  },\n});\n```\n\n### Implicit Memory Settings\n\nDefault settings have been changed to more reasonable defaults as the previous defaults were often surprising for users.\n`lastMessages` was previously set to 40 and is now lowered to 10.\n`semanticRecall` was enabled by default and now must be explicitly turned on.\n`generateTitle` was enabled by default and would trigger an additional call to the LLM on the first user message, requesting the LLM to generate a conversation thread title. This introduced additional latency and it wasn't obvious why. It's now disabled by default.\n\n#### Before\n\n<hr/>\n\nThese were the implicit defaults:\n\n```tsx\nimport { Memory } from \"@mastra/memory\";\n\nconst memory = new Memory({\n  options: {\n    lastMessages: 40,\n    semanticRecall: {\n      topK: 2,\n      messageRange: 2,\n    },\n    threads: {\n      generateTitle: true,\n    },\n  },\n});\n```\n\n#### After\n\n<hr/>\n\nThese are now the implicit defaults (if not specified):\n\n```tsx\nimport { Memory } from \"@mastra/memory\";\n\nconst memory = new Memory({\n  options: {\n    lastMessages: 10,\n    semanticRecall: false,\n    threads: {\n      generateTitle: false,\n    },\n  },\n});\n```\n\n## `@mastra/core 0.9.x -> 0.10.0`\n\nWe aimed to provide good defaults for @mastra/core to ensure users had an excellent out-of-the-box experience. We chose SQLite as the default storage solution for tracings, evals, memory, and Pino as the default logger. Initially, this approach worked well, allowing users to quickly set up Mastra for development and utilize a fully functional agent and workflow. However, it soon became evident that these defaults posed challenges when users attempted to deploy Mastra agents to production and cloud providers.\n\n### No more default storage\n\nYou need to install a storage provider and connect it to the Mastra instance. Our `npm create mastra` experience will include `@mastra/libsql`by by default.\n\n#### Before\n\n<hr/>\n<br/>\n```tsx\nimport { Mastra } from \"@mastra/core\";\nimport { weatherAgent } from \"./agents\";\n\nexport const mastra = new Mastra({\n  agents: {\n    weatherAgent,\n  },\n});\n```\n\n#### After\n<hr/>\n\nInstall any supported storage provider and hook it into the Mastra instance.\n\n\n\n```tsx\nimport { Mastra } from \"@mastra/core\";\nimport { LibSQLStore } from \"@mastra/libsql\";\nimport { weatherAgent } from \"./agents\";\n\nconst storage = new LibSQLStore({\n  url: \"file:./mastra.db\",\n});\n\nexport const mastra = new Mastra({\n  agents: {\n    weatherAgent,\n  },\n  storage,\n});\n```\n\n### Removal of default Pino logger\n\nMastra is using Pino as a default logger, the fastest and best logger for production use. Some providers, like Cloudflare, do not play well with Pino. We’ve decided to remove Pino as the default logger in Mastra using the native `console logger,` meaning the log format will change. Our recommended logger for environments like Vercel/Netlify or Mastra Cloud is Pino. Mastra Cloud will ship Pino as a default, similar to our getting started experience with `create mastra`\n\n#### Before\n\n<hr/>\n<br/>\n\n```jsx\nimport { Mastra } from \"@mastra/core\";\nimport { weatherAgent } from \"./agents\";\n\nconst memory = new Mastra({\n  agents: {\n    weatherAgent,\n  },\n});\n```\n\n#### After\n<hr/>\n\nInstall any supported logger and hook it into the Mastra instance:\n\n\n\n```jsx\nimport { Mastra } from \"@mastra/core\";\nimport { weatherAgent } from \"./agents\";\nimport { PinoLogger } from \"@mastra/loggers\";\n\nconst memory = new Mastra({\n  agents: {\n    weatherAgent,\n  },\n  logger: new PinoLogger({ name: \"Mastra\", level: \"debug\" }),\n});\n```\n\n### `@mastra/core` moved to Peer Dependencies\n\nWe've noticed that our users often have multiple @mastra/core packages when upgrading their Mastra dependencies or when dealing with mixed versions of mastra components. By moving @mastra/core to be a peer dependency across all packages, we ensure that the only version of mastra will be in the project itself, which should already be the case.\n\n### `@mastra/core` default import will only include Mastra\n\nMultiple warnings have been added when importing components directly from @mastra/core instead of using a subpath. We will be removing all exports except for the Mastra class itself. This change ensures that embedding Mastra into an existing application will only utilize the necessary components.\n\n#### Before\n\n<hr/>\n<br/>\n\n```jsx\nimport { Mastra, MastraStorage } from \"@mastra/core\";\nimport { weatherAgent } from \"./agents\";\n\nclass CustomStorage extends MastraStorage {}\n\nconst memory = new Mastra({\n  agents: {\n    weatherAgent,\n  },\n});\n```\n\n#### After\n\n<hr/>\n<br/>\n```jsx\nimport { Mastra } from \"@mastra/core\";\nimport { MastraStorage } from \"@mastra/core/storage\";\n\nclass CustomStorage extends MastraStorage {}\n\nconst memory = new Mastra({\n  agents: {\n    weatherAgent,\n  },\n});\n```\n\n## `@mastra/deployer 0.3.x -> 0.10.0`\n\nWe are removing the deploy command from our deployers. Previously, you could deploy a project using `mastra deploy,` which would trigger the vendor's CLI to run the deployment. Most vendors build on each git commit and use their own tools for building and deploying. This change removes redundant tasks and simplifies the deployment process."
  },
  {
    "metadata": {
      "title": "Mastra Changelog 2025-05-15",
      "publishedAt": "2025-05-15",
      "summary": "Mastra's latest updates: A2A support, MCP updates, and more.",
      "author": "Shane Thomas",
      "draft": false,
      "categories": ["changelogs"]
    },
    "slug": "changelog-2025-05-15",
    "content": "The team has shipped a ton of updates in the last week or so, including A2A support, MCP updates, and more. Let’s dig in…\n\n## Support for A2A\n\nMastra now supports direct agent communication using an Agent-to-Agent (A2A) protocol based on Google’s A2A standard and JSON-RPC 2.0. Agents can now send messages, manage tasks, and interact with each other natively within the framework.\n\nHere’s a simple code example demonstrating how to use the new Agent-to-Agent (A2A) communication in Mastra:\n\n```tsx\nimport { A2A } from \"@mastra/client-js\";\n\n// Initialize the A2A client\nconst a2a = new A2A({ serverUrl: \"https://your-mastra-server.com\" });\n\n// Retrieve an agent card (info about another agent)\nconst agentCard = await a2a.getAgentCard(\"agent-id-123\");\n\n// Send a message to another agent\nawait a2a.sendMessage({\n  to: \"agent-id-123\",\n  from: \"my-agent-id\",\n  content: \"Hello from my agent!\",\n});\n\n// Create and manage a task for another agent\nconst task = await a2a.createTask({\n  agentId: \"agent-id-123\",\n  taskType: \"processData\",\n  payload: { data: [1, 2, 3] },\n});\n\n// Optionally, stream task updates\nconst stream = a2a.streamTaskUpdates(task.id, (update) => {\n  console.log(\"Task update:\", update);\n});\n```\n\nThis example shows how to initiative the A2A client, retrieve information about another agent, send a message, create a task for another agent, and stream updates for that task.\n\n## MCP Updates\n\n### **New MCP Tool Compatibility Layer**\n\nMastra now includes a tool compatibility layer to smooth out differences between models from various providers. Some models don’t fully support all zod or JSON schema properties, which can cause issues when using tools built for a different provider.\n\nWith this update, Mastra automatically adapts tool schemas to match the target model’s capabilities, removing unsupported properties and appending key instructions to the tool description. This ensures tools work consistently, no matter which model you use.\n\nCompatibility classes are included for Google Gemini, Anthropic, OpenAI (including reasoning models), DeepSeek, and Meta.\n\n### Pass MCP servers to the Mastra class\n\nWe now support passing MCP servers directly to the Mastra class, making it easier to configure and manage your server connections.\n\nThis update also introduces new API endpoints for posting messages and adds Server-Sent Events (SSE) capabilities for real-time communication with MCP servers. Importantly, these enhancements maintain compatibility with older, deprecated MCP specifications, ensuring a smooth transition for existing projects.\n\n### getResources()\n\nThe MCPClient class now includes a new `getResources()` method, allowing clients to easily retrieve resources from connected MCP servers.\n\nThis update also comes with improved documentation and expanded tests, all while maintaining full backward compatibility with existing implementations.\n\n## Pass workflows directly to agents\n\nYou can now pass workflows directly to agents in Mastra. This makes it easier to define and assign custom workflows for your agents at runtime, giving you more flexibility and control over agent behavior.\n\nFor example:\n\n```tsx\nconst myWorkflow = {\n  steps: [\n    { type: \"task\", name: \"fetchData\" },\n    { type: \"task\", name: \"processData\" },\n  ],\n};\n\nconst agent = new Mastra.Agent({\n  name: \"MyAgent\",\n  workflow: myWorkflow,\n});\n```\n\n## Other updates\n\n- We added Klavis AI to the MCP registry list, expanding the ecosystem of verified MCP server providers for AI applications. (https://github.com/mastra-ai/mastra/pull/4201)\n- Mastra now handles jsonScheme implementations on Vercel tools ([#4200](https://github.com/mastra-ai/mastra/pull/4200))\n\nAs always, you can find the complete release logs [here](https://github.com/mastra-ai/mastra/releases).\n\nHappy building 🚀"
  },
  {
    "metadata": {
      "title": "AI prompting techniques",
      "publishedAt": "2025-05-13",
      "summary": "Learn about effective AI prompting techniques, including types of prompts, handling their output, and evals.",
      "author": "Sam Bhagwat",
      "draft": false,
      "categories": ["foundations"]
    },
    "slug": "ai-prompting-techniques",
    "content": "# AI prompting techniques\n\nOne of the foundational skills in AI engineering is writing effective prompts. Whether you're building agents, implementing RAG pipelines, \nor designing multi-step workflows, writing great prompts helps LLMs follow instructions and produce reliable outputs.\n\n## Types of AI prompts\nThere are several common prompt formats used in AI development, each offering a different level of control over the model’s output. The most \nbasic is **zero shot prompting**, where you simply ask a question without giving the model any examples. \n\n**Single-shot prompting** and **few-shot prompting** build on that by giving the model example inputs and outputs which improve structure, \ntone, and reliability.\n\nYou can also guide the model’s behavior more directly, influencing role or personality, using **system prompts**.\n\n## Zero shot prompting\n\nZero-shot prompting gives the LLM maximum freedom in its response. You prompt the LLM and hope for the best.\n\nThis approach works for simple queries but offers limited control over the format and quality. It's the fastest but least reliable method.\n\n**Prompt example:**\n```\nExplain how recursion works in programming.\n```\n\n## Single shot prompting\n\nSingle shot prompts provide one example with input and output. \n\nSingle shot prompting establishes a pattern the model can follow, making it more reliable—especially when format matters.\n\n**Prompt example:**\n\n```\nSummarize these paragraphs in one sentence:\n\nInput: The study examined the effects of caffeine on cognitive performance. Participants who consumed caffeine showed improved reaction times and better focus on attention-based tasks compared to the control group. However, these benefits diminished after 4-5 hours.\n\nOutput: Caffeine temporarily improved reaction times and focus in study participants, with effects lasting 4-5 hours.\n\nInput: Recent advancements in renewable energy technology have led to significant cost reductions. Solar panel efficiency has increased while manufacturing costs have decreased by 75% over the last decade. This has made solar energy competitive with fossil fuels in many markets.\n\nOutput:\n```\n\n## Few shot prompting\n\nFew shot prompts give multiple examples for more precise control over the output. More examples = more guidance, \nbut also more expensive. Choose your approach based on how much precision you need.\n\n**Prompt example:**\n\n```\nClassify these sentences as positive, neutral, or negative:\n\nText: The service was quick and the staff was friendly.  \nSentiment: Positive\n\nText: It arrived on time but wasn't exactly what I expected.  \nSentiment: Neutral\n\nText: I waited an hour and the product was damaged.  \nSentiment: Negative\n\nText: The website was easy to navigate.  \nSentiment:\n```\n\n---\n\n## The Seed Crystal approach\n\nNot sure where to start? Ask the model to generate a prompt for you.\n\n```\nGenerate a prompt for requesting a picture of a dog playing with a whale.\n```\n\nThis gives you a solid V1 to refine. You can also ask the model how to improve it.  \n\nUse the same model you're prompting for best results (e.g., Claude for Claude, GPT-4 for GPT-4).\n\n---\n\n## System prompts\n\nWhen accessing models via API, they usually have the ability to set a system prompt, which gives the model characteristics that you want \nit to have. This will be in addition to the specific \"user prompt\" that gets passed in.\n\nYou can ask the model to answer the same question as different personas, like Steve Jobs or Harry Potter, or pretend you’re a cat expert assistant. \nHere’s an example of what that looks like in Mastra:\n\n\n```ts\nimport { openai } from \"@ai-sdk/openai\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { createTool } from \"@mastra/core/tools\";\nimport { z } from \"zod\";\n\nconst instructions = `You are a helpful cat expert assistant. When discussing cats, you should always include an interesting cat fact.\n\nYour main responsibilities:\n1. Answer questions about cats\n2. Use the catFact tool to provide verified cat facts\n3. Incorporate the cat facts naturally into your responses\n\nAlways use the catFact tool at least once in your responses to ensure accuracy.`;\n\nconst getCatFact = async () => {\n  const { fact } = await fetch(\"https://catfact.ninja/fact\").then((res) =>\n    res.json()\n  );\n  return fact;\n};\n\nconst catFact = createTool({\n  id: \"Get cat facts\",\n  inputSchema: z.object({}),\n  description: \"Fetches cat facts\",\n  execute: async () => {\n    console.log(\"using tool to fetch cat fact\");\n    return {\n      catFact: await getCatFact(),\n    };\n  },\n});\n\nconst catOne = new Agent({\n  instructions,\n  tools: {\n    catFact,\n  },\n});\n\nconst result = await catOne.generate(\"Tell me a cat fact\");\n\nconsole.log(result.text);\n```\n\nSystem prompts are useful for **tone and persona shaping**, but don’t usually improve **factual accuracy**.\n\n## Prompt formatting tricks\n\nAI models are sensitive to formatting. You can use this to your advantage:\n\n- Use **CAPITALIZATION** for emphasis on certain words \n- Use **XML-like structure** to provide a clear path for instruction-following \n- Use **structured sections** for clarity and detail (e.g., context, task, constraints)  \n\nTweak and iterate—small formatting changes can yield big results.\n\n## Example: A code generation prompt\n\nHere's a simple code generation prompt that utilizes formatting to improve the model’s output:\n\n```\n# CONTEXT\nYou are helping a developer build a React component.\n\n# TASK\nCreate a responsive navigation bar component.\n\n# REQUIREMENTS\n- Use functional components with hooks\n- Include mobile and desktop views\n- Support dark/light modes\n- Implement accessibility features\n\n# OUTPUT FORMAT\nProvide the complete code with comments explaining key parts.\n```\n\n## Prompting for AI agents\n\nWhen building AI applications, it's important to consider how you will handle prompt output as well as evaluate its performance. Mastra\nprovides structured output and evals to make this easy for developers building their first AI agent.\n\n### Structured prompt output\n\nBy default, LLMs return unstructured text as their repsonse. With Mastra, the output of your prompts is returned in a structured format instead of unstructured text. \nMastra supports [structured output](https://mastra.ai/en/docs/agents/overview#3-structured-output) in JSON or Zod.\n\n## Evaluating prompt performance\n\nMastra provides comprehensive eval capabilities. Each eval returns a normalized score between 0-1 that can be logged and compared, helping you measure \nthe quality of your prompts.\n\nMastra supports various eval metrics for assessing agent outputs, including metrics for answer relevancy, completeness, and prompt alignment. \n[Read more about Mastra evals](https://mastra.ai/en/docs/evals/overview).\n\n## Conclusion\n\nEffective prompt engineering is clear communication.\n\nStart simple. Iterate. Refine. Great prompts are rarely perfect on the first try.\n\nExperimenting with prompting techniques helps you get the most out of LLMs.\n\nFor a comprehensive introduction to building AI agents, download the book [Principles of Building AI Agents](https://mastra.ai/book)."
  },
  {
    "metadata": {
      "title": "Building an Interactive Detective Game with Multi-Agent AI",
      "publishedAt": "2025-05-10",
      "summary": "How we built an interactive detective game using Mastra's agent system and Next.js.",
      "author": "Shane Thomas",
      "draft": false,
      "categories": ["examples"]
    },
    "slug": "the-detective-game",
    "content": "We built an interactive detective game using Mastra's agent system and Next.js, where each suspect is a unique AI agent with its own personality, background, and potential motives.\n\nWe also used a smaller models (`gpt-4o-mini` in this case) for this use case. We found that smaller models are not only more cost-effective but also provide faster response times, which is crucial for maintaining engagement in an interactive game.\n\n## How the murder mystery works\n\n![Demo of the detective game showing an interview with a suspect](/detectivegame.gif)\n\nOur game revolves around the murder of Michael Reynolds, the CEO of TechVision Solutions. The player takes on the role of a detective who must interview five key suspects, each represented by a distinct AI agent:\n\n- Sarah Reynolds: The victim's sister and VP of Marketing\n- Marcus Chen: The victim's business partner\n- Diana Wilkins: A neighbor with a secret\n- Tom Johnson: The building manager\n- Alex Rivera: A local barista\n\nInterviews are conducted via a chat interface.\n\n## **Building Character-Driven Agents**\n\nThe key to creating an engaging detective game lies in developing believable characters. We used Mastra's Agent system to create distinct personalities for each suspect. Here's an example of how we structured our agents:\n\n```tsx\nexport const sarahReynoldsAgent = new Agent({\n  name: \"Sarah Reynolds\",\n\n  instructions: `You are Sarah Reynolds, the victim's sister (Role: Victim's Sister).\n\nYou are cooperative and have medium trust in the detective.\n\nBackground: Michael's younger sister and VP of Marketing at TechVision.\n\nNamed as beneficiary on Michael's recently updated life insurance policy worth $2 million.\n\nAlibi: Claims to have been at home alone during the time of murder.\n\nMotive: Financial gain from insurance policy; would inherit Michael's shares in the company.\n\nAnswer questions as Sarah, staying in character.`,\n\n  model: openai(\"gpt-4o-mini\"),\n});\n```\n\nEach agent is configured with:\n\n- A distinct personality trait (cooperative, defensive, nervous, etc.)\n- A trust level towards the detective\n- A detailed background\n- An alibi\n- A potential motive\n\n## **Implementing a Detective Interview System**\n\nWe implemented the interview system using Next.js API routes, where each interaction with a suspect is handled through a streaming response:\n\n```tsx\nexport async function POST(req: Request) {\n  const { message, personId }: RequestBody = await req.json();\n\n  let streamResult;\n\n  if (personId === \"sarah-reynolds\") {\n    streamResult = await sarahReynoldsAgent.stream(message);\n  }\n\n  // ... handle other agents\n\n  return streamResult.toDataStreamResponse();\n}\n```\n\nImplementing streaming responses creates a more natural conversation flow. Characters appear to \"think\" as they respond, making the interaction feel more authentic than if responses appeared all at once.\n\n## Improvements we’d make to future versions\n\nWe're considering several enhancements to make the game even more engaging:\n\n1. **Memory System**: Implementing agent memory to track what information has been revealed to the detective\n2. **Cross-Character Interactions**: Allowing agents to reference or contradict each other's statements\n3. **Evidence System**: Adding physical evidence that agents must respond to when confronted\n\n## Try the game yourself\n\nPlay The Detective Game [here](https://the-detective-game.vercel.app/).\n\nAlso, the source code for this example is available in our [GitHub repository](https://github.com/zinyando/the-detective-game).\n\nWe'd love to see what tweaks or additional interactive experiences you build with Mastra!\n\nHappy building 🚀"
  },
  {
    "metadata": {
      "title": "FlashGenius: instantly generate flashcards on any topic",
      "publishedAt": "2025-05-08",
      "summary": "How we vibecoded Flash Genius, an instant flashcard generator, in 1 hour",
      "author": "Sam Bhagwat",
      "draft": false,
      "categories": ["examples"]
    },
    "slug": "flash-genius",
    "content": "FlashGenius generates high-quality educational flashcards on any topic. If you’ve ever wanted to quickly create study materials for yourself or your students, FlashGenius is for you.\n\nFlashGenius was vibecoded in 1 hour on [the April 24, 2025 Mastra workshop](https://www.youtube.com/watch?v=rVWewDk4W0g).\n\nIt consists of a backend based on Mastra agents and workflows and a frontend that lets users instantly generate and review flashcards.\n\n![Demo of FlashGenius flashcard generator](/flashgenius.gif)\n\n## How it works\n\n### FlashGenius backend: a combination of Mastra agents and workflows\n\nAt the core of FlashGenius is a Mastra agent that specializes in generating flashcards. Here’s how it’s defined:\n\n```tsx\n// src/mastra/agents/flashcardAgent.ts\nimport { openai } from \"@ai-sdk/openai\";\nimport { Agent } from \"@mastra/core/agent\";\n\nexport const flashcardAgent = new Agent({\n  name: \"flashcard-generator\",\n  instructions: `\n    You are an expert educational content creator specializing in creating high-quality flashcards.\n    Your task is to generate informative and educational flashcards on any given topic.\n    ...\n    Return the flashcards as a structured array of objects with 'question' and 'answer' properties.\n    ...\n  `,\n  model: openai(\"gpt-4o\"),\n});\n```\n\nThis agent is given clear instructions to generate flashcards in a structured format, using OpenAI’s GPT-4o model.\n\nTo orchestrate the process, we use a Mastra workflow. The workflow takes user input (topic, difficulty, number of cards), calls the agent, and validates the output:\n\n```tsx\n// src/mastra/workflow/index.ts\nconst generateFlashcardsStep = new Step({\n  id: \"generateFlashcards\",\n  outputSchema: z.object({\n    flashcards: flashcardsArraySchema,\n  }),\n  execute: async (context: any) => {\n    const mastra = context.mastra;\n    const { topic, difficulty, count } = context.trigger;\n    const agent = mastra.getAgent(\"flashcardAgent\");\n    const result = await agent.generate(\n      `Generate ${count} flashcards about \"${topic}\" at ${difficulty} level. ...`,\n      { output: z.object({ flashcards: flashcardsArraySchema }) },\n    );\n    return { flashcards: result.object.flashcards };\n  },\n});\n\nexport const generateFlashcardsWorkflow = new Workflow({\n  name: \"generate-flashcards-workflow\",\n  triggerSchema: z.object({\n    topic: z.string(),\n    difficulty: z.enum([\"beginner\", \"intermediate\", \"advanced\"]),\n    count: z.number().min(1).max(20),\n  }),\n});\ngenerateFlashcardsWorkflow.step(generateFlashcardsStep).commit();\n```\n\nThe workflow ensures that the agent’s output is always a well-formed list of flashcards, ready for the frontend.\n\n### FlashGenius frontend\n\nThe frontend is a modern React app (using TypeScript), with a simple, clean UI. It uses React Router for navigation and Tailwind CSS for styling. Here’s a quick look at the main app structure:\n\n```tsx\n// src/App.tsx\n<Router>\n  <Header />\n  <main>\n    <Routes>\n      <Route path=\"/\" element={<Home />} />\n      <Route path=\"/create\" element={<Create />} />\n    </Routes>\n  </main>\n  <footer>...</footer>\n</Router>\n```\n\nThe key interaction happens on the “Create” page. When a user submits a topic and difficulty, the frontend calls the backend via a Mastra client SDK:\n\n```tsx\n// src/client.ts\nexport const generateFlashcards = async ({ topic, difficulty, cardCount }) => {\n  const agent = mastraClient.getAgent(\"flashcardAgent\");\n  const response = await agent.generate({\n    messages: [\n      {\n        role: \"user\",\n        content: `Generate ${cardCount} flashcards about \"${topic}\" at ${difficulty} level.`,\n      },\n    ],\n  });\n  // ...parse and return flashcards...\n};\n```\n\nThe frontend handles loading states, errors, and even falls back to mock data if the backend is unavailable, making for a smooth user experience.\n\n## Build your own flashcard app\n\nFlashGenius is a simple example of what you can build with Mastra. The agent/workflow pattern makes it easy to define AI-powered features and the frontend integration is straightforward.\n\n[Try the demo here](https://flashgenius.vercel.app/)\n\n[Find the source code here](https://github.com/mastra-ai/flashgenius)\n\n[Watch the full workshop here](https://youtu.be/rVWewDk4W0g)\n\nWe’d love to see what apps you build (or what tweaks you make to FlashGenius!)\n\nHappy building 🚀"
  },
  {
    "metadata": {
      "title": "Mastra Changelog 2025-05-07",
      "publishedAt": "2025-05-07",
      "summary": "Mastra's latest updates: vNext is live, AGUI integration, and linting.",
      "author": "Shane Thomas",
      "draft": false,
      "categories": ["changelogs"]
    },
    "slug": "changelog-2025-05-07",
    "content": "## vNext Workflows is fully launched 🎉\n\nWe officially switched over to vNext on May 6th, 2025. That means importing from **`@mastra/core/workflows`** will give you the **`vNext`** workflows.\n\nIf you’re using the legacy workflow system, and you want to upgrade Mastra versions but you’re not yet ready to switch over, you can use the **`legacy`** import and prefix, eg:\n\n```tsx\nimport { Step, Workflow } from '@mastra/core/workflows/legacy'\nimport { Mastra } from '@mastra/core/mastra'\nconst step1 = new Step(...)\nconst workflow = new Workflow(...).step(step1).commit();\n\nconst mastra = new Mastra({\n  // registering legacy workflows\n  legacy_workflows: {\n    myWorkflow: workflow\n  }\n})\n\n// accessing the workflow through the mastra class:\nmastra.legacy_getWorkflow('myWorkflow')\n```\n\nRead the full [blogpost](https://mastra.ai/blog/vNext-workflows) to learn all about vNext. Full docs [here](https://mastra.ai/en/docs/workflows-vnext/overview).\n\n## AGUI integration\n\nMastra now works with AGUI clients. You can stream messages, trigger tool calls, and manage agents using a new adapter layer.\n\nThis makes it straightforward to plug Mastra agents into AGUI-powered frontends.\n\n**What's new:**\n\n- Added an `AGUIAdapter` class for handling message streaming and tool invocations.\n\n- New `getAGUI()` helper for managing adapters.\n\n- Tests added for UUID generation and AGUI message conversion.\n\n## New: Comprehensive Linting for Mastra CLI\n\nWe’ve added a robust linting system to the Mastra CLI to help you catch configuration and dependency issues before they become problems.\n\nThe new linter includes rules for validating project setups and dependencies across different deployment platforms, making it easier to maintain healthy, production-ready projects.\n\nThat’s all for now. Find the full release notes [here](https://github.com/mastra-ai/mastra/releases).\n\nHappy building 🚀"
  },
  {
    "metadata": {
      "title": "Introducing Mastra Docs Chatbot: Your AI Documentation Assistant",
      "publishedAt": "2025-05-07",
      "author": "Shane Thomas",
      "summary": "Explore how our new docs chatbot leverages the Mastra MCP Docs server to enhance user experience and streamline access to documentation.",
      "draft": false,
      "categories": ["announcements"]
    },
    "slug": "introducing-docs-chatbot",
    "content": "![Docs Chatbot demo](/images/blog/DocsChatbot.gif)\n\nIn an effort to improve user experience and accessibility of our documentation, we are excited to \nintroduce an experimental feature: the Docs Chatbot. This makes it easy to help users find answers \nto their questions more efficiently by leveraging the [Mastra MCP Docs server](/blog/introducing-mastra-mcp).\n\n## The Challenge\n\nWe noticed a recurring issue: many users were asking questions on our Discord that were already answered in \nour documentation. However, finding these answers wasn't always straightforward. Our goal was to create a \nsolution that would make it easier for users to access the information they need without having to sift \nthrough extensive documentation manually.\n\n## The Solution: Docs Chatbot\n\nThe Docs Chatbot is a new feature that utilizes our Mastra MCP Docs server to retrieve information from our \ndocumentation, examples, and blog posts. This agent is deployed on Mastra Cloud and interacts with our docs \nwebsite through the Mastra Client, providing a seamless and efficient user experience.\n\n### Key Features\n\n- **CopilotKit Integration**: We have integrated CopilotKit for the chat interface, ensuring a smooth and user-friendly chat experience.\n- **Real-time Information Retrieval**: The chatbot can access the complete Mastra documentation, code examples, and blog posts, providing users with accurate and up-to-date information.\n- **Experimental and Community-Driven**: While it's currently labeled as experimental, we are eager to receive feedback from the community while continuing to make improvements.\n\n## How we built it\n\nFirst we set up the MCP Client to connect to the Mastra docs server:\n\n```tsx\n// /src/mastra/mcp-config.ts\nimport { MCPClient } from \"@mastra/mcp\";\n\n// Create an MCP configuration for the Mastra docs server\nexport const docsMcp = new MCPClient({\n  id: \"mastra-docs\", // Unique identifier to prevent memory leaks\n  servers: {\n    mastraDocs: {\n      // Using npx to run the Mastra docs server\n      command: \"npx\",\n      args: [\"-y\", \"@mastra/mcp-docs-server@latest\"],\n    },\n  },\n});\n```\n\nNext we set up the Mastra agent that uses the MCP Client:\n\n```tsx\n// /src/mastra/agents/docs-agent.ts\nimport { openai } from '@ai-sdk/openai';\nimport { Agent } from '@mastra/core/agent';\nimport { docsMcp } from '../mcp-config';\nimport { linkCheckerTool } from '../tools/link-checker';\n\nconst tools = await docsMcp.getTools();\n\nexport const docsAgent = new Agent({\n  name: 'docsAgent',\n  instructions:\n    // Persistence reminder - ensures the model keeps going through multi-step process\n    'You are a helpful assistant specialized in Mastra documentation and usage. ' +\n    \"You are an agent - please keep going until the user's query is completely resolved, before ending your turn and yielding back to the user. Only terminate your turn when you are sure that the problem is solved. \" +\n    // Tool-calling reminder - encourages proper use of available tools\n    'You have access to the complete Mastra documentation, code examples, blog posts, and package changelogs through your tools. ' +\n    \"If you are not sure about specific Mastra features, documentation, or codebase structure pertaining to the user's request, use your tools to search and gather the relevant information: do NOT guess or make up an answer. \" +\n    // Planning reminder - ensures thoughtful approach before tool use\n    'You MUST plan extensively before each tool call, and reflect extensively on the outcomes of the previous tool calls. This will help you provide more accurate and helpful information. ' +\n    \"Don't answer questions about Mastra that are not related to the documentation or codebase. If you are not sure about the user's question, say so. \" +\n    \"Don't answer questions unrelated to Mastra. If you are not sure about the user's question, say so. \" +\n    // [check repo for full instructions...]\n    \"\",\n  model: openai('gpt-4.1'),\n  tools: {\n    ...tools,\n    linkCheckerTool,\n  },\n});\n```\n\nThen we set up the CopilotKit UI. We started with setting up the API route:\n\n```tsx\n// /src/app/api/copilotkit/route.ts\nimport {\n  CopilotRuntime,\n  ExperimentalEmptyAdapter,\n  copilotRuntimeNextJSAppRouterEndpoint,\n} from \"@copilotkit/runtime\";\nimport { NextRequest } from \"next/server\";\nimport { MastraClient } from \"@mastra/client-js\";\n\nconst baseUrl = process.env.MASTRA_AGENT_URL || \"http://localhost:4111\";\n\nconst client = new MastraClient({\n  baseUrl,\n});\n\nexport const POST = async (req: NextRequest) => {\n  const runtime = new CopilotRuntime({\n    agents: await client.getAGUI({ resourceId: \"docsAgent\" }),\n  });\n\n  const { handleRequest } = copilotRuntimeNextJSAppRouterEndpoint({\n    runtime,\n    serviceAdapter: new ExperimentalEmptyAdapter(),\n    endpoint: \"/api/copilotkit\",\n  });\n\n  return handleRequest(req);\n};\n\n```\n\nNow we could set up the ChatWidget. You can view the full [ChatWidget source code](https://github.com/mastra-ai/mastra/blob/main/docs/src/chatbot/components/chat-widget.tsx).\n\n```tsx\n// /src/chatbot/components/chat-widget.tsx\nconst DocsChat: React.FC<{\n  setIsAgentMode: (isAgentMode: boolean) => void;\n  searchQuery: string;\n}> = ({ setIsAgentMode, searchQuery }) => {\n  return (\n    <CopilotKit\n      runtimeUrl=\"/api/copilotkit\"\n      showDevConsole={false}\n      // agent lock to the relevant agent\n      agent=\"docsAgent\"\n    >\n      <CustomChatInterface\n        setIsAgentMode={setIsAgentMode}\n        searchQuery={searchQuery}\n      />\n    </CopilotKit>\n  );\n};\n```\n\n## Try it out!\n\nPlease try out the Docs Chatbot and share your thoughts with us on [Discord](https://discord.gg/BTYqqHKUrf)."
  },
  {
    "metadata": {
      "title": "Mastra Changelog 2025-05-01",
      "publishedAt": "2025-05-01",
      "summary": "Mastra's latest updates: vNext workflows, MongoDB vectorDB provider, Streamable HTTP MCP transport, and more.",
      "author": "Shane Thomas",
      "draft": false,
      "categories": ["changelogs"]
    },
    "slug": "changelog-2025-05-01",
    "content": "Our latest release features major workflow updates, support for MongoDB, support for the newest MCP transport type, and more.\n\n## vNext workflows: the next generation of Mastra workflows\n\nIn case you missed it in our [changelog](https://mastra.ai/blog/changelog-2025-04-24#vnext-workflows-new-control-flow-multi-engine-workflows-and-more) from last week, we’ve given workflows [a major overhaul](https://github.com/mastra-ai/mastra/tree/main/packages/core/src/workflows/vNext) based on user feedback. On Wednesday we released **`vNext`** workflows in alpha; now, we’re releasing **`vNext`** in stable (0.9.1).\n\nvNext will feature several improvements that largely address 3 big needs: stronger control flow, better type safety, and multi-engine support.\n\nHere’s a code sample that shows the new syntax:\n\n```tsx\nworkflow\n  .then(fetchWeather)\n  .branch([\n    [\n      async ({ inputData }) => {\n        return inputData?.rainChance > 0.5;\n      }, // conditional\n      planIndoorActivities, // step or workflow\n    ],\n    [\n      async ({ inputData }) => {\n        return inputData?.rainChance <= 0.5;\n      },\n      planActivities, // step or workflow\n    ],\n  ])\n  .commit();\n```\n\nHere are some of the biggest improvements we’ve made around control flows:\n\n- Nested Workflows are now first-class citizens and the first primitive to reach for when composing any nested control flows or circular execution structures\n- Looping (**`while`** or **`until`**) accepts a single Step or Workflow and repeats until conditions are met\n- **`.branch()`** replaces if/else, providing clearer conditional paths. Each truthy condition executes in parallel. And **`.parallel()`** for condition-less parallel execution path branching.\n  - Branching creates a visual mental model of forking paths in a tree, which accurately represents workflow conditions\n- **`.then()`** is now the universal connector (**`.step()`** has been retired)\n- Mastra primitives, like agents and tools, can be transformed to workflow steps using **`createStep()`**\n- We no longer have the **`.after()`** command; users reported the loop-back syntax was hard to reason about. Now, in vNext, branching and dependencies are encapsulated in child workflows and explicit control flow changes.\n\n### Transitioning from legacy to vNext\n\nNext week, we’ll be switching over to vNext during our next official release on May 6th, 2025. That means importing from **`@mastra/core/workflows`** will give you the **`vNext`** workflows.\n\nIf you’re using the legacy workflow system, and you want to upgrade Mastra versions but you’re not yet ready to switch over, you can use the **`legacy`** import and prefix, eg:\n\n```tsx\nimport { Step, Workflow } from '@mastra/core/workflows/legacy'\nimport { Mastra } from '@mastra/core/mastra'\nconst step1 = new Step(...)\nconst workflow = new Workflow(...).step(step1).commit();\n\nconst mastra = new Mastra({\n  // registering legacy workflows\n  legacy_workflows: {\n    myWorkflow: workflow\n  }\n})\n\n// accessing the workflow through the mastra class:\nmastra.legacy_getWorkflow('myWorkflow')\n```\n\nRead the full [blogpost](https://mastra.ai/blog/vNext-workflows) to learn all about vNext. Full docs [here](https://mastra.ai/en/docs/workflows-vnext/overview).\n\n## New vectorDB provider: MongoDB\n\nWe've [added](https://github.com/mastra-ai/mastra/pull/3823) MongoDB as a vector database provider, giving you another solid option for storing and retrieving vector embeddings in your applications. Mastra now has 12 vector DB integrations including Postgres/pgvector, libsql, Chroma, and many others.\n\nMongoDB has been pushing the envelope with vector search since early 2023 and we’re happy that you can now use MongoDB’s vector capabilities directly with Mastra.\n\n## Streamable HTTP MCP transport\n\nWe now [support](https://github.com/mastra-ai/mastra/pull/3834) Streamable HTTP as an MCP transport.\n\nPreviously, there were two ways to create an MCP server:\n\n1. Stdio transport (using a program running on your computer for the MCP tools)\n2. SSE transport (a web server hosted somewhere where you connect via URL)\n\nMCP [deprecated](https://github.com/modelcontextprotocol/modelcontextprotocol/pull/206) the latter (SSE) and replaced it with Streamable HTTP. We now support this new Streamable HTTP MCP transport type.\n\n## Other updates\n\n- We recently added Docker as a new MCP registry option to The MCP registry registry\n- We’re hosting the very first Mastra virtual hackathon, [MASTRA.BUILD](https://mastra.ai/hackathon), from May 12th-16h. Cash prizes and tons of hands-on building. [Find the schedule & registration on our Lu.ma](https://lu.ma/x637wtj0).\n\n[Find the full release log for the past week here](https://github.com/mastra-ai/mastra/releases/tag/%40mastra%2Fcore%400.9.1).\n\nHappy building and see you next week… 🚀"
  },
  {
    "metadata": {
      "title": "Introducing vNext Workflows: the next generation of Mastra Workflows",
      "publishedAt": "2025-04-29",
      "author": "Tony Kovanen",
      "summary": "vNext brings stronger control flow, better type safety, and multi-engine support.",
      "draft": false,
      "categories": ["announcements"]
    },
    "slug": "vNext-workflows",
    "content": "In case you missed it in our [changelog](https://mastra.ai/blog/changelog-2025-04-24#vnext-workflows-new-control-flow-multi-engine-workflows-and-more) from last week, we’ve given workflows [a major overhaul](https://github.com/mastra-ai/mastra/tree/main/packages/core/src/workflows/vNext) based on user feedback. On Wednesday we released `vNext` workflows in alpha; today, we’re releasing `vNext` in stable (0.10.0).\n\nWhile people really appreciated the fluent workflow syntax, they had two issues.\n\nFirst, it was too difficult to reason about and manage complex control flows.\n\nSecond, many users had existing workflow engines like Temporal or Inngest they wanted to integrate with.\n\nSo today, we’re excited to share vNext: a better, faster, stronger version of Mastra workflows. (We’d have started that with “[harder](https://www.youtube.com/watch?v=gAjR4_CbPpQ)”, but in this case it’s actually easier.)\n\n## What we’ve changed: better control flow, stronger type safety, and multi-engine support\n\nvNext will feature several improvements that largely address 3 big needs: stronger control flow, better type safety, and multi-engine support.\n\nHere’s a code sample that shows the new syntax:\n\n```tsx\nworkflow\n  .then(fetchWeather)\n  .branch([\n    // could use .parallel()\n    [\n      async ({ inputData }) => {\n        return inputData?.rainChance > 0.5;\n      }, // conditional\n      planIndoorActivities, // step or workflow\n    ],\n    [\n      async ({ inputData }) => {\n        return inputData?.rainChance <= 0.5;\n      },\n      planActivities, // step or workflow\n    ],\n  ])\n  .commit();\n```\n\n### Control flow\n\nHere are some of the biggest improvements we’ve made around control flows:\n\n- Nested Workflows are now first-class citizens and the first primitive to reach for when composing any nested control flows or circular execution structures\n- Looping (**`while`** or **`until`**) accepts a single Step or Workflow and repeats until conditions are met\n- **`.branch()`** replaces if/else, providing clearer conditional paths. Each truthy condition executes in parallel. And `.parallel()` for condition-less parallel execution path branching.\n  - Branching creates a visual mental model of forking paths in a tree, which accurately represents workflow conditions\n- **`.then()`** is now the universal connector (**`.step()`** has been retired)\n- Mastra primitives, like agents and tools, can be transformed to workflow steps using `createStep()`\n- We no longer have the `.after()` command; users reported the loop-back syntax was hard to reason about. Now, in vNext, branching and dependencies are encapsulated in child workflows and explicit control flow changes.\n\n### Type safety\n\nHere are some of the biggest improvements we’ve made around type safety:\n\n- All steps require input and output schemas\n- A step's input is:\n  - For the first step: the input provided to the workflow\n  - For subsequent steps: the output of the previous step\n- Parallel and branching operations return a union of the step results `{ [stepId]: output }`\n- Workflow outputs are defined as the final executed step's output\n- You can pass a resumeSchema argument for type safety when resuming a step\n  - The payload is passed into the execute function as resumeData\n  - Makes it easier to identify when a step is being resumed\n  - Helps separate inputs from previous steps and the resume context\n\n### Multi-engine workflows\n\nBesides having access to Mastra’s native, built-in workflows, soon you’ll also be able to swap-in other workflow engines — all while using the same top-level Mastra syntax.\n\nMany users came to Mastra with pre-existing workflows built on external engines. We’ve made it easy to plug those right into your projects or even switch between engines to see which works best.\n\n## Transitioning to vNext workflows\n\nNext week, we’ll be switching over to vNext during our next official release on May 6th, 2025. That means importing from `@mastra/core/workflows` will give you the `vNext` workflows.\n\nIf you’re using the legacy workflow system, and you want to upgrade Mastra versions but you’re not yet ready to switch over, you can use the `legacy` import and prefix, eg:\n\n```tsx\nimport { Step, Workflow } from '@mastra/core/workflows/legacy'\nimport { Mastra } from '@mastra/core/mastra'\nconst step1 = new Step(...)\nconst workflow = new Workflow(...).step(step1).commit();\n\nconst mastra = new Mastra({\n  // registering legacy workflows\n  legacy_workflows: {\n    myWorkflow: workflow\n  }\n})\n\n// accessing the workflow through the mastra class:\nmastra.legacy_getWorkflow('myWorkflow')\n```\n\n## What’s Next\n\nWe’ll be working with counterpart teams over vNext will support [Temporal](https://temporal.io/), [Inngest](https://www.inngest.com/), and [Cloudflare workers](https://workers.cloudflare.com/). We plan to add additional engines in the future.\n\nWe’re excited to see how vNext helps you build better workflows. Do share any sample projects and demos with us on [Discord](https://discord.gg/BTYqqHKUrf). And ping us anytime if you need help with the transition."
  },
  {
    "metadata": {
      "title": "Mastra Changelog 2025-04-24",
      "publishedAt": "2025-04-24",
      "summary": "Mastra's latest updates: dynamic agents, MCPServer support, vNext workflows, and more.",
      "author": "Shane Thomas",
      "draft": false,
      "categories": ["changelogs"]
    },
    "slug": "changelog-2025-04-24",
    "content": "In the last couple weeks, we shipped dynamic agents, MCP Server Support, vNext workflows, and more. Let’s dig in…\n\n## Dynamic agents: new runtime context\n\nMastra agents can now be customized on a per-user basis. In other words, you can let users specify which model your agent runs on or what toolset is available. You can also tailor your system prompt to your user.\n\nJust define a **`RuntimeContext`** and pass it to agent methods. You can set variables on the context and access them in your agent logic and tools:\n\n```tsx\nimport { RuntimeContext } from \"@mastra/core/di\";\n\n// Define the shape of your runtime context\ntype WeatherContext = {\n  \"temperature-scale\": \"celsius\" | \"fahrenheit\";\n};\n\nconst runtimeContext = new RuntimeContext<WeatherContext>();\nruntimeContext.set(\"temperature-scale\", \"celsius\");\n\nconst response = await agent.generate(\"What's the weather like today?\", {\n  runtimeContext,\n});\n```\n\nMore details on configuring `runtimeContext` found [here](https://mastra.ai/blog/dynamic-agents).\n\n## MCP Server Support\n\nYou can now convert any Mastra tools into MCP-compatible servers. You can also share your tools with any MCP client.\n\nHere’s how to get started:\n\n```tsx\n// stdio.ts\n#!/usr/bin/env node\n\nimport { MCPServer } from \"@mastra/mcp\";\nimport {\n  myFirstTool,\n  mySecondTool,\n  // ... more tools\n} from './tools';\n\nconst server = new MCPServer({\n  name: \"my-mcp-server\",\n  version: \"1.0.0\",\n  tools: {\n    myFirstTool,\n    mySecondTool,\n    // ... more tools\n  },\n});\n\nserver.startStdio().catch((error) => {\n  console.error(\"Error running MCP server:\", error);\n  process.exit(1);\n});\n```\n\nFor more tips on MCP servers, check out our [complete guide on deploying an MCPServer](https://mastra.ai/en/examples/agents/deploying-mcp-server). Investor Alana Goyal recently [used Mastra to build her own MCP Server](https://basecase.vc/blog/unpacking-mcp).\n\n## vNext Workflows: new control flow, multi-engine workflows, and more\n\nWe're giving workflows [a major overhaul](https://github.com/mastra-ai/mastra/tree/main/packages/core/src/workflows/vNext) based on user feedback. vPrevious (legacy workflows) made it too difficult to reason about and manage complex control flows.\n\nvNext features several improvements, many of which help to streamline control flows:\n\n- Nested Workflows are now first-class citizens and the first primitive to reach for when composing complex workflows\n- Looping (`while` or `until`) accepts a single Step or Workflow and repeats until conditions are met\n  - For infinite loops, you can loop a Nested Workflow from your \"main workflow\"\n- `.branch()` replaces if/else, providing clearer conditional paths. Each truthy condition executes in parallel.\n  - Branching creates a visual mental model of forking paths in a tree, which accurately represents workflow conditions\n- `.parallel()` for simple concurrent execution\n- `.then()` is now the universal connector (`.step()` has been retired)\n\nSoon you’ll also be able to swap-in other workflow engines (like [Temporal](https://temporal.io/), [Inngest](https://www.inngest.com/), and [Cloudflare workers](https://workers.cloudflare.com/)) while maintaining the same top-level Mastra syntax.\n\nFind the latest release timeline [here](https://discord.com/channels/1309558646228779139/1361964300511609014/1364473020625715231) and stay tuned for updates…\n\n## Other updates\n\n- We added Cloudflare D1 support to our storage engine. [#2932](https://github.com/mastra-ai/mastra/pull/2932)\n- We resolved bundling issues with workspace packages that use native dependencies by detecting such packages among externals, creating tarballs for them, and referencing them locally in package.json to enable proper installation during deployment. [#3602](https://github.com/mastra-ai/mastra/pull/3602)\n- We added CommonJS support in TypeScript for non-bundler environments, enabling easier migration of older projects to Mastra and adds end-to-end tests for these scenarios. [#3613](https://github.com/mastra-ai/mastra/pull/3613)\n- We updated Memory: Message ordering in @mastra/core. [#3672](https://github.com/mastra-ai/mastra/pull/3672)\n- All new messages are saved at least 1ms apart by adjusting `createdAt` timestamps. We added a function to correct previously misordered tool calls and included new tests. [#3654](https://github.com/mastra-ai/mastra/pull/3654)\n\n[Find the complete release log here](https://github.com/mastra-ai/mastra/releases/tag/%40mastra%2Fcore%400.9.0).\n\nThat’s it for now… Happy building 🚀"
  },
  {
    "metadata": {
      "title": "Building a Web Browsing Agent with Mastra and Stagehand",
      "publishedAt": "2025-04-23",
      "author": "Shane Thomas",
      "summary": "See how we built a web browsing agent using Mastra and Stagehand. Learn about the tools needed to allow your agents to control a browser.",
      "draft": false,
      "categories": ["examples"]
    },
    "slug": "web-browsing-agent",
    "content": "![Web Browsing Agent Demo buying items on Amazon](/images/blog/PersonalGiftShopper.gif)\n\n## Overview\n\nI didn't originally intend to automate all my gift shopping, but here we are. It started out with the goal of creating a Mastra agent capable of browsing the web using [Stagehand](https://github.com/browserbase/stagehand).\nThanks to some help from [Browserbase](https://browserbase.com), we just released an example that allows a Mastra agent to control a browser (and yes, the agent can do more than automate my personal shopping)!\n\n## How We Broke Down the Problem\n\nTo tackle this project, we decided to start by building the tools that would enable the agent to interact with web pages. We focused on creating a few core tools:\n\n1. **Web Action Tool**: This tool allows the agent to perform actions on a webpage, such as clicking buttons or filling out forms.\n\n   ```typescript\n   const performWebAction = async (url: string, action: string) => {\n     const stagehand = await createStagehand();\n     const page = stagehand.page;\n\n     try {\n       // Navigate to the URL\n       await page.goto(url);\n\n       // Perform the action\n       await page.act(action);\n\n       await stagehand.close();\n       return {\n         success: true,\n         message: `Successfully performed: ${action}`,\n       };\n     } catch (error: any) {\n       await stagehand.close();\n       throw new Error(`Stagehand action failed: ${error.message}`);\n     }\n   };\n   ```\n\n2. **Web Observation Tool**: This tool allows the agent to observe elements on a webpage to plan actions.\n\n   ```typescript\n   const performWebObservation = async (url: string, instruction: string) => {\n     const stagehand = await createStagehand();\n     const page = stagehand.page;\n\n     try {\n       // Navigate to the URL\n       await page.goto(url);\n\n       // Observe the page\n       const actions = await page.observe(instruction);\n\n       await stagehand.close();\n       return actions;\n     } catch (error: any) {\n       await stagehand.close();\n       throw new Error(`Stagehand observation failed: ${error.message}`);\n     }\n   };\n   ```\n\n3. **Web Extraction Tool**: This tool allows the agent to extract data from a webpage.\n\n   ```typescript\n   const performWebExtraction = async (\n     url: string,\n     instruction: string,\n     schemaObj: Record<string, any>,\n     useTextExtract?: boolean,\n   ) => {\n     const stagehand = await createStagehand();\n     const page = stagehand.page;\n\n     try {\n       // Navigate to the URL\n       await page.goto(url);\n\n       // Convert schema object to Zod schema\n       const schema = buildZodSchema(schemaObj);\n\n       // Extract data\n       const result = await page.extract({\n         instruction,\n         schema,\n         useTextExtract,\n       });\n\n       await stagehand.close();\n       return result;\n     } catch (error: any) {\n       await stagehand.close();\n       throw new Error(`Stagehand extraction failed: ${error.message}`);\n     }\n   };\n   ```\n\nWe tested each tool individually using the Mastra Dev Playground. This approach allowed us to ensure each tool worked correctly in isolation before integrating them into the agent.\n\n![Web Browsing Tools in Mastra Dev Playground](/images/blog/MastraDevPlaygroundBrowserTools.png)\n\n## Building the Agent\n\nWith the tools ready, we moved on to building the agent. We equipped the agent with the tools and began testing its ability to perform complex tasks on web pages. Here's the agent code with its instructions:\n\n```typescript\nexport const webAgent = new Agent({\n  name: \"Web Assistant\",\n  instructions: `\n      You are a helpful web assistant that can navigate websites and extract information.\n\n      Your primary functions are:\n      - Navigate to websites\n      - Observe elements on webpages\n      - Perform actions like clicking buttons or filling forms\n      - Extract data from webpages\n\n      When responding:\n      - Ask for a specific URL if none is provided\n      - Be specific about what actions to perform\n      - When extracting data, be clear about what information you need\n\n      Use the stagehandActTool to perform actions on webpages.\n      Use the stagehandObserveTool to find elements on webpages.\n      Use the stagehandExtractTool to extract data from webpages.\n  `,\n  model: openai(\"gpt-4o\"),\n  tools: { stagehandActTool, stagehandObserveTool, stagehandExtractTool },\n});\n```\n\nDuring testing, we found it useful to increase the `maxSteps` setting on our agent. This allowed the agent to perform more actions autonomously, improving its ability to handle complex tasks without additional input.\n\n## Conclusion\n\nBuilding a web browsing agent with Mastra and Stagehand was straightforward and required minimal code.\nWith these building blocks in place, we can now integrate this functionality into more complex multi-agent networks or agentic workflows.\n\nHere is the [source code](https://github.com/browserbase/Stagehand-Mastra-App) for the example. Let us know what browsing tasks your agents will be automating.\n\nHappy browsing!"
  },
  {
    "metadata": {
      "title": "Dynamic Agents: Inserting Runtime Context in Mastra",
      "publishedAt": "2025-04-22",
      "summary": "Mastra's dynamic agents provide a powerful way to handle context in AI applications, without exposing sensitive information to the LLM or relying on globals.",
      "author": "Sam Bhagwat",
      "draft": false,
      "categories": ["examples"]
    },
    "slug": "dynamic-agents",
    "content": "It's often the case that real-world agents need to be customized on a per-user basis: they may need access to user-specific data, API keys, or other runtime context.\n\nLet's say you want to let your user choose the model, or you want to customize the system prompt on a per-user basis, or customize an agent's toolsets on a per-request basis.\n\nPreviously Mastra only allowed you to define these fields statically. As of `mastra@0.9.0`, Mastra provides a runtime context system, which is a dependency injection pattern so you can pass configuration at runtime in a type-safe way.\n\n## Use cases\n\nSome things we've seen Mastra users do with runtime context so far:\n\n- Run user metadata through an LLM prompt to customize the system prompt for the agent responding to that user\n- Build several dozen templated agents for a multi-location hospitality chain, using location details to customize the system prompt and tool selection\n- Use a smaller model if a user was on a free plan and a larger one if they were on a paid plan\n- Give the agent a different set of tools based on the user's role\n\n## How RuntimeContext works\n\nMastra lets you define a `RuntimeContext` and pass it to agent methods. You can set variables on the context, and access them in your agent logic and tools.\n\n```typescript showLineNumbers copy\nimport { RuntimeContext } from \"@mastra/core/di\";\n\n// Define the shape of your runtime context\ntype WeatherContext = {\n  \"temperature-scale\": \"celsius\" | \"fahrenheit\";\n};\n\nconst runtimeContext = new RuntimeContext<WeatherContext>();\nruntimeContext.set(\"temperature-scale\", \"celsius\");\n\nconst response = await agent.generate(\"What's the weather like today?\", {\n  runtimeContext,\n});\n```\n\nYou can use `runtimeContext` in your agent configuration functions:\n\n```typescript showLineNumbers copy\nconst dynamicAgent = new Agent({\n  tools: ({ runtimeContext }) => {\n    // Use runtimeContext.get() to access variables\n    const temperatureScale = runtimeContext.get(\"temperature-scale\");\n    return temperatureScale === \"celsius\" ? celsiusTools : fahrenheitTools;\n  },\n  instructions: ({ runtimeContext }) => {\n    const temperatureScale = runtimeContext.get(\"temperature-scale\");\n    return `You are assisting with ${temperatureScale} temperature.`;\n  },\n  model: ({ runtimeContext }) => {\n    return runtimeContext.get(\"preferFast\")\n      ? openai(\"gpt-4o-mini\")\n      : openai(\"gpt-4o\");\n  },\n});\n```\n\nAnd in tools:\n\n```typescript showLineNumbers copy\nimport { createTool } from \"@mastra/core/tools\";\nimport { z } from \"zod\";\n\nexport const weatherTool = createTool({\n  id: \"getWeather\",\n  description: \"Get the current weather for a location\",\n  inputSchema: z.object({\n    location: z.string().describe(\"The location to get weather for\"),\n  }),\n  execute: async ({ context, runtimeContext }) => {\n    const temperatureUnit = runtimeContext.get(\"temperature-scale\");\n    // ... use temperatureUnit in your logic\n  },\n});\n```\n\n## Kudos\n\nThanks to [@mozharovsky](https://github.com/mozharovsky) for creating a [detailed issue](https://github.com/mastra-ai/mastra/issues/2881) and [initial implementation](https://github.com/mastra-ai/mastra/pull/3033)."
  },
  {
    "metadata": {
      "title": "Announcing MCP Server Support in Mastra",
      "publishedAt": "2025-04-21",
      "summary": "Create and share your AI tools with the Model Context Protocol (MCP) - now available in our latest stable release.",
      "author": "Nik Aiyer",
      "draft": false,
      "categories": ["announcements"]
    },
    "slug": "introducing-mcp-support",
    "content": "We're excited to announce MCP Server support in Mastra, available now in our latest stable tagged release (0.9.0). \n\nNow you can convert your Mastra tools into MCP-compatible servers, and share your tools with any MCP client. \n\nIf you already have an agent in Mastra, create an MCP server in five lines of code. \n\nIf you don't, you can write the tools in Typescript, then create your MCP server in five lines of code.\n\n## Creating an MCP Server\n\nHere's how to get started:\n\n```typescript\n// stdio.ts\n#!/usr/bin/env node\n\nimport { MCPServer } from \"@mastra/mcp\";\nimport {\n  myFirstTool,\n  mySecondTool,\n  // ... more tools\n} from './tools';\n\nconst server = new MCPServer({\n  name: \"my-mcp-server\",\n  version: \"1.0.0\",\n  tools: { \n    myFirstTool,\n    mySecondTool,\n    // ... more tools\n  },\n});\n\nserver.startStdio().catch((error) => {\n  console.error(\"Error running MCP server:\", error);\n  process.exit(1);\n});\n```\n\n## Publishing and Using Your MCP Server\n\nSee the [Deploying an MCPServer example](https://mastra.ai/en/examples/agents/deploying-mcp-server) in the examples section of the docs for a full workflow:\n\n1. Building your MCP server with Mastra tools\n2. Publishing it as an NPM package\n3. Using it from different MCP clients\n\nThis example provides step-by-step instructions and best practices for deployment. Note that since Mastra already lets you create an MCP client, you can now bundle your MCP server with your Mastra agent.\n\n## Kudos\n\nThanks to [Alana Goyal](https://basecase.vc/blog/unpacking-mcp) for spending a lot of her weekend on Slack with Shane helping us debug subtle differences in how different MCP clients implement the protocol.\n\n## What's next\n\nSome follow-up work we're working on:\n\n- Hosted MCP servers in Mastra Cloud without publishing to NPM\n- Enhanced compatibility across all MCP clients\n- Better debugging tools for MCP server development\n- Support for the [Streamable HTTP MCP transport](https://github.com/modelcontextprotocol/modelcontextprotocol/pull/206)\n\nWe're excited to see what you'll build! MCP on 🚀"
  },
  {
    "metadata": {
      "title": "AI with JavaScript in 2025",
      "publishedAt": "2025-04-20",
      "summary": "A dive into four leading open-source JS AI libraries: Vercel AI SDK for model routing, Mastra for agents and workflows, Langfuse for observability, and Stagehand for web automation.",
      "author": "Sam Bhagwat",
      "draft": false,
      "categories": ["foundations"]
    },
    "slug": "ai-engineering-javascript",
    "content": "Two years ago, if you wanted to build AI applications, you learned Python. That made sense when AI engineering meant training models or fine-tuning them. The ML tools were in Python, the tutorials were in Python, the jobs were for Python.\n\nNot anymore. \n\nAI engineering in 2025 means building on top of foundation models through their APIs. You don't need to train models - you need to wire them together, build UIs around them, and make them reliable. Sounds like web development, right? This is where JavaScript shines.\n\n## Building with AI in JavaScript\n\nSo why Javascript with building with AI?\n\nIf you're building with AI, you're building agents and applications that use LLMs as their brain, but with a lot of advanced capabilities on top: interfacing with users, storing memory, calling external services, reasoning step-by-step, and executing reliable workflows. It's systems programming with unreliable components.\n\nIf you're a fullstack JavaScript developer, you may be surprised that already know a lot of what you need. Your web development and backend skills translate directly. You just need the AI-specific knowledge. \n\nI've picked four open-source JavaScript libraries that can give you a good overview of the landscape.\n\n<div className=\"overflow-x-auto\">\n  <table className=\"min-w-full my-6\">\n    <thead>\n      <tr>\n        <th className=\"text-center py-4 px-6 font-semibold\">Library</th>\n        <th className=\"text-center py-4 px-6 font-semibold\">GitHub Stars</th>\n        <th className=\"text-center py-4 px-6 font-semibold\">Focus</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td className=\"text-center py-3 px-6\"><a href=\"https://github.com/vercel/ai\" className=\"text-blue-500 hover:underline\" target=\"_blank\" rel=\"noopener noreferrer\">Vercel AI SDK</a></td>\n        <td className=\"text-center py-3 px-6\">13.6k</td>\n        <td className=\"text-center py-3 px-6\">Model Routing</td>\n      </tr>\n      <tr>\n        <td className=\"text-center py-3 px-6\"><a href=\"https://github.com/mastra-ai/mastra\" className=\"text-blue-500 hover:underline\" target=\"_blank\" rel=\"noopener noreferrer\">Mastra</a></td>\n        <td className=\"text-center py-3 px-6\">12.1k</td>\n        <td className=\"text-center py-3 px-6\">Agent Framework</td>\n      </tr>\n      <tr>\n        <td className=\"text-center py-3 px-6\"><a href=\"https://github.com/langfuse/langfuse\" className=\"text-blue-500 hover:underline\" target=\"_blank\" rel=\"noopener noreferrer\">Langfuse</a></td>\n        <td className=\"text-center py-3 px-6\">10.5k</td>\n        <td className=\"text-center py-3 px-6\">Observability</td>\n      </tr>\n      <tr>\n        <td className=\"text-center py-3 px-6\"><a href=\"https://github.com/browserbase/stagehand\" className=\"text-blue-500 hover:underline\" target=\"_blank\" rel=\"noopener noreferrer\">Stagehand</a></td>\n        <td className=\"text-center py-3 px-6\">10.8k</td>\n        <td className=\"text-center py-3 px-6\">Web Browsing</td>\n      </tr>\n    </tbody>\n  </table>\n</div>\n\n## Vercel AI SDK\n\nIf you've tried to build a chat interface with streaming responses, you know it's surprisingly annoying. <a href=\"https://github.com/vercel/ai\" className=\"text-blue-500 hover:underline\" target=\"_blank\" rel=\"noopener noreferrer\">Vercel AI SDK</a> handles this headache for you, especially in React and Next.js apps.\n\nWhat's great is how it makes models completely swappable. The import syntax is simple, and switching between them requires minimal code changes. (I'll show a code example in the next section.)\n\nWhat you get:\n- **Model Routing**: Switch between OpenAI and Anthropic without rewriting code\n- **Streaming Primitives**: No more manually chunking response streams\n- **UI Components**: Drop-in chat UI that looks pretty good\n\n## mastra\n\nI'm obviously biased here, but we built <a href=\"https://github.com/mastra-ai/mastra\" className=\"text-blue-500 hover:underline\" target=\"_blank\" rel=\"noopener noreferrer\">Mastra</a> to be a complete framework for AI engineering. It works WITH the AI SDK and builds on top of it, so you get the best of both worlds.\n\nWhat we've seen is that for most folks, the hardest thing about AI engineering is the delicate dance between you and the LLM. You're building a harness around a tremendously powerful but unpredictable, non-deterministic engine.\n\nWe wanted to make that dance more intuitive, so Mastra put tracing directly in your local dev console. You can chat with the agent you're building, tab over to tracing, and see the full stack trace -- every input and output for every step in your every workflow and agent. \n\nFor a lot of people, this is the *aha* moment. You build something, and of course it doesn't work the first time. Then, you look at the trace, and suddenly you can see that the LLM is hallucinating a tool call, or that the structured output response is not what you're expecting. That kind of visibility is game-changing.\n\nHere's an example of a mastra agent that highlights AI SDK syntax as well:\n\n```typescript showLineNumbers copy\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\n \nexport const myAgent = new Agent({\n  name: \"My Agent\",\n  instructions: \"You are a helpful assistant.\",\n  model: openai(\"gpt-4o-mini\"),\n});\n```\n\nMastra covers all the bases of AI engineering: agents, workflows, RAG, tracing, evals, tool calling, memory, plus the local dev environment.\n\nYou can run an agent in a loop, you run wild with multi-agent capabilities, or go more precise with structured workflow graphs. With Mastra, AI engineering is a bit magical and a lot of fun...\n\n## Langfuse\n\nWe were just talking about tracing and how much it matters for AI engineering. <a href=\"https://github.com/langfuse/langfuse\" className=\"text-blue-500 hover:underline\" target=\"_blank\" rel=\"noopener noreferrer\">Langfuse</a> continues that conversation. It's observability designed specifically for LLMs - think Datadog but for AI, with an open-source core.\n\nWe've talked to dozens of teams using various observability platforms, and people consistently rate Langfuse as one of the most complete feature sets. And it's completely free and open source.\n\nWhat we like it for:\n- **Tracing**: See which prompts take forever and why they're failing\n- **Cost Monitoring**: Find out which feature is burning through your API budget\n- **Prompt Management**: Version control for your prompts\n\nThe killer feature is the trace visualization. Seeing your entire AI app call graph in one view is ridiculously helpful for debugging. You can literally see where the flow breaks down and focus your effort there.\n\n## Stagehand\n\n<a href=\"https://github.com/browserbase/stagehand\" className=\"text-blue-500 hover:underline\" target=\"_blank\" rel=\"noopener noreferrer\">Stagehand</a> solves a specific problem really well: making JavaScript agents that can browse the web. It runs on top of Playwright but with a MUCH more approachable syntax.\n\nA lot of AI engineering involves scraping the web to collect various sorts of data for LLM processing. If you've tried to do browser automation with raw Playwright, you probably experienced the pain by your third `.click()`. Stagehand is a higher-level abstraction and a great step in the right direction.\n\nKey capabilities:\n- **Browser Automation**: Simpler API than Playwright for common tasks\n- **Memory Systems**: Maintains history across multiple page loads\n- **Context Management**: Keeps track of what the agent has seen and done\n\nI appreciate stagehand's focus. It handles this one core use case and does it well. The API is clean, the documentation is solid, and the team are enthusiastic.\n\n## Putting It All Together\n\nTwo years ago, the JavaScript AI ecosystem barely existed. Today, it's thriving. (I didn't even get to my honorable mentions: [Assistant UI](https://www.assistant-ui.com/) and [Copilot Kit](https://www.copilotkit.ai/).) \n\nMore, they complement each other. You could use Vercel AI SDK for model routing, Mastra for agents and workflows, Langfuse to monitor everything, and Stagehand for your web-browsing capabilities.\n\nThese libraries are battle-tested, used in prod, and all open-source. No vendor lock-in, just code you can read, modify, and deploy. \n\nAs models get better (and they will), these libraries help take care of the plumbing so you can focus on building features users want.\n\nIn 2025, you don't need a PhD to build AI apps. If you can build a decent web app, you can build a solid AI application. These libraries handle the AI-specific complexities so you can focus on your domain expertise."
  },
  {
    "metadata": {
      "title": "Choosing a JavaScript Agent Framework",
      "publishedAt": "2025-04-19",
      "author": "Sam Bhagwat",
      "summary": "When choosing a JavaScript agent framework, consider the needs of your project and the team's expertise. Mastra and LangGraph.js are two leading options.",
      "draft": false,
      "categories": ["foundations"]
    },
    "slug": "choosing-a-js-agent-framework",
    "content": "As AI agents and assistants become increasingly table stakes for modern SaaS applications, developers face the challenge of selecting the right framework to build, manage, and deploy these intelligent systems.\n\nFor JavaScript developers, Mastra and LangGraph.js have emerged as leading options. Each offers distinct approaches to agent architecture, development workflows, and deployment strategies.\n\n## Mastra: a comprehensive, component-based framework\n\nMastra is a JavaScript framework designed for building, testing, and deploying AI agents. It offers an ecosystem that spans from local development to production deployment through a cloud platform (along with support for other serverless clouds). It's built by the team who previously built Gatsby.js.\n\n### Key Features\n\n- **Agent Management**: Mastra provides a cohesive architecture for defining agents, tools, workflows, and memory systems.\n- **Workflow Orchestration**: Built-in workflow capabilities for multi-step processes and complex agent interactions.\n- **Memory Systems**: Sophisticated memory management with support for various storage backends (PostgreSQL, LibSQL, Upstash).\n- **Voice Capabilities**: Native support for voice interfaces, including speech-to-text and text-to-speech functionality.\n- **Observability**: Comprehensive tracing and monitoring tools, particularly robust in Mastra Cloud.\n- **Deployment Options**: Multiple deployment paths from self-hosted to serverless to fully-managed with Mastra Cloud.\n\n### Development Experience\n\nMastra follows a declarative approach to agent definition, with TypeScript support providing strong type safety. Its architecture emphasizes component reusability and extension patterns:\n\n```typescript copy\nimport { Mastra, Agent } from '@mastra/core';\nimport { openai } from '@ai-sdk/openai';\nimport { getWeather } from './tools/weather';\n\nconst weatherAgent = new Agent({\n  model: openai('gpt-4o')\n  instructions: 'You are a helpful weather assistant...',\n  tools: {\n    getWeather,\n  },\n});\n\nexport const mastra = new Mastra({\n  agents: {\n    weatherAgent,\n  }\n});\n```\n\n## LangGraph.js: a graph-based framework for agent orchestration\n\nLangGraph.js focuses on representing agent behavior as graph-based workflows, allowing for complex decision trees and state management. It's the agent framework from the people who created Langchain.\n\n### Key Features\n\n- **Graph-Based Orchestration**: Define agent behavior as nodes and edges in a computational graph.\n- **State Management**: First-class support for managing agent states.\n- **LangChain Integration**: Tight integration with the LangChain ecosystem.\n- **Tool Calling**: Framework for connecting agents to external tools and APIs.\n\n### Development Experience\n\nLangGraph.js uses a graph-construction API to define the flow of agent execution:\n\n```typescript copy\nimport { createGraph } from \"langgraph\";\n\nconst graph = createGraph();\n\ngraph.addNode(\"understand\", async (state) => {\n  // Process user input\n});\n\ngraph.addNode(\"toolCall\", async (state) => {\n  // Call external tools if needed\n});\n\ngraph.addNode(\"respond\", async (state) => {\n  // Generate response\n});\n\ngraph.addEdge(\"understand\", \"toolCall\");\ngraph.addEdge(\"toolCall\", \"respond\");\n```\n\n## Framework Comparison\n\n<div className=\"overflow-x-auto\">\n  <table className=\"w-full my-6 text-sm\">\n    <thead>\n      <tr>\n        <th className=\"text-center py-2 px-2 font-semibold w-2/4\">Feature</th>\n        <th className=\"text-center py-2 px-2 font-semibold w-1/4\">Mastra</th>\n        <th className=\"text-center py-2 px-2 font-semibold w-1/4\">\n          LangGraph.js\n        </th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td className=\"text-center py-2 px-2\">Workflow Orchestration</td>\n        <td className=\"text-center py-2 px-2\">✅</td>\n        <td className=\"text-center py-2 px-2\">✅</td>\n      </tr>\n      <tr>\n        <td className=\"text-center py-2 px-2\">Agent Abstractions</td>\n        <td className=\"text-center py-2 px-2\">✅</td>\n        <td className=\"text-center py-2 px-2\">✅</td>\n      </tr>\n      <tr>\n        <td className=\"text-center py-2 px-2\">Multi-Agent</td>\n        <td className=\"text-center py-2 px-2\">⚠️</td>\n        <td className=\"text-center py-2 px-2\">✅</td>\n      </tr>\n      <tr>\n        <td className=\"text-center py-2 px-2\">Declarative API</td>\n        <td className=\"text-center py-2 px-2\">✅</td>\n        <td className=\"text-center py-2 px-2\">✅</td>\n      </tr>\n      <tr>\n        <td className=\"text-center py-2 px-2\">Short-term Memory</td>\n        <td className=\"text-center py-2 px-2\">✅</td>\n        <td className=\"text-center py-2 px-2\">✅</td>\n      </tr>\n      <tr>\n        <td className=\"text-center py-2 px-2\">Long-term Memory</td>\n        <td className=\"text-center py-2 px-2\">✅</td>\n        <td className=\"text-center py-2 px-2\">✅</td>\n      </tr>\n      <tr>\n        <td className=\"text-center py-2 px-2\">Human-in-the-loop</td>\n        <td className=\"text-center py-2 px-2\">✅</td>\n        <td className=\"text-center py-2 px-2\">✅</td>\n      </tr>\n      <tr>\n        <td className=\"text-center py-2 px-2\">Streaming</td>\n        <td className=\"text-center py-2 px-2\">✅</td>\n        <td className=\"text-center py-2 px-2\">✅</td>\n      </tr>\n      <tr>\n        <td className=\"text-center py-2 px-2\">Local Dev Console</td>\n        <td className=\"text-center py-2 px-2\">✅</td>\n        <td className=\"text-center py-2 px-2\">✅</td>\n      </tr>\n      <tr>\n        <td className=\"text-center py-2 px-2\">Local Dev Tracing</td>\n        <td className=\"text-center py-2 px-2\">✅</td>\n        <td className=\"text-center py-2 px-2\">❌</td>\n      </tr>\n      <tr>\n        <td className=\"text-center py-2 px-2\">Project Setup</td>\n        <td className=\"text-center py-2 px-2\">✅</td>\n        <td className=\"text-center py-2 px-2\">⚠️</td>\n      </tr>\n      <tr>\n        <td className=\"text-center py-2 px-2\">Server Deployment</td>\n        <td className=\"text-center py-2 px-2\">Hono</td>\n        <td className=\"text-center py-2 px-2\">Docker</td>\n      </tr>\n      <tr>\n        <td className=\"text-center py-2 px-2\">Serverless Deployment</td>\n        <td className=\"text-center py-2 px-2\">✅</td>\n        <td className=\"text-center py-2 px-2\">❌</td>\n      </tr>\n      <tr>\n        <td className=\"text-center py-2 px-2\">Cloud Hosting</td>\n        <td className=\"text-center py-2 px-2\">✅</td>\n        <td className=\"text-center py-2 px-2\">✅</td>\n      </tr>\n      <tr>\n        <td className=\"text-center py-2 px-2\">Cloud Tracing</td>\n        <td className=\"text-center py-2 px-2\">✅</td>\n        <td className=\"text-center py-2 px-2\">✅</td>\n      </tr>\n    </tbody>\n  </table>\n</div>\n\n## Choosing the Right Framework\n\n### Consider Mastra when:\n\n- You want a local development environment with tracing built-in\n- Memory management is important\n\n### Consider LangGraph.js when:\n\n- You're already using other LangChain tools\n- You like the graph-based workflow syntax\n\n## Conclusion\n\nThe choice between Mastra and LangGraph.js depends largely on your specific requirements as well as which you feel most comfortable in.\n\n- Mastra offers both agent and workflow capabilities, and a workflow syntax that's familiar to JavaScript devs.\n- LangGraph.js is good for graph-based agent orchestration and applications that already use LangChain.\n\nAs the agent ecosystem continues to evolve rapidly, each of these frameworks is likely to develop new capabilities and address current limitations. The best approach may be to prototype a simple agent in each framework to get a feel for their development experience and evaluate how they align with your team's workflow.\n\nNo matter which framework you choose, the most important factors will be your team's ability to understand, extend, and maintain the codebase as your agent's capabilities grow over time.\n\n\n\n## FAQ\n\n#### What are the key differences between Mastra and LangGraph.js?\nMastra offers both agent and workflow capabilities, with a workflow syntax familiar to JavaScript developers. LangGraph.js is suited for graph-based agent orchestration and applications that already use LangChain. \n\n#### When should I choose Mastra over LangGraph.js?\nConsider Mastra when you prefer TypeScript and want a local development environment with tracing built-in and for memory management.\n\n#### How does Mastra compare to LangChain?\nLangChain is a flexible, modular framework primarily focused on integrating large language models (LLMs) into complex workflows, while Mastra provides a cohesive architecture for defining agents, tools, workflows, and memory systems, with built-in workflow capabilities for multi-step processes and complex agent interactions. \n\n#### What JavaScript frameworks are available for building AI agents?\nNotable JavaScript frameworks include Mastra, LangGraph.js, and LangChain.js. Each offers distinct approaches to agent architecture, development workflows, and deployment strategies.\n\n#### Which framework is best for production-ready AI applications?\nMastra is designed for building, testing, and deploying AI agents, offering an ecosystem that spans from local development to production deployment through a cloud platform, along with support for other serverless clouds."
  },
  {
    "metadata": {
      "title": "Building a Personal Assistant with Mastra and MCP",
      "publishedAt": "2025-04-18",
      "author": "Shane Thomas",
      "summary": "How we built a personal assistant that can manage emails, monitor GitHub activity, schedule social media posts, and more - all through a Telegram interface.",
      "draft": false,
      "categories": ["examples"]
    },
    "slug": "personal-assistant",
    "content": "![Personal Assistant Telegram Tweet demo](/images/blog/PersonalAssistantTweet.gif)\n\nAt Mastra, [we are all-in on MCP](https://mastra.ai/blog/mastra-mcp). So we built a personal assistant that can help with various tasks like managing emails,\nmonitoring GitHub activity, scheduling social media posts, and providing weather information.\nWhat made this project particularly interesting was how little code it took to build a powerful\nassistant with access to multiple tools and services.\n\n## The Power of MCP\n\nThe key to building this assistant quickly was leveraging Mastra agents with Model Context Protocol (MCP).\nMCP made it incredibly easy to integrate with various services:\n\n- **Zapier MCP**: For Gmail and Typefully integration\n- **Composio MCP**: For GitHub activity monitoring\n- **HackerNews MCP**: A community-built server from npm\n- **TextEditor MCP**: For local filesystem access\n\nYou can find more MCP servers using our [MCP Registry Registry](https://mastra.ai/mcp-registry-registry).\n\nYou can easily connect all your MCP servers using [Mastra's MCP package](https://mastra.ai/en/docs/agents/mcp-guide).\nHere's how we set up the MCP configuration:\n\n```typescript\nconst mcp = new MCPConfiguration({\n  servers: {\n    zapier: {\n      url: new URL(process.env.ZAPIER_MCP_URL || \"\"),\n    },\n    github: {\n      url: new URL(process.env.COMPOSIO_MCP_GITHUB || \"\"),\n    },\n    hackernews: {\n      command: \"npx\",\n      args: [\"-y\", \"@devabdultech/hn-mcp-server\"],\n    },\n    textEditor: {\n      command: \"pnpx\",\n      args: [\n        `@modelcontextprotocol/server-filesystem`,\n        path.join(process.cwd(), \"../\", \"../\", \"notes\"),\n      ],\n    },\n  },\n});\n```\n\nThis configuration gave our assistant access to all the tools it needed with minimal setup code.\n\n## The Agent's Instructions\n\nThe heart of our assistant is its system prompt, which clearly defines its capabilities and\nhow to use each tool:\n\n```typescript\nexport const personalAssistantAgent = new Agent({\n  name: \"Personal Assistant\",\n  instructions: `\n      You are a helpful personal assistant that can help with various tasks such as email, \n      monitoring github activity, scheduling social media posts and providing weather information.\n      \n      You have access to the following tools:\n      \n      1. Gmail:\n         - Use these tools for reading and categorizing emails from Gmail\n         - You can categorize emails by priority, identify action items, and summarize content\n         - You can also use this tool to send emails\n      \n      2. GitHub:\n         - Use these tools for monitoring and summarizing GitHub activity\n         - You can summarize recent commits, pull requests, issues, and development patterns\n      \n      3. Typefully:\n         - Use these tools for \n         - It can also create and manage tweet drafts with Typefully\n         - It focuses on AI, Javascript, Typescript, and Science topics\n      \n      4. Weather:\n         - Use this tool for getting weather information for specific locations\n         - It can provide details like temperature, humidity, wind conditions, and weather conditions\n         - Always ask for the location or if it's not provided try to use your working memory \n           to get the user's last requested location\n\n      5. Hackernews:\n         - Use this tool to search for stories on Hackernews\n         - You can use it to get the top stories or specific stories\n         - You can use it to retrieve comments for stories\n\n      6. Filesystem:\n         - You also have filesystem read/write access to a notes directory. \n         - You can use that to store information such as reminders for later use or organize info for the user.\n         - You can use this notes directory to keep track of to do list items for the user.\n  `,\n  model: openai(\"gpt-4o\"),\n  tools: { ...mcpTools, weatherTool },\n  memory,\n});\n```\n\n## Agent Memory Management\n\nWe leveraged [Mastra's memory system](https://mastra.ai/en/docs/memory/overview) to make the assistant more context-aware and personalized:\n\n```typescript\nconst memory = new Memory({\n  options: {\n    // Keep last 20 messages in context\n    lastMessages: 20,\n    // Enable semantic search to find relevant past conversations\n    semanticRecall: {\n      topK: 3,\n      messageRange: {\n        before: 2,\n        after: 1,\n      },\n    },\n    // Enable working memory to remember user information\n    workingMemory: {\n      enabled: true,\n      template: `<user>\n          <first_name></first_name>\n          <username></username>\n          <preferences></preferences>\n          <interests></interests>\n          <conversation_style></conversation_style>\n        </user>`,\n      use: \"tool-call\",\n    },\n  },\n});\n```\n\nThis configuration allows the assistant to:\n\n- Maintain conversation history\n- Find relevant past interactions using semantic search\n- Remember user preferences and information\n- Provide more personalized responses\n\n## Telegram Integration\n\nTo make the assistant accessible from anywhere, we built a Telegram bot interface. The implementation handles message streaming, markdown formatting, and proper error handling.\n[You can find the full Telegram integration code here](https://github.com/mastra-ai/personal-assistant-example/blob/main/src/mastra/integrations/telegram.ts).\n\n## Key Learnings\n\n1. **MCP Simplifies Integration**: Using MCP servers dramatically reduced the amount of code needed to integrate with external services. Each service required just a few lines of configuration.\n\n2. **Clear Instructions Matter**: The system prompt played a crucial role in ensuring the assistant used tools appropriately. By clearly defining each tool's purpose and usage guidelines, we got more reliable results.\n\n3. **Memory Enhances Experience**: The memory system made the assistant feel more personalized and context-aware. It could reference past conversations and maintain user preferences.\n\n4. **Telegram as an Interface**: Using Telegram as the interface made the assistant accessible from anywhere while keeping the core logic running locally. This approach provided a great balance of accessibility and privacy.\n\n## Try It Yourself\n\nThe complete code for this personal assistant is available on [GitHub](https://github.com/mastra-ai/personal-assistant-example). You can use it as a starting point to build your own assistant with the tools and services you need.\n\nWe're excited to see what people build using Mastra Agents + MCP."
  },
  {
    "metadata": {
      "title": "Mastra Changelog 2025-04-16",
      "publishedAt": "2025-04-16",
      "summary": "Mastra's latest updates: MCP registry registry, model settings in Playground UI, and new Memory docs",
      "author": "Shane Thomas",
      "draft": false,
      "categories": ["changelogs"]
    },
    "slug": "changelog-2025-04-16",
    "content": "Last week, we shipped the first MCP registry registry, added model settings in Playground UI, and published some new docs. Let’s dig in…\n\n## The MCP registry registry (and mcp server)\n\nWe launched the very first [MCP registry registry](https://mastra.ai/mcp-registry-registry), which is exactly what it sounds like it is: a registry of the most popular MCP registries.\n\n![MCP Registry Registry](/mcpregistryregistry.png)\n\n\nSince searching within two nested lists can be difficult, we also shipped an MCP registry registry MCP server.  Just `npm install @mastra/mcp-registry-registry` to start asking questions and explore how to use MCP with Mastra Agents.\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ARE4tHUe1R4?si=yVRG76cv2s5DZwGY\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\n## Model settings in Playground UI\n\nWe enabled model setting controls in Mastra local dev. This means you can directly adjust [model settings like temperature, max tokens, and max retries](https://mastra.ai/docs/local-dev/mastra-dev#model-settings) when chatting with agents — and learn which settings work best.\n\n\n![Model settings in Mastra local dev](/modelsettings.gif)\n\n\n## New Memory docs section\n\nWe shipped [new documentation on how Memory works](https://mastra.ai/docs/memory/overview) in Mastra.  You’ll find core definitions, installation instructions, and all sorts of tools and ideas for customizing memory configurations, e.g.:  \n\n\n*In Mastra, context is broken up into three parts: system instructions and information about the user ([working memory](https://mastra.ai/docs/memory/working-memory)), recent messages ([message history](https://mastra.ai/docs/memory/overview#conversation-history)), and older messages that are relevant to the user's query ([semantic recall](https://mastra.ai/docs/memory/semantic-recall)).*\n\n*In addition, we provide [memory processors](https://mastra.ai/docs/memory/memory-processors) to trim context or remove information if the context is too long.*\n\n\nThat&apos;s all for recent major Mastra updates. [Find the full release log here](https://github.com/mastra-ai/mastra/releases/tag/%40mastra%2Fcore%400.8.3). And, as usual, stay tuned for more changes next time…\n\nHappy building 🚀"
  },
  {
    "metadata": {
      "title": "PDF-to-Blog: Giving documents a second life with Mastra and Mistral OCR",
      "publishedAt": "2025-04-11",
      "summary": "A technical guide showing how to build a PDF-to-blog converter using Mastra and Mistral OCR",
      "author": "Shane Thomas",
      "categories": ["examples"]
    },
    "slug": "pdf-to-blog",
    "content": "Content teams often struggle to repurpose information locked in PDF documents. To solve this problem, we built a system that automatically converts PDFs into well-structured blog posts using Mastra and Mistral OCR.\n\n## What is Mistral OCR?\n\n[Mistral OCR](https://mistral.ai/news/mistral-ocr) is an advanced optical character recognition (OCR) technology designed to extract text from documents like PDFs. Unlike traditional OCR tools, Mistral is particularly effective with complex layouts, tables, and mixed-format documents, making it ideal for technical documentation.\n\n## The Technical Challenge\n\nOur task was to create a bridge between Mastra and Mistral OCR that would:\n\n1. Extract high-quality text from PDFs using Mistral OCR\n2. Process that text with Mastra to generate readable blog content\n3. Handle everything automatically through a reusable workflow\n\n## Our solution\n\n### Starting with an API route\n\nFirst we create an API route that takes the PDF file and triggers a workflow.\n\nThe route is called from our [Mastra instance](https://github.com/mastra-ai/pdf-blogify-mastra/blob/main/src/mastra/index.ts):\n\n```tsx\napiRoutes: [\n  registerApiRoute(\"/mastra/upload-pdf\", {\n    method: \"POST\",\n    handler: uploadPdfHandler,\n  }),\n],\n```\n\nAnd is defined as: \n\n```tsx\nexport const uploadPdfHandler = async (c: Context) => {\n  try {\n    const formData = await c.req.formData();\n    const pdfFile = formData.get('pdf');\n    \n    if (!pdfFile || !(pdfFile instanceof File)) {\n      return c.json({ error: 'No PDF file uploaded. Use key \"pdf\" in form-data.' }, 400);\n    }\n    \n    const arrayBuffer = await pdfFile.arrayBuffer();\n    const buffer = Buffer.from(arrayBuffer);\n    \n    \n    const { start } = pdfToBlogWorkflow.createRun();\n    const result = await start({ triggerData: { pdfFile: buffer } });\n    \n  }\n}\n```\n\n### Connecting Mistral OCR and Mastra with tools\n\nBefore we can define a complete workflow, we need to create the relevant tool(s) and agent(s). \n \n\nWe use Mastra’s `createTool` functionality to create a tool that extracts text from PDF files using Mistral OCR: \n\n```tsx\nimport { createTool } from '@mastra/core/tools';\nimport { z } from 'zod';\nimport { Mistral } from '@mistralai/mistralai';\n\nconst client = new Mistral({ apiKey: process.env.MISTRAL_API_KEY });\n\n// Define the OCR response interface\ninterface OCRResponse {\n  pages: {\n    markdown: string;\n    metadata?: any;\n  }[];\n}\n\n// Create the OCR tool using Mastra's createTool pattern\nexport const mistralOCRTool = createTool({\n  id: 'mistral-ocr',\n  description: 'Extract text from PDF files using Mistral OCR',\n  inputSchema: z.object({\n    pdfBuffer: z.instanceof(Buffer).describe('The PDF file buffer to process'),\n  }),\n  outputSchema: z.object({\n    extractedText: z.string().describe('The text extracted from the PDF'),\n    pagesCount: z.number().describe('Number of pages processed'),\n  }),\n  execute: async ({ context }) => {\n    return await processOCR(context.pdfBuffer);\n  },\n});\n```\n\nWe also define [a process function for OCR extraction](https://github.com/mastra-ai/pdf-blogify-mastra/blob/main/src/mastra/tools/mistralOCR.ts).\n\n### Creating a blogpost generator agent\n\nThen we define a blogpost generator agent: \n\n```tsx\nimport { Agent } from '@mastra/core/agent';\nimport { mistral } from '@ai-sdk/mistral';\nimport { mistralOCRTool } from '../tools/mistralOCR';\n\nexport const blogPostAgent = new Agent({\n  name: 'Blog Post Generator Pro',\n  instructions: `\nYou're writing a concise technical post for fellow developers. Aim for a natural, conversational tone as if you're explaining something to a colleague during a coffee break.\n\n**🎯 TITLE**\nCreate a clear, specific title that tells readers exactly what to expect.\n\n═══════════════════════════\n\n**📝 INTRODUCTION**\nWrite a brief, direct introduction that explains what this post covers and why it matters.\n\n═══════════════════════════\n\n**🔍 MAIN CONTENT**\n\n**➤ Section 1: Core Concept**\n- Use everyday language, not marketing speak\n- **Bold** important terms and *italicize* for emphasis\n- Include concrete examples with code blocks when relevant:\n  \\`\\`\\`javascript\n  // Example code here with syntax highlighting\n  \\`\\`\\`\n\n**➤ Section 2: Practical Implementation**\n- Share insights as if from personal experience (\"I've found that...\")\n- Break down processes with numbered steps when appropriate\n- Add helpful tips in boxed format:\n  ═══════════════════════════\n  PRO TIP: Short, actionable advice here\n  ═══════════════════════════\n\n**➤ Section 3: Key Takeaways** (optional)\n- Compare approaches using tables if relevant:\n  | Approach | Advantage | Best Use Case |\n  |----------|-----------|---------------|\n  | Option A | Speed     | Simple tasks  |\n  | Option B | Accuracy  | Complex data  |\n\n═══════════════════════════\n\n**✨ CONCLUSION**\nBriefly summarize the key takeaway and possibly pose a thoughtful question.\n\nAvoid:\n- Buzzwords and clichés like \"revolutionary,\" \"game-changing,\" or \"in today's fast-paced world\"\n- Long, complex sentences\n- Obvious transitions like \"firstly,\" \"secondly,\" or \"in conclusion\"\n- Making it obvious the content is AI-generated\n- Marketing-speak or overly formal academic language\n\nGuidelines:\n1. Use **bold** for headers/subsections and *italics* for technical terms\n2. Maintain 1-3 sentence paragraphs for readability\n3. Blend professional tone with conversational elements\n4. Preserve code blocks with syntax highlighting\n5. Use boxed text for important warnings/tips\n6. Include practical examples for every concept\n7. Ensure SEO optimization through *strategic keyword placement*\n\nThe final blog post should sound like it was written by a real developer sharing practical knowledge from experience - natural, helpful, and concise (600-900 words total).\n  `,\n  model: mistral('mistral-large-latest'),\n  tools: { mistralOCRTool },\n});\n```\n\nWe’re careful to give it a relevant name and write highly detailed, specific instructions about what it should produce. Remember: you can almost never be too detailed when it comes to writing system prompts. \n\n### Putting it all together: PDF-to-blog Workflow\n\nMastra's workflow system provides a structured way to handle multi-step processes. This is where the magic happens, the place where all our tools and agents come together. \n\nHere's how we implemented our PDF-to-blog workflow:\n\n```tsx\nexport const pdfToBlogWorkflow = new Workflow({\n  name: 'pdf-to-blog',\n  triggerSchema: pdfInputSchema,\n})\n  // First step: Extract text from PDF\n  .step(extractTextStep)\n  \n  // Second step: Generate blog post (runs if text extraction succeeds)\n  .then(generateBlogPostStep)\n  \n  // Third step: Fallback blog post (runs if primary generation fails/returns success: false)\n  .then(fallbackBlogPostStep, {\n    when: async ({ context }) => {\n      const primaryStep = context.steps['generate-blog-post'];\n      const shouldRun = !primaryStep || primaryStep.status !== 'success' || \n                        !primaryStep.output.success;\n      return shouldRun;\n    },\n  })\n  \n  // Fourth step: Final fallback (runs if fallback generation fails/returns success: false)\n  .then(finalFallbackStep, {\n    when: async ({ context }) => {\n      const fallbackStep = context.steps['fallback-blog-post'];\n      \n      let shouldRun: boolean;\n      if (fallbackStep?.status === 'success') {\n        shouldRun = !fallbackStep.output.success;\n      } else {\n        // If fallback step didn't run or failed, we should run the final fallback\n        shouldRun = true;\n      }\n      return shouldRun;\n    },\n  });\n```\n\nThis workflow takes a PDF input and processes it through a series of defined steps. It extracts text from the PDF, generates a blogpost, and has two fallback methods in case the PDF is too long. \n\n## Links\n\n[Try the demo yourself](https://pdf-blogify.vercel.app/)\n\n[Frontend site source code](https://github.com/mastra-ai/pdf-blogify)\n\n[Mastra source code](https://github.com/mastra-ai/pdf-blogify-mastra/)\n\nWe hope the PDF-to-blog converter will help anyone looking to repurpose PDF content. If you decide to build on our example, we’d love to see what you make."
  },
  {
    "metadata": {
      "title": "Announcing Mastra's MCP Registry Registry",
      "publishedAt": "2025-04-10",
      "summary": "Mastra's MCP Registry Registry simplifies the process of finding MCP registries.",
      "author": "Sam Bhagwat",
      "draft": false,
      "categories": ["announcements"]
    },
    "slug": "mcp-registry-registry",
    "content": "MCP (Model Context Protocol) has been getting a lot of love recently — and for good reason. With MCP, your agents can directly interface with other applications and autonomously complete tasks like sending emails, posting on social media, fetching GitHub issues, etc. \n\nMastra supports [MCP](https://mastra.ai/reference/tools/mcp-configuration) and even has its own [MCP documentation server](https://mastra.ai/blog/introducing-mastra-mcp). But, unlike our peer group, we aren’t launching our own MCP registry. \n\nSince there are already so many great MCP registries out there, we instead built [The MCP Registry Registry](https://mastra.ai/mcp-registry-registry).\n\n![MCP Registry Registry](/mcpregistryregistry.png)\n\n\n## How the registry came to life\n\nA couple weeks ago, a groupchat we were in started sharing lists of MCP servers. \n\nDaniel Chalef from Zep started the thread. Tod from Pipedream shared a few. Others jumped in. Our response:\n\n![Registry Registry Tweet](/registryregistry.png)\n\n\nOver the weeks, the idea slowly grew on us and, voila, the meta registry was born. \n\nIf you choose to make your own MCP registry registry, let us know so we can also ship the first MCP registry registry registry — but remember:\n\n![Registry Registry Meme](/registrymeme.png)"
  },
  {
    "metadata": {
      "title": "Mastra Text-to-SQL: from natural language to database queries",
      "publishedAt": "2025-04-04",
      "summary": "Use Mastra's Text-to-SQL system to convert natural language to database queries",
      "author": "Shane Thomas",
      "draft": false,
      "categories": ["examples"]
    },
    "slug": "txt-to-sql",
    "content": "The [Mastra Text-to-SQL demo](https://mastra-text-to-sql.vercel.app/) is an interactive web app that generates and executes SQL queries based on natural language prompts. \n\nAs it’s written, it specializes in answering questions about various global city populations on a sample database. You could easily repurpose the example for different use cases, however. \n\nHere’s a look at how it works: \n\n## Agent\n\nText-to-SQL is based off a single agent: a SQL expert for a city population database. \n\nThe agent is pre-defined to already understand the database schema, query guidelines key SQL formatting tips, and what its basic workflow is (analyze the user's question about city data, generate an appropriate SQL query, execute the query, and return results).    \n\n```typescript\nimport { openai } from \"@ai-sdk/openai\";\nimport { Agent } from \"@mastra/core/agent\";\nimport * as tools from \"../tools/population-info\";\nimport { LanguageModelV1 } from \"@ai-sdk/provider\";\n\nexport const sqlAgent = new Agent({\n  name: \"SQL Agent\",\n  instructions: ``You are a SQL (PostgreSQL) expert for a city population database. Generate and execute queries that answer user questions about city data.\n\n    DATABASE SCHEMA:\n    cities (\n      id SERIAL PRIMARY KEY,\n      popularity INTEGER,\n      geoname_id INTEGER,\n      name_en VARCHAR(255),\n      country_code VARCHAR(10),\n      population BIGINT,\n      latitude DECIMAL(10, 6),\n      longitude DECIMAL(10, 6),\n      country VARCHAR(255),\n      region VARCHAR(255),\n      continent VARCHAR(255), /* Africa, Asia, Europe, North America, Oceania, South America, Antarctica */\n      code2 VARCHAR(10),\n      code VARCHAR(10),\n      province VARCHAR(255)\n    );\n\n    QUERY GUIDELINES:\n    - Only retrieval queries are allowed\n    - For string comparisons, use: LOWER(field) ILIKE LOWER(term)\n    - Use \"United Kingdom\" for UK and \"United States\" for USA\n    - This dataset contains only current information, not historical data\n    - Always return at least two columns for visualization purposes\n    - If a user asks for a single column, include a count of that column\n    - Format rates as decimals (e.g., 0.1 for 10%)\n\n    Key SQL formatting tips:\n    - Start main clauses (SELECT, FROM, WHERE, etc.) on new lines\n    - Indent subqueries and complex conditions\n    - Align related items (like column lists) for readability\n    - Put each JOIN on a new line\n    - Use consistent capitalization for SQL keywords\n\n    WORKFLOW:\n    1. Analyze the user\\'s question about city data\n    2. Generate an appropriate SQL query\n    3. Execute the query using the Execute SQL Query tool\n    4. Return results in markdown format with these sections:\n\n       ### SQL Query\n       \\`\\`\\`sql\n       [The executed SQL query with proper formatting and line breaks for readability]\n       \\`\\`\\`\n\n       ### Explanation\n       [Clear explanation of what the query does]\n\n       ### Results\n       [Query results in table format]\n    ``,\n  model: openai(\"gpt-4o\") as LanguageModelV1,\n  tools: {\n    populationInfo: tools.populationInfo,\n  },\n);\n\n```\n\n## Tools\n\nOnce the agent generates an appropriate SQL query, it uses a tool to execute it.\n\nThe `populationInfo` tool is a PostgreSQL database query executor specifically designed for the cities database: \n\n```tsx\n\nexport const populationInfo = createTool({\n  id: \"Execute SQL Query\",\n  inputSchema: z.object({\n    query: z\n      .string()\n      .describe(\"SQL query to execute against the cities database\"),\n  }),\n  description: `Executes a SQL query against the cities database and returns the results`,\n  execute: async ({ context: { query } }) => {\n    try {\n      const trimmedQuery = query.trim().toLowerCase();\n      if (!trimmedQuery.startsWith(\"select\")) {\n        throw new Error(\"Only SELECT queries are allowed for security reasons\");\n      }\n\n      return await executeQuery(query);\n    } catch (error) {\n      throw new Error(\n        `Failed to execute SQL query: ${error instanceof Error ? error.message : String(error)}`\n      );\n    }\n  },\n});\n```\n\nThe tool only allows `SELECT` queries to prevent malicious database modifications. This is enforced through a simple check of the query's starting text. \n\nThe tool also implements basic error handling at multiple levels. [Find the complete code and follow along here.](https://github.com/mastra-ai/text-to-sql-example)\n\nAnd be sure to tell us if you build any new text-to-SQL applications yourself. \n\nHappy building 🚀"
  },
  {
    "metadata": {
      "title": "Mastra Changelog 2025-04-02",
      "publishedAt": "2025-04-02",
      "summary": "Mastra's Latest Updates: API routing, memory processors, and more",
      "author": "Shane Thomas",
      "draft": false,
      "categories": ["changelogs"]
    },
    "slug": "changelog-2025-04-02",
    "content": "This week we added custom API routes, pluggable memory processors, and more.\n\n## Custom API routes: build your AI backend\n\nMastra now allows developers to create custom API routes, transforming it into a complete backend solution for AI applications. While Mastra's core functionality remains protected under `/api/*` routes, developers can now define their own endpoints anywhere else in the URL space. This means you can create custom backend logic that directly integrates with Mastra's AI capabilities, agents, and workflows.\n\nFor example, you could create a custom weather endpoint that leverages a Mastra agent:\n\n```tsx\nregisterApiRoute('/weather/forecast', {\n  method: 'POST',\n  handler: async (c) => {\n    const weatherAgent = mastra.getAgent('weatherAgent');\n    const forecast = await weatherAgent.generate('What is the weather today?');\n    return c.json({ forecast });\n  }\n});\n```\n\nThe implementation also enforces this separation between system and custom routes through type-safe validation.\n\nCustom API routes are currently only available in alpha. \n\n## Memory processors\n\nMastra now supports pluggable memory processors. In other words, you can customize and transform memories before they’re added to your agent.\n\n This week we added built-in `TokenLimiter` and `ToolCallFilter` processors.\n\n`TokenLimiter` will limit the total number of tokens added to the context window via retrieved memories. \n\nFor instance, GPT-4o has a max token limit of 128000 tokens so this can be used to make sure that limit isn’t exceeded: \n\n```tsx\nimport { Memory } from \"@mastra/memory\";\nimport { TokenLimiter } from \"@mastra/memory/processors\";\n \nconst memory = new Memory({\n  processors: [\n    // Limit message history to approximately 127000 tokens (ex. for gpt-4o)\n    new TokenLimiter(127000),\n  ],\n});\n```\n\nThe `ToolCallFilter` will remove tool calls by tool id (or all tools) from the history. This is helpful if you only ever want the agent to see the most recent tool call. \n\nHere’s a sample implementation: \n\n```tsx\nimport { Memory } from \"@mastra/memory\";\nimport { ToolCallFilter } from \"@mastra/memory/processors\";\n \nconst memory = new Memory({\n  processors: [\n    // Remove all tool calls and results\n    new ToolCallFilter(),\n \n    // Or exclude only specific tools\n    new ToolCallFilter({\n      exclude: [\"imageGenTool\"],\n    }),\n  ],\n});\n```\n\nWe also support the ability to combine multiple processors and even write your own custom ones. For implementation details & more on memory processors, please [check out our docs](https://mastra.ai/docs/reference/memory/memory-processors).\n\n## A few other updates\n\n- We expanded Mastra Storage’s database support to include Clickhouse, providing users with an additional option for storing their data.\n- We updated the framework to use version 4.2.2 of the Vercel AI SDK.\n\nWe’re always improving Mastra and would [love to hear your feedback](https://discord.gg/gCj26JKF). \n\nMore to come next week!"
  },
  {
    "metadata": {
      "title": "Mastra Storage: a flexible storage system for AI Applications",
      "publishedAt": "2025-04-01",
      "summary": "A look at how Mastra's storage system simplifies building AI applications by managing workflows, memory, traces, and eval data with flexible storage backends.",
      "author": "Nik Aiyer",
      "draft": false,
      "categories": ["foundations"]
    },
    "slug": "mastra-storage",
    "content": "## Introducing Mastra Storage\n\nIf you've built AI applications before, you know how quickly data management can become overwhelming. Databases used in AI applications have unique challenges that come from - maintaining contextual conversation histories across multiple turns, persisting complex workflow states, tracking detailed evaluation metrics, or storing the monitoring data needed to understand your agents' behavior. \n\nMastra Storage was built to address this. It provides a unified system that stores, retrieves, and manages your AI application's data, from conversation threads to workflow states. \n\nBut what makes Mastra Storage particularly powerful is its flexibility. It's easy to get started but also easy to scale. Mastra Storage offers flexible storage backends that can change as your needs evolve. Whether you're building a prototype or scaling to production, the storage system is designed to adapt.\n\n## How Mastra Storage Works\n\n![Diagram showing storage in Mastra](/images/mastra-storage/mastra-storage-overview-light.png)\n\nAt its core, Mastra Storage manages four essential types of data:\n\n### 1. Memory\n\nEvery conversation with an AI agent generates a thread of messages. These threads are organized by resource IDs, making it easy to maintain context across multiple interactions. Whether you're building a customer support bot or a coding assistant, the agent can recall past conversations and maintain coherent dialogs.\n\n### 2. Workflow\n\nAI workflows often need to pause and resume, especially when waiting for user input or external data. Mastra Storage automatically handles the serialization and restoration of workflow states. This means your workflows can seamlessly pick up where they left off, maintaining their context and progress.\n\n### 3. Evaluation Data\n\nImproving AI systems requires measuring their performance. Our storage system keeps track of evaluation results, storing both the raw scores and the reasoning behind them. This data helps you understand how your agents are performing and where they need improvement.\n\n### 4. Observability\n\nDebugging AI applications can be challenging. That's why we integrated OpenTelemetry traces into our storage system. Every important operation is tracked, giving you visibility into what your agents are doing and why they made certain decisions.\n\n## Flexible by design\n\nWhile Mastra Storage provides a unified interface for all these features, we understand that different applications have different needs. That's why we have flexible database options. Here's how it works in practice.\n\nMastra comes with default storage (LibSQL) built-in, so you can start with just:\n\n```typescript copy\nimport { Mastra } from '@mastra/core/mastra';\nimport { agent } from './agents'; // Your agent\n\nconst mastra = new Mastra({\n  agents: { agent },\n});\n```\n\nIf you need to customize the storage location:\n\n```typescript copy\nimport { Mastra } from '@mastra/core/mastra';\nimport { DefaultStorage } from '@mastra/core/storage/libsql';\nimport { agent } from './agents';\n\nconst mastra = new Mastra({\n  agents: { agent },\n  storage: new DefaultStorage({\n    config: {\n      url: \"file:.mastra/mastra.db\"\n    }\n  })\n});\n```\n\nIf you want to use PostgreSQL for production, just switch by changing the storage provider:\n\n```typescript copy\nimport { Mastra } from '@mastra/core/mastra';\nimport { PostgresStore } from '@mastra/pg';\nimport { agent } from './agents';\n\nconst mastra = new Mastra({\n  agents: { agent },\n  storage: new PostgresStore({\n    connectionString: \"postgresql://user:password@localhost:5432/myapp\"\n  })\n});\n```\n\nThe unified interface allows data access patterns to remain the same, letting you bring the database you want.\n\n> **Note:** Switching databases requires migrating your existing data between backends.\n\n## Installation\n\nTo get started with Mastra Storage, first install the core package:\n\n```bash\nnpm install @mastra/core\n```\n\nThen add any storage providers you need:\n\n```bash\nnpm install @mastra/pg         # PostgreSQL\nnpm install @mastra/upstash    # Upstash\nnpm install @mastra/clickhouse # ClickHouse\n```\n\nTo learn more about Mastra Storage, check out our [documentation](https://mastra.ai/docs/storage/overview).\n\nWe're always looking to improve Mastra based on real-world usage patterns. If you're building AI applications, we'd love to [hear](https://discord.gg/BTYqqHKUrf) about your storage needs and how we can help address them."
  },
  {
    "metadata": {
      "title": "Mastra Changelog 2025-03-27",
      "publishedAt": "2025-03-27",
      "summary": "Mastra's Latest Updates: Nested Workflows, Voice to Voice, and more",
      "author": "Shane Thomas",
      "draft": false,
      "categories": ["changelogs"]
    },
    "slug": "changelog-2025-03-27",
    "content": "This week brings significant improvements across Mastra's ecosystem, with major updates to workflow capabilities, Mastra Voice, and more.\n\n## **Nested Workflows & Concurrency**\n\nThe core package received substantial enhancements to its workflow system. \n\nYou can now give a workflow directly as a step, which makes executing parallel branches easier:\n\n```tsx\nparentWorkflow\n  .step(nestedWorkflowA)\n  .step(nestedWorkflowB)\n  .after([nestedWorkflowA, nestedWorkflowB])\n  .step(finalStep);\n```\n\nConditional branching is also easier to follow with the new structure that accepts workflows. You can  execute a whole nested workflow, and then continue execution as normal on the next `.then()` ’d step:\n\n```tsx\n// Create nested workflows for different paths\nconst workflowA = new Workflow({ name: \"workflow-a\" })\n  .step(stepA1)\n  .then(stepA2)\n  .commit();\n\nconst workflowB = new Workflow({ name: \"workflow-b\" })\n  .step(stepB1)\n  .then(stepB2)\n  .commit();\n\n// Use the new if-else syntax with nested workflows\nparentWorkflow\n  .step(initialStep)\n  .if(\n    async ({ context }) => {\n      // Your condition here\n      return someCondition;\n    },\n    workflowA, // if branch\n    workflowB, // else branch\n  )\n  .then(finalStep)\n  .commit();\n```\n\nAnd to pass arguments to nested workflows, you can use variable mappings. \n\nWe also added new workflow result schemas to make it easier to propagate results back from a nested workflow execution back to the parent workflow:\n\n```tsx\n\n const workflowA = new Workflow({\n\t name: 'workflow-a',\n   result: {\n   schema: z.object({\n\t activities: z.string(),\n   }),\n   mapping: {\n   activities: {\n   step: planActivities,\n   path: 'activities',\n\t    },\n\t  },\n\t},\n})\n  .step(fetchWeather)\n  .then(planActivities)\n  .commit();\n\nconst weatherWorkflow = new Workflow({\n  name: 'weather-workflow',\n  triggerSchema: z.object({\n    city: z.string().describe('The city to get the weather for'),\n  }),\n  result: {\n    schema: z.object({\n      activities: z.string(),\n    }),\n    mapping: {\n      activities: {\n        step: workflowA,\n        path: 'result.activities',\n      },\n    },\n  },\n})\n  .step(workflowA, {\n    variables: {\n      city: {\n        step: 'trigger',\n        path: 'city',\n      },\n    },\n  })\n  .step(doSomethingElseStep)\n  .commit();\n```\n\nCheck out [our docs](https://mastra.ai/docs/workflows/nested-workflows) to explore all things nested workflows.\n\n## Voice\n\nWe [shipped some big changes and new features to Mastra Voice](https://mastra.ai/blog/speech-to-speech). They include: \n\n- Support for speech to speech providers, with OpenAI Realtime API being our first.\n- Support for WebSocket connections to establish a persistent connection to voice providers (like OpenAI) instead of separate HTTP requests. This enables bidirectional audio streaming without the request-wait-response pattern of traditional HTTP.\n- A new event-driven architecture for voice interactions. Instead of **`const text = await agent.voice.listen(audio)`**, you now use **`agent.voice.on('writing', ({ text }) => { ... })`**, creating a more responsive experience without managing any WebSocket complexity.\n\nHere's an example of setting up a real-time voice agent that can participate in continuous conversations:\n\n```tsx\nimport { Agent } from \"@mastra/core/agent\";\nimport { OpenAIRealtimeVoice } from \"@mastra/voice-openai-realtime\";\n\nconst agent = new Agent({\n  name: 'Agent',\n  instructions: `You are a helpful assistant with real-time voice capabilities.`,\n  model: openai('gpt-4o'),\n  voice: new OpenAIRealtimeVoice(),\n});\n\n// Connect to the voice service\nawait agent.voice.connect();\n\n// Listen for agent audio responses\nagent.voice.on('speaker', (stream) => {\n  // The 'speaker' can be any audio output implementation that accepts streams for instance, node-speaker\n  stream.pipe(speaker)\n});\n\n// Initiate the conversation by emitting a 'speaking' event\nawait agent.voice.speak('How can I help you today?');\n\n// Send continuous audio from microphone\nconst micStream = mic.getAudioStream();\nawait agent.voice.send(micStream);\n```\n\nAdditional details, including code examples of the new event-driven architecture, can be found in our [blog post](https://mastra.ai/blog/speech-to-speech).\n\n## New LLM Provider Options\n\nBoth Cerebras and Groq were added as LLM provider options in `create-mastra` expanding the available choices for AI backends. \n\n## Performance optimizations\n\nServerless deployments saw improvements like: \n\n- A marked reduction in bundle sizes. We went from 90mb to 8mb when using memory and our Postgres store\n- Improved deployments to Vercel, Netlify and Cloudflare edge functions\n- Improved cold start times\n\nFind details on our most recent optimizations [here](https://mastra.ai/blog/seamless-edge-deployments). \n\n## **Memory improvements**\n\nMemory operations also saw notable optimization, like:\n\n- The addition of a [new Mem0 integration](https://mastra.ai/docs/integrations#example-adding-the-mem0-integration) `@mastra/mem0`\n- Improved semantic recall for long message inputs\n- Complete compatibility for using memory with AI SDK's `useChat()` hooks\n\nStay tuned for more updates next week…"
  },
  {
    "metadata": {
      "title": "The Voice Connection: Mastra's Speech-to-Speech Capabilities",
      "publishedAt": "2025-03-26",
      "author": "Yujohn Nattrass",
      "summary": "We've expanded Mastra Voice with real-time speech-to-speech capabilities for Agents.",
      "draft": false,
      "categories": ["announcements"]
    },
    "slug": "speech-to-speech",
    "content": "Today we're expanding Mastra Voice with real-time speech-to-speech capabilities for Agents. We also made a few other updates supporting this feature.\n\n<iframe\n  width=\"100%\"\n  height=\"400\"\n  src=\"https://www.youtube.com/embed/jzRAwrA3R_c?si=BiFFOGb-4ifM7whD&rel=0\"\n  title=\"Speech-to-Speech Demo\"\n  frameBorder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n  allowFullScreen\n></iframe>\n\n\n\n\n## What's New\n\n- Support for speech to speech providers, with OpenAI Realtime API being our first.\n- We now support WebSocket connections to establish a persistent connection to voice providers (like OpenAI) instead of separate HTTP requests. This enables bidirectional audio streaming without the request-wait-response pattern of traditional HTTP.\n- We've introduced an event-driven architecture for voice interactions. Instead of `const text = await agent.voice.listen(audio)`, you now use `agent.voice.on('writing', ({ text }) => { ... })`, creating a more responsive experience without managing any WebSocket complexity.\n\nHere's an example of setting up a real-time voice agent that can participate in continuous conversations:\n\n```tsx\nimport { Agent } from \"@mastra/core/agent\";\nimport { OpenAIRealtimeVoice } from \"@mastra/voice-openai-realtime\";\n\nconst agent = new Agent({\n  name: 'Agent',\n  instructions: `You are a helpful assistant with real-time voice capabilities.`,\n  model: openai('gpt-4o'),\n  voice: new OpenAIRealtimeVoice(),\n});\n\n// Connect to the voice service\nawait agent.voice.connect();\n\n// Listen for agent audio responses\nagent.voice.on('speaker', (stream) => {\n  // The 'speaker' can be any audio output implementation that accepts streams for instance, node-speaker\n  stream.pipe(speaker)\n});\n\n// Initiate the conversation by emitting a 'speaking' event\nawait agent.voice.speak('How can I help you today?');\n\n// Send continuous audio from microphone\nconst micStream = mic.getAudioStream();\nawait agent.voice.send(micStream);\n```\n\n## Why speech-to-speech matters\n\nTraditional voice systems require multiple steps: converting speech to text, processing that text, and then converting text back to speech. Each transition introduces latency and can lose important aspects of communication like tone, emphasis, and natural pauses.\n\nSpeech-to-speech streamlines this process by maintaining audio as the primary medium throughout the interaction. This approach reduces latency and preserves more of the expressiveness in communication, resulting in more natural-feeling conversations.\n\nFor applications like language learning, customer service, or health coaching, this improved experience can make a significant difference in user engagement and effectiveness.\n\n## An Event-Based Approach\n\nFor our speech-to-speech implementation, we've introduced an event-driven approach alongside our existing voice API. While our standard voice methods continue to work as before for text-to-speech and speech-to-text, speech-to-speech providers use events for a more streaming-oriented experience:\n\n```tsx\n// Standard voice API (still fully supported)\nconst text = await agent.listen(audioStream); // Returns transcribed text\nconst audioStream = await agent.speak(\"Hello\"); // Returns audio stream\n\n// New event-based approach for speech-to-speech\n// Set up event listeners\nagent.voice.on('speaker', (stream) => {\n\tstream.pipe(speaker);\n});\n\nagent.voice.on('writing', ({ text, role }) => {\n  if (role === 'user') {\n    process.stdout.write(chalk.green(text));\n  } else {\n    process.stdout.write(chalk.blue(text));\n  }\n});\n\nagent.voice.on('error', (error) => {\n  console.error('Voice error:', error);\n});\n\nawait agent.voice.send(microphoneStream); // Sends audio\n\n// Trigger events\nawait agent.voice.speak(\"Hello\"); // Triggers 'speaking' events\nawait agent.voice.listen(\"Hi\"); // Triggers 'writing' events\n\n```\n\nThis event-based architecture offers two key advantages:\n\n1. **Continuous streaming** - Audio can be processed in chunks as it becomes available\n2. **Real-time feedback** - Speech is recognized and processed as it happens\n\nThe event system is particularly powerful for web and mobile applications where you need to update the interface in real-time or handle audio playback as it's being generated.\n\n## Adding Tools to Speech-to-Speech Conversations\n\nWhile each speech-to-speech provider implements tools in their own way, Mastra abstracts these differences away with a consistent interface. Any tools configured on your agent are automatically available to your voice provider:\n\n```tsx\nimport { Agent } from \"@mastra/core/agent\";\nimport { OpenAIRealtimeVoice } from \"@mastra/voice-openai-realtime\";\nimport { search, getWeather, fetchHoroscope } from \"./tools\";\n\nconst agent = new Agent({\n  name: 'Agent',\n  instructions: `You are a helpful assistant with speech-to-speech capabilities.`,\n  model: openai('gpt-4o'),\n  tools: {\n    search,\n    getWeather,\n    fetchHoroscope\n  },\n  voice: new OpenAIRealtimeVoice()\n});\n\n// Tools are automatically available to the voice system\nawait agent.voice.connect();\nawait agent.voice.speak(\"How can I help you today?\");\n```\n\n## Speech-to-Speech in Action\n\nWe're starting with OpenAI's Realtime API as our first speech-to-speech provider in Mastra, with plans to expand our provider options as they become available.\n\nHere's an example of how you might build a voice agent that handles microphone input and audio playback:\n\n```tsx\nimport Speaker from \"@mastra/node-speaker\";\nimport NodeMic from \"node-mic\";\nimport { mastra } from \"./mastra\";\nimport chalk from \"chalk\";\n\nconst agent = mastra.getAgent(\"dane\");\n\nif (!agent.voice) {\n  throw new Error(\"Agent does not have voice capabilities\");\n}\n\nlet speaker: Speaker | undefined;\n\nconst makeSpeaker = () =>\n  new Speaker({\n    sampleRate: 24100,  // Audio sample rate in Hz - standard for high-quality audio on MacBook Pro\n    channels: 1,        // Mono audio output (as opposed to stereo which would be 2)\n    bitDepth: 16,       // Bit depth for audio quality - CD quality standard (16-bit resolution)\n  });\n\nconst mic = new NodeMic({\n  rate: 24100,  // Audio sample rate in Hz - matches the speaker configuration for consistent audio processing\n});\n\nagent.voice.on(\"writing\", (ev) => {\n  if (ev.role === 'user') {\n    process.stdout.write(chalk.green(ev.text));\n  } else {\n    process.stdout.write(chalk.blue(ev.text));\n  }\n})\n\nagent.voice.on(\"speaker\", (stream) => {\n  if (speaker) {\n    speaker.removeAllListeners();\n    speaker.close(true);\n  }\n\n  mic.pause();\n  speaker = makeSpeaker();\n  \n  stream.pipe(speaker);\n\n  speaker.on('close', () => {\n    console.log(\"Speaker finished, resuming mic\");\n    mic.resume();\n  })\n})\n\n// Error from voice provider\nagent.voice.on(\"error\", (error) => {\n  console.error(\"Voice error:\", error);\n});\n\nawait agent.voice.connect();\n\nmic.start();\n\nconst microphoneStream = mic.getAudioStream();\nagent.voice.send(microphoneStream);\n\nagent.voice.speak('Hello how can I help you today?')\n```\n\nThis example demonstrates a browser assistant that listens to microphone input, processes it through a speech-to-speech agent, and plays the agent's responses. \n\nYou can find a complete demo in our [GitHub repository](https://github.com/mastra-ai/realtime-voice-demo) and try it out for yourself.\n\nWe're actively developing Mastra! If you encounter any issues or have suggestions for improvements, please open an issue on our GitHub repository or contribute directly with a pull request.\n\nGet started with speech-to-speech for Mastra agents today by installing the latest version of our packages:\n\n```tsx\nnpm install @mastra/core @mastra/voice-openai-realtime\n```\n\nThe full documentation is available [here](https://mastra.ai/docs/voice/voice-to-voice) with additional examples and configuration options."
  },
  {
    "metadata": {
      "title": "Optimizing Mastra for Seamless Edge Deployments",
      "publishedAt": "2025-03-25",
      "summary": "Exploring a recent handful of Mastra optimizations that help it run smoothly on edge functions",
      "author": "Ward Peeters",
      "draft": false,
      "categories": ["foundations"]
    },
    "slug": "seamless-edge-deployments",
    "content": "Serverless functions come with strict size limits in order to remain lightweight and responsive. This necessity poses a significant challenge when edge deploying feature-rich applications like Mastra.\n\nTo overcome this challenge, we recently made a handful of optimizations. Together, these optimizations helped us go from 90mb to 8mb (when using memory and our postgres store), improve cold start times, and enhance Mastra’s overall performance.\n\nLet’s take a look at how we did that… \n\n## The challenge: Node.js dependencies\n\nOne of the biggest hurdles in the Node.js ecosystem is the management of dependencies. A single **`npm install`** can inadvertently introduce hundreds—or even thousands!—of transitive dependencies. \n\nHere’s how this often plays out in practice:\n\n- A simple utility package may pull in complex layers of dependencies.\n- These dependencies often contain code for features we’ll never utilize.\n- Every additional dependency increases the size of our deployment bundle.\n- Many packages offer both browser-specific and Node.js specific code, even when only one is needed.\n\n![A visualization showing how node_modules are a nightmare](/images/node_modules.png)\n\n\nThis \"dependency bloat\" can quickly lead to exceeding edge function size limits, making successful deployment a challenge without effective optimization.\n\n## Our solution: smart bundling techniques\n\n### 1. Cleanup transitive dependencies by pre-bundling\n\nAs mentioned above, node_modules can be large and bloated. We optimize our dependencies by analyzing what exports are used from a dependency and pre-bundling them so we don’t need to install all transitive dependencies.\n\nAssume the following code:\n\n```tsx\nimport { Agent } from '@mastra/core/agent';\nimport {openai} from '@ai-sdk/openai'\n\nexport const myAgent = new Agent({\n  instructions: \\`You're an helpful assistant\\`,\n  model: openai('gpt-4o')\n})\n```\n\nBy pre-bundling the `Agent` class, we can remove all unused dependencies, such as `@libsql/client`, and reduce the bundle size. The pre-bundled code can be found in the `.mastra/.build` folder.\n\n[Check the bundle size out for yourself here!](https://bundlejs.com/?q=%40mastra%2Fcore%2Fagent&treeshake=%5B%7B+Agent+%7D%5D&config=%7B%22esbuild%22%3A%7B%22external%22%3A%5B%22%40opentelemetry%2Fcore%22%2C%22%40opentelemetry%2Fresources%22%5D%7D%7D)\n\n### **2. Analyzing the Mastra Instance**\n\nThe deployer option in the Mastra class is intended for deploying the Mastra server but is not utilized within Mastra itself. We are analyzing the Mastra class and remove the deployer dependency as a whole:\n\n\n```tsx\nimport { Mastra } from '@mastra/core/mastra';\nimport { VercelDeployer } from '@mastra/deployer-vercel';\n\nexport const mastra = new Mastra({\n  deployer: new VercelDeployer({\n    //args\n  })\n});\n\n//      ↓ ↓ ↓ ↓ ↓ ↓\n\nimport { Mastra } from '@mastra/core/mastra';\n\nexport const mastra = new Mastra({\n});\n```\n\n### 3. Remove unused code\n\nUsing a bundler-like Rollup, we can analyze the code, remove unused exports, and reduce the overall bundle size. This process, known as tree-shaking, helps eliminate dead code not used in your application. \n\nTree-shaking is particularly valuable in larger projects where dependencies might include many features that your application doesn't utilize. The bundler analyzes the import/export statements and only includes code that's actually referenced in the final bundle. \n\nConsider the following example:\n\n```tsx\n// file1.js\nexport function myFunction() {\n  return 'Hello, world!';\n}\n\nexport function myOtherFunction() {\n  return 'Hello, world!';\n}\n\n// file2.js\nimport { myFunction } from './file1';\n\nmyFunction();\n```\n\n\nWithout tree-shaking, the final code that will be executed looks like:\n\n```tsx\n\nfunction myFunction() {\n  return 'Hello, world!';\n}\n\nfunction myOtherFunction() {\n  return 'Hello, world!';\n}\n\nmyFunction();\n```\n\nBy enabling tree-shaking, we can remove the unused function called `myOtherFunction`\n\n```tsx\nfunction myFunction() {\n  return 'Hello, world!';\n}\n\nmyFunction();\n```\n\n## Next steps\n\nWe believe there’s still room for improvement and are currently considering:\n\n- Removing all default dependencies when unused like **`@libsql/client`** and **`fastembed`**.\n- Improving native dependency detection and extraction\n\nLook out for future changelogs where we’ll share updates on our progress.\n\nAnd if you’re interested in learning more about our optimization journey or have suggestions for improvement, we’d love to hear from you. Join our [Discord](https://discord.com/invite/BTYqqHKUrf) or check out our [documentation](https://mastra.ai/docs) for more technical insights."
  },
  {
    "metadata": {
      "title": "Mastra Changelog 2025-03-20",
      "publishedAt": "2025-03-20",
      "summary": "Mastra MCP documentation server, AgentNetwork updates, and more",
      "author": "Sam Bhagwat",
      "draft": false,
      "categories": ["changelogs"]
    },
    "slug": "changelog-2025-03-20",
    "content": "This week we launched our very own MCP documentation server, shipped improvements to AgentNetwork, and continued to make enhancements to workflows.\n\n## Mastra MCP Documentation Server\n\n<iframe\n  width=\"560\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/vciV57lF0og?si=4i5xFi34rjDFBJBJ&rel=0\"\n  title=\"Mastra MCP documentation server\"\n  frameBorder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n  allowFullScreen\n  className=\"w-full rounded-lg border border-border-1\"\n></iframe>\n\n\nAs per user requests, we built a documentation server that allows an AI assistant or IDE with MCP support (like Cursor, Windsurf, or Cline) to query Mastra's complete knowledge base. \n\nHere are a few sample prompts you might try using with our documentation server:\n\n- \"Add evals to my agent and write tests\"\n- \"Does Mastra work with the AI SDK?\"\n- \"I'm running into a bug with agent memory, have there been any related changes or bug fixes recently?\"\n\nWe've seen that coding agents using this system are able to find the information they need to complete a task more quickly and efficiently.\n\nTo install, run **`pnpm create mastra@latest`** and select Cursor or Windsurf when prompted to install the MCP server. Further details on the feature can be found [here](https://mastra.ai/blog/introducing-mastra-mcp).\n\n## New AgentNetwork primitive\n\nThe `AgentNetwork` class provides a way to create a network of specialized agents that can collaborate to solve complex tasks. Unlike Workflows, which require explicit control over execution paths, `AgentNetwork` uses an LLM-based router to dynamically determine which agent to call next. \n\nIn its latest implementation, the `RoutingAgent` has access to a single tool called \"transmit.” This tool handles:\n\n- Single agent calls\n- Multiple parallel agent calls\n- Context sharing between agents\n\n`AgentNetwork` [remains an experimental feature](https://mastra.ai/docs/reference/networks/agent-network) but is now available in Mastra’s alpha and latest versions.\n\n## Workflow improvements\n\nThe latest workflow updates focused on enhancing control flow and execution handling:\n\n- We're also exposing event functionality in workflows now. A major addition is experimental `.afterEvent()` support. This provides more flexible workflow orchestration.\n    \n```typescript\nexport const queryProcessingWorkflow = new Workflow({\n    name: 'query-processing-workflow',\n    // Define the schema for the initial trigger data\n    triggerSchema: z.object({\n        query: z.string().describe('The user query'),\n    }),\n    // Define events that can be triggered during workflow execution\n    events: {\n        userFeedback: {\n            schema: z.object({\n                feedback: z.string().describe('User feedback on the initial response'),\n            }),\n        },\n    },\n});\n\n\nqueryProcessingWorkflow\n    .step(generateResponse)\n    .afterEvent('userFeedback')\n    .step(improveResponse)\n    .commit();\n```\n    \n- Suspend/resume functionality was improved, with suspend metadata now being stored in `context.resumeData` for better state management.\n- Several bugs were fixed, including issues with the `.after()` method execution when handling multiple passes, proper execution of else branches when if-branches contain loops, and corrected handling of skipped conditions on awaited steps.\n- Finally, the workflow visualization in playground UI was enhanced with resizable panels (note: we also improved agent visualization).\n\nThat’s all the major updates for this week! 🚀\n\nStay tuned for more… \n\n… and if you’re impatient, here are 3 emojis as a hint for what’s coming next: 👤💬👤"
  },
  {
    "metadata": {
      "title": "Introducing Mastra MCP Documentation Server",
      "publishedAt": "2025-03-18",
      "summary": "We built a Mastra MCP documentation server that provides AI assistants and IDE's with direct access to Mastra.ai's complete knowledge base. ",
      "author": "Tyler Barnes",
      "draft": false,
      "categories": ["announcements"]
    },
    "slug": "introducing-mastra-mcp",
    "content": "<iframe\n  width=\"560\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/vciV57lF0og?si=rZ--01obYadPBkh0&rel=0\"\n  title=\"Mastra MCP documentation server\"\n  frameBorder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n  allowFullScreen\n  className=\"w-full rounded-lg border border-border-1\"\n></iframe>\n\nWe built a Mastra MCP documentation server that provides AI assistants and IDE's with direct access to Mastra.ai's complete knowledge base.\n\n## What is MCP?\n\nMCP is an emerging standard for providing AI models with structured, real-time access to domain-specific knowledge bases and documentation. It was developed in late 2024 by Anthropic researchers and, since it is OSS, has continued to evolve thanks to commmunity contributions. The protocol standardizes how AI models access external knowledge sources in real-time, building on earlier work in RAG.\n\n## What we built\n\n[We've already had MCP support for a while](https://mastra.ai/docs/reference/tools/mcp-configuration), but it was only available to our own agents. Now we've built a documentation server that allows any AI assistant or IDE to query Mastra's complete knowledge base.\n\nOur MCP documentation server builds on concepts from `llms.txt` (exposing documentation as LLM context) but expands on it by enabling agents to query specific contextual information about Mastra. Since the agent can make additional queries when it needs to, minimal context is used in comparison to `llms.txt`.\n\nWe've seen that coding agents using this system are able to find the information they need to complete a task more quickly and efficiently. This is because the context window isn't full of documentation info that isn't relevant to the specific task at hand. Instead, only the information the agent queries is added to context.\n\nWhen an agent encounters something that doesn't work correctly it can check code examples and package changelogs to see if it's not using an API the right way (or if a new package version has been released with a bug fix).\n\nIn the future we may expand our docs server to allow the agent to query Github issues to find if anyone else is encountering a problem, or prefill a Github issue template and open it in the users browser.\n\n## Getting Started\n\n`@mastra/mcp-docs-server` provides direct access to Mastra's complete knowledge base in Cursor, Windsurf, Cline, or any other IDE that supports MCP.\n\nTo install, run `pnpm create mastra@latest` and select Cursor or Windsurf when prompted to install the MCP server.\n\n### Enabling Mastra's MCP server in Cursor\n\n1. Open Cursor settings\n2. Navigate to MCP settings\n3. Click \"enable\" on the Mastra MCP server\n4. If you have an agent chat open, you'll need to re-open it or start a new chat to use the MCP server\n\n### Enabling Mastra's MCP server in Windsurf\n\n1. Fully quit and re-open Windsurf\n2. If tool calls start failing, go to Windsurfs MCP settings and re-start the MCP server. This is a common Windsurf MCP issue and isn't related to Mastra. Right now Cursor's MCP implementation is more stable than Windsurfs is.\n\nNote: In both IDEs it may take a minute for the MCP server to start the first time as it needs to download the package from npm.\n\n## Enabling Mastra's MCP server in other IDEs\n\nDetails on manual installation for other IDEs can be found [here](https://mastra.ai/docs/getting-started/mcp-docs-server).\n\n## Example Usage\n\nOnce it's installed in your IDE you can write prompts and assume the agent will understand everything about Mastra.\n\nHere are some prompts you might try:\n\n- \"Add evals to my agent and write tests\"\n- \"Does Mastra work with the AI SDK?\"\n- \"I'm running into a bug with agent memory, have there been any related changes or bug fixes recently?\"\n\nWe're excited to see how Mastra's MCP server can help you build better agents. If you have any feedback or feature requests, please let us know on [Discord](https://discord.gg/mastra)."
  },
  {
    "metadata": {
      "title": "Mastra Changelog 2025-03-14",
      "publishedAt": "2025-03-14",
      "summary": "Major workflow enhancements, new Mastra Voice features, and more",
      "author": "Sam Bhagwat",
      "draft": false,
      "categories": ["changelogs"]
    },
    "slug": "changelog-2025-03-14",
    "content": "This week we launched major workflow enhancements, new features to Mastra Voice, and improvements to memory and vector management.\n\nWe’re also sharing a preview of `AgentNetwork`, a new multi-agent collaboration system. This feature isn’t fully shipped yet so we are open to feedback.\n\n## Workflow enhancements\n\nThere were two major workflow control flow improvements added this week, as well as one experimental one:\n\n**While/Until Loops**: Added new looping constructs that allow steps to repeat until a condition is met.\n\nHere’s a basic workflow with a `while` loop:\n\n```typescript\nworkflow\n  .step(incrementStep)\n  .while(async ({ context }) => {\n    // Continue as long as the value is less than 10\n    const result = context.getStepResult<{ value: number }>(\"increment\");\n    return (result?.value ?? 0) < 10;\n  }, incrementStep)\n  .then(finalStep);\n```\n\nAnd here’s the same workflow using `until` instead:\n\n```typescript\nworkflow\n  .step(incrementStep)\n  .until(async ({ context }) => {\n    // Stop when the value reaches or exceeds 10\n    const result = context.getStepResult<{ value: number }>(\"increment\");\n    return (result?.value ?? 0) >= 10;\n  }, incrementStep)\n  .then(finalStep);\n```\n\nYou can also use a reference based condition:\n\n```typescript\nworkflow\n  .step(incrementStep)\n  .until(\n    {\n      ref: { step: incrementStep, path: \"value\" },\n      query: { $gte: 10 },\n    },\n    incrementStep,\n  )\n  .then(finalStep);\n```\n\n**Compound After Syntax**: Enhanced the `.after` dependency syntax to support more complex step orchestration.\n\nThe code below shows how to branch after stepA and later converge on stepF:\n\n```typescript\nmyWorkflow\n  .step(stepA)\n  .then(stepB)\n  .then(stepD)\n  .after(stepA)\n  .step(stepC)\n  .then(stepE)\n  .after(stepD)\n  .step(stepF)\n  .step(stepE);\n```\n\nIn this example:\n\n- `stepA` leads to `stepB`, then to `stepD`.\n- Separately, `stepA` also triggers `stepC`, which in turn leads to `stepE`.\n- Separately, `stepD` also triggers `stepF` and `stepE` in parallel.\n\n**If-Else Branching**: Added experimental support for conditional branching between workflow steps. This allows workflows to take different paths based on conditions.\n\nThe `.else()` method creates an alternative branch in the workflow that executes when the preceding `if` condition evaluates to false:\n\n```typescript\nworkflow\n  .step(startStep)\n  .if(async ({ context }) => {\n    const value = context.getStepResult<{ value: number }>(\"start\")?.value;\n    return value < 10; // If true, execute the \"if\" branch\n  })\n  .then(ifBranchStep)\n  .else() // Alternative branch when the condition is false\n  .then(elseBranchStep)\n  .commit();\n```\n\nAll these changes allow for more complex workflow patterns while maintaining readability.\n\n## New Mastra Voice features\n\nWe launched 3 new features for [Mastra Voice](https://mastra.ai/blog/giving-agents-a-voice):\n\n- We now support WebSocket connections to establish a persistent connection to voice providers like OpenAI (instead of separate HTTP requests.) This enables bidirectional audio streaming without the request-wait-response pattern of traditional HTTP.\n- We've introduced an event-driven architecture for voice interactions. Instead of `const text = await agent.voice.listen(audio)`, you now use `agent.voice.on('writing', ({ text }) => { ... }).` This creates a more responsive experience without needing to manage any WebSocket complexity.\n- We are adding support for speech to speech providers, with OpenAI Realtime API being our first.\n\n## Memory & vector management\n\nThe team has introduced more granular control over vector storage with the ability to update and delete specific index entries by ID. This provides better data management capabilities without requiring full index rebuilds.\n\nMemory thread management received a security enhancement through resourceId validation, ensuring proper access control:\n\n```typescript\nconst memory = new Memory();\n\n// Now validates resource ownership\nawait memory.query({\n  threadId: \"thread_123\",\n  resourceId: \"user_456\", // Throws if thread doesn't belong to this resource\n});\n```\n\n## Sneak peek: AgentNetwork\n\nThe `AgentNetwork` feature allows you to create collaborative agent systems where multiple agents can work together, share context, and coordinate on complex tasks. The system handles memory synchronization automatically between agents, ensuring each has access to relevant shared context without manual management.\n\n<iframe\n  width=\"560\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/K4-HrIGXSGQ?si=7YVWQ4bndDA1J5p4&rel=0\"\n  title=\"AgentNetwork Demo\"\n  frameBorder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n  allowFullScreen\n></iframe>\n\nWhile experimental, `AgentNetwork` is [available to try out](https://github.com/mastra-ai/mastra/pull/2744).\n\nFor reference, this is how to start building a research agent network consisting of a researcher and summary agent:\n\n```typescript\nimport { AgentNetwork } from \"@mastra/core/network\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\n\n// Create specialized agents\nconst researchAgent = new Agent({\n  name: \"Research\",\n  instructions: \"You search for and gather information on topics\",\n  model: openai(\"gpt-4o\"),\n});\n\nconst summaryAgent = new Agent({\n  name: \"Summary\",\n  instructions: \"You summarize information into concise points\",\n  model: openai(\"gpt-4o\"),\n});\n\n// Create a network with LLM-based routing\nexport const researchNetwork = new AgentNetwork({\n  name: \"Research Assistant\",\n  instructions:\n    \"This network researches topics and provides summarized information.\",\n  agents: [researchAgent, summaryAgent],\n  routingModel: openai(\"gpt-4o\"),\n});\n```\n\nStay tuned for more updates next week 🚀"
  },
  {
    "metadata": {
      "title": "Announcing our new book: Principles of Building AI agents",
      "publishedAt": "2025-03-12",
      "author": "Sam Bhagwat",
      "summary": "Mastra founder Sam Bhagwat is excited to announce the release of our new book: Principles of Building AI agents. This book is a guide for developers who want to rapidly build AI applications.",
      "draft": false,
      "categories": ["announcements"]
    },
    "slug": "principles-of-ai-engineering",
    "content": "![Sam, Shane, and Abhi with the Principles of Building AI agents book](/images/booklaunch.png)\n\nToday is YC demo day and we're excited to announce the release of our new book: Principles of Building AI agents. While we've been giving print copies away to attendees, the [digital version is available to everyone](https://mastra.ai/book).\n\nThe book covers everything you need to get started building AI agents and applications, such as: the basics of choosing a model, prompt engineering 101, building your first agent, workflows and evals, and even serverless deployment.\n\nHere's a glimpse at the foreward, told directly in my (Sam's) voice. It gets at why we wrote the book:\n\n## Foreward\n\nFor the last three months, I’ve been holed up in an apartment in San Francisco’s Dogpatch district with my cofounders, Shane Thomas and Abhi Aiyer.\n\nWe’re building an open-source JavaScript frame- work called Mastra to help people build their own AI agents and assistants.\n\nWe’ve come to the right spot.\n\nWe’re in the Winter 2025 batch of YCombinator, the most popular startup incubator in the world (colloquially, YC W25).\n\nOver half of the batch is building some sort of “vertical agent” — AI application generating CAD diagrams for aerospace engineers, Excel financials for private equity, a customer support agent for iOS apps.\n\nThese three months have come at some personal sacrifice.\n\nShane has traveled from South Dakota with his girlfriend Elizabeth, their three-year-old daughter and newborn son. I usually have 50-50 custody of my seven-year-old son and five-year-old daughter, but for these three months I’m down to every-other- weekend. Abhi’s up from LA, where he bleeds Lakers purple and gold.\n\nOur backstory is that Shane, Abhi and I met while building a popular open-source JavaScript website framework called Gatsby. I was the co- founder, and Shane and Abhi were key engineers.\n\nWhile OpenAI and Anthropic’s models are widely available, the secrets of building effective AI applications are hidden in niche Twitter/X accounts, in-person SF meetups, and founder groupchats.\n\nBut AI engineering is just a new domain, like data engineering a few years ago, or DevOps before that. It’s not impossibly complex. An engineer with a framework like Mastra should be able get up to speed in a day or two. With the right tools, it’s easy to build an agent as it is to build a website.\n\nThis book is intentionally a short read, even with the code examples and diagrams we’ve included. It should fit in your back pocket, or slide into your purse. You should be able to use the code examples and get something simple working in a day or two.\n\n## Getting the book\n\nThe book is [available as a download to everyone online](https://mastra.ai/book)."
  },
  {
    "metadata": {
      "title": "AI Beats Laboratory: A Multi-Agent Music Generation System",
      "summary": "A demo using Mastra to show how multiple AI agents can work together to generate music",
      "author": "Abhi Aiyer",
      "publishedAt": "2025-03-11",
      "draft": false,
      "categories": ["case studies"]
    },
    "slug": "ai-beats-lab",
    "content": "<iframe\n  width=\"560\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/M_v2cyRUXgY?si=d0H-7Pl7uNWiRICK&rel=0\"\n  title=\"AI Beats Lab Demo\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n  allowFullScreen\n  className=\"rounded-lg border border-border-1 w-full\"\n></iframe>\n\nThe AI Beats Laboratory is an interactive web application that generates musical beats and melodies using AI agents. Here's how it works:\n\n## Agents\n\nThe system uses two specialized Mastra agents:\n\n- A music reference agent that analyzes musical styles and references\n- A music generation agent that creates drum patterns and melodies\n\nHere's the reference agent definition:\n\n```typescript\nexport const musicReferenceAgent = new Agent({\n  name: \"music-reference-agent\",\n  instructions: `\n    You are given a style of music, an artist or song as a reference point. \n    First think about what keys and what drum patterns fit this reference point.\n    Based on this knowledge, generate a drum pattern and a minimal melody that fits the style.\n    Pick a key based on the style of the music. All notes should be in this key.\n    `,\n  model: {\n    provider: \"ANTHROPIC\",\n    name: \"claude-3-5-sonnet-20241022\",\n    toolChoice: \"auto\",\n  },\n});\n```\n\nHere's the music generation agent definition:\n\n```typescript\nexport const musicAgent = new Agent({\n  name: \"music-agent\",\n  instructions: `\n    \n    For the pianoSequence:\n    - Create wonderful melodies\n    - Available notes:\n      * High register: ['C5', 'B4', 'A4', 'G4']\n      * Middle register: ['F4', 'E4', 'D4', 'C4']\n      * Low register: ['B3', 'A3', 'G3']\n    - Each note should have an array of step numbers (0-15)\n    For the drumSequence:\n    - Available sounds:\n      * Core rhythm: ['Kick', 'Snare', 'HiHat']\n      * Accents: ['Clap', 'OpenHat', 'Crash']\n      * Percussion: ['Tom', 'Ride', 'Shaker', 'Cowbell']\n    - Each sound should have an array of step numbers (0-15)\n    Response format must be:\n    {\n      \"pianoSequence\": {\n        \"C5\": [numbers],\n        \"B4\": [numbers],\n        // ... other piano notes\n      },\n      \"drumSequence\": {\n        \"Kick\": [numbers],\n        \"Snare\": [numbers],\n        // ... other drum sounds\n      }\n    }\n`,\n  model: anthropic(\"claude-3-5-sonnet-20241022\"),\n});\n```\n\nIt turns out LLMs are not very good at music so most of the time was spent iterating on the system prompt. Anthropic’s Claude 3.5 Sonnet was better than OpenAI’s 4o.\n\n## User Interface Components\n\nThe main interface is built around the Sequencer component which provides:\n\n- A 16-step grid for both piano notes and drum sounds\n- Interactive controls for playing/stopping sequences\n- Tempo controls\n- Export/share functionality\n- AI generation controls\n\nThe sequencer layout is defined in:\n\n```typescript\nconst STEPS = 16;\nconst PIANO_NOTES = [\n  \"C5\",\n  \"B4\",\n  \"A4\",\n  \"G4\",\n  \"F4\",\n  \"E4\",\n  \"D4\",\n  \"C4\",\n  \"B3\",\n  \"A3\",\n  \"G3\",\n];\nconst DRUM_SOUNDS = [\n  \"Kick\",\n  \"Snare\",\n  \"HiHat\",\n  \"Clap\",\n  \"OpenHat\",\n  \"Tom\",\n  \"Crash\",\n  \"Ride\",\n  \"Shaker\",\n  \"Cowbell\",\n];\n```\n\n## Audio System\n\nThe application uses the Web Audio API for sound generation. The audio system is initialized with:\n\n```typescript\n// Create a single audio context for the entire application\nlet audioContext: AudioContext | null = null;\n\nexport const getAudioContext = () => {\n  if (!audioContext) {\n    audioContext = new AudioContext();\n    // Resume audio context on creation to handle auto-play restrictions\n    audioContext.resume();\n  }\n  return audioContext;\n};\n```\n\nPiano notes are mapped to frequencies:\n\n```typescript\nconst NOTE_FREQUENCIES: { [key: string]: number } = {\n  C5: 523.25,\n  B4: 493.88,\n  A4: 440.0,\n  G4: 392.0,\n  F4: 349.23,\n  E4: 329.63,\n  D4: 293.66,\n  C4: 261.63,\n  B3: 246.94,\n  A3: 220.0,\n  G3: 196.0,\n};\n```\n\n## Generation Flow\n\nWhen a user requests a new beat:\n\n- The user enters a prompt describing their desired musical style\n- The music reference agent analyzes the prompt and provides musical context\n- The music generation agent creates patterns based on this context\n- The patterns are rendered in the sequencer grid\n\nThe generation process is handled in:\n\n```typescript\nconst handleGenerateSequence = async () => {\n  if (!prompt) return;\n  setIsGenerating(true);\n\n  try {\n    const ctx = getAudioContext();\n    ctx.resume();\n\n    // First, get musical analysis from reference agent\n    const refAgent =\n      getMastraFetchUrl() + \"/api/agents/musicReferenceAgent/generate\";\n    const response = await window.fetch(refAgent, {\n      method: \"POST\",\n      headers: { \"Content-Type\": \"application/json\" },\n      body: JSON.stringify({\n        messages: [`Please analyze the users request \"${prompt}\"`],\n      }),\n    });\n\n    const d = await response.json();\n    setReference(d.text);\n\n    // Then, generate the actual beat pattern using music agent\n    const uri = getMastraFetchUrl() + \"/api/agents/musicAgent/generate\";\n    const result = await window.fetch(uri, {\n      method: \"POST\",\n      headers: { \"Content-Type\": \"application/json\" },\n      body: JSON.stringify({\n        messages: [\n          `Please make me a beat based on this information: ${d.text}`,\n        ],\n        output: {\n          // ... JSON schema defining required notes and drum sounds\n          // Each property (C5, B4, Kick, Snare, etc.) expects an array of integers\n          // representing the steps where that note/sound should play\n        },\n      }),\n    });\n\n    const data = await result.json();\n\n    // Map the response data to piano and drum sequences\n    const pianoSequence = {\n      C5: data.object.C5 || [],\n      B4: data.object.B4 || [],\n      // ... additional piano notes C5 through G3\n    };\n\n    const drumSequence = {\n      Kick: data.object.Kick || [],\n      Snare: data.object.Snare || [],\n      // ... additional drum sounds\n    };\n\n    setDrumSequence(drumSequence);\n    setPianoSequence(pianoSequence);\n    stopSequence();\n  } catch (error) {\n    console.error(\"Error generating sequence:\", error);\n  } finally {\n    setIsGenerating(false);\n  }\n};\n```\n\n## Sharing and Export\n\nThe system [supports](https://github.com/mastra-ai/ai-beat-lab/blob/1338a851dcf0a60c4ef32bd643a6efdbdd5b3a1e/src/components/Sequencer.tsx#L95):\n\n- Sharing beats via URL encoding\n- Exporting to MIDI format\n- Generating variations of existing patterns\n\nBy the way, you can find all the code on [Github](https://github.com/mastra-ai/ai-beat-lab) and try the demo yourself [here](https://ai-beat-lab.lovable.app/)."
  },
  {
    "metadata": {
      "title": "Integrating Mastra with Next.js: Two Approaches for AI-Powered Apps",
      "publishedAt": "2025-03-06",
      "summary": "A comprehensive guide to adding Mastra's agent capabilities to your Next.js applications, with flexible integration options for projects of any size",
      "author": "Ehindero Israel",
      "draft": false,
      "categories": ["examples"]
    },
    "slug": "nextjs-integration-guide",
    "content": "One of the most common questions we've been receiving in our Discord and during our Live Workshops is: **\"How do I integrate Mastra with my Next.js application?\"** While we've supported Next.js for a while, we realized we needed to provide clearer guidance on the different integration approaches.\n\nToday, we're excited to share our [comprehensive guide](/docs/frameworks/01-next-js) on integrating Mastra with Next.js, offering two flexible approaches to fit different project needs.\n\n## Why Next.js + Mastra?\n\nNext.js has become the go-to React framework for building modern web applications, offering an excellent developer experience with features like server components, API routes, and optimized builds. Combining Next.js with Mastra's agent capabilities creates a powerful foundation for AI-powered applications.\n\nThis integration lets you:\n\n- Build AI agents directly into your web application\n- Leverage Next.js server components and server actions for AI processing\n- Create streaming AI responses with minimal client-side code\n- Deploy everything together or as separate services based on your scaling needs\n\n## Two Integration Approaches\n\nWe've documented two main ways to integrate Mastra with your Next.js applications, each with its own advantages:\n\n![Diagram showing the two Next.js integration approaches](/images/nextjs/MastraNextJSIntegration.png)\n\n### 1. Separate Backend Integration\n\nThis approach is ideal for larger projects where you want to:\n\n- Scale your AI backend independently from your frontend\n- Maintain a clear separation of concerns\n- Have more deployment flexibility\n- Support multiple frontends with the same AI backend\n\n```typescript\n// In your Next.js app\nimport { MastraClient } from \"@mastra/client-js\";\n\n// Initialize the client\nexport const mastraClient = new MastraClient({\n  baseUrl: process.env.NEXT_PUBLIC_MASTRA_API_URL || \"http://localhost:4111\",\n});\n\n// Use it in your components\nconst agent = mastraClient.getAgent(\"weatherAgent\");\nconst response = await agent.generate(`What is the weather like in ${city}?`);\n```\n\nWith this approach, your Mastra backend runs as a separate service, and your Next.js application communicates with it via the client library. This gives you the flexibility to scale each part independently and even support multiple frontends with the same AI backend.\n\n> **Want to skip the backend setup?** [Request access to the Mastra Cloud Private Beta](https://mastra.ai/cloud-beta) to instantly deploy your Mastra backend services without managing infrastructure.\n\n### 2. Direct Integration\n\nFor smaller projects or prototypes, you can integrate Mastra directly into your Next.js application:\n\n```typescript\n// In your Next.js server component or API route\nimport { mastra } from \"@/mastra\";\n\nexport async function getWeatherInfo(city: string) {\n  const agent = mastra.getAgent(\"weatherAgent\");\n  const result = await agent.generate(`What is the weather like in ${city}?`);\n  return result;\n}\n```\n\nThis approach bundles Mastra directly with your Next.js application, simplifying deployment and development. It's perfect for prototypes, MVPs, or smaller applications where you don't need to scale your AI backend separately.\n\n## Server Actions + Mastra: A Perfect Match\n\nNext.js 14's server actions are particularly well-suited for AI agent integration. They allow you to run your AI agents on the server while providing a simple interface for client components:\n\n```typescript\n// app/actions.ts\n'use server'\n\nimport { mastra } from '@/mastra'\n\nexport async function getWeatherInfo(city: string) {\n  const agent = mastra.getAgent('weatherAgent')\n  const result = await agent.generate(`What is the weather like in ${city}?`)\n  return result\n}\n\n// app/components/Weather.tsx\n'use client'\n\nimport { getWeatherInfo } from '../actions'\n\nexport function Weather() {\n  async function handleSubmit(formData: FormData) {\n    const city = formData.get('city') as string\n    const result = await getWeatherInfo(city)\n    // Handle the result\n  }\n\n  return (\n    <form action={handleSubmit}>\n      <input name=\"city\" placeholder=\"Enter city name\" />\n      <button type=\"submit\">Get Weather</button>\n    </form>\n  )\n}\n```\n\nThis pattern keeps your API keys and agent logic secure on the server while providing a clean, type-safe interface for your client components.\n\n## Getting Started\n\nTo get started with Mastra and Next.js, you have two options:\n\n1. **For separate backend integration:**\n\n   - Create a new Mastra project with `npx create-mastra@latest`\n   - Install the client in your Next.js app with `npm install @mastra/client-js`\n   - Connect your Next.js app to your Mastra backend\n\n2. **For direct integration:**\n   - Navigate to your Next.js project root\n   - Run `npx mastra@latest init` to set up Mastra in your project\n   - Configure your `next.config.js` to include Mastra packages\n\nFor detailed step-by-step instructions, check out our [comprehensive guide in the documentation](https://mastra.ai/docs/frameworks/01-next-js).\n\n## Go Build Something Great!\n\nWhether you're building a small prototype or a large-scale AI application, Mastra and Next.js provide a powerful combination for creating AI-powered web experiences.\n\nWe're excited to see what you build with Mastra and Next.js!"
  },
  {
    "metadata": {
      "title": "Why We're All-In on MCP",
      "publishedAt": "2025-03-05",
      "summary": "A deep dive into why Mastra is adopting Model Context Protocol (MCP) for AI tool integration, comparing it with Agents.json and Composio, addressing current limitations in discovery, quality, and configuration, and providing a framework-friendly proposal with practical code examples for implementation.",
      "author": "Tyler Barnes",
      "draft": false,
      "categories": ["foundations"]
    },
    "slug": "mastra-mcp",
    "content": "Tool integration for AI agents is a mess.\nEven for developers building agents every day, it's frustrating to navigate the fragmented ecosystem. Finding high-quality tools is difficult - search for \"MCP Calendar integration\" and you'll find ten different implementations with no way to know which one is best.\n\nIf you've ever tried to integrate tools with an LLM, you know this pain. Tool discovery, installation, and configuration aren't solved problems yet.\n\nAt Mastra we've been going deep on this topic to find the best solution for our users. So lets talk about agent tool standards and why we believe Anthropic's Model Context Protocol (MCP) is the **future of agent-tool interaction.**\n\n## Emerging Tool Standards\n\nAs with any new technology space, several competing approaches are emerging:\n\n### [Agents.json](https://docs.wild-card.ai/agentsjson/introduction) by Wildcard\n\nAgents.json is an open specification that extends OpenAPI to optimize API interactions for LLMs. Built by [Wildcard AI](https://wild-card.ai), it addresses a fundamental challenge in the agent ecosystem: the disconnect between how APIs are designed for developers versus what LLMs need to use them effectively.\n\nAgents.json solves the problem of LLMs struggling with complex API sequences. Instead of requiring API providers to create new agent-specific endpoints or infrastructure, it enriches existing API endpoints with the context LLMs need to execute reliable chains of actions.\n\n### [Composio](https://composio.dev)\n\nComposio has its own tool specification and a comprehensive library of pre-built integrations. Until recently, Composio was primarily an option for teams prioritizing immediate access to quality tools over architectural flexibility, with potential concerns about ecosystem lock-in due to its closed-source components. However, Composio [just announced](https://x.com/composiohq/status/1896968949654495291) they've added MCP support to their existing platform. This means teams can now choose to use either Composio's native specification or the MCP standard when working with their tool ecosystem.\n\n### [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction)\n\nMCP’s strength is in it being an open standard, stewarded by Anthropic, but made for and with the help of the OSS community.\n\nMCP is like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect devices to various peripherals, MCP provides a standardized interface for connecting LLMs to external data sources and tools.\n\n## The Current Challenges with MCP\n\nMCP as a protocol is technically impressive, but the ecosystem around it faces three main challenges:\n\n1. **Discovery**: There's no centralized or standardized way to find MCP tools. Each provider is building their own discovery mechanism, creating fragmentation.\n2. **Quality**: With no centralized registry or verification process, tool quality varies dramatically. There's no equivalent of NPM’s package scoring or verification badges.\n3. **Configuration**: Each provider has its own configuration schema and APIs, making it difficult for frameworks to provide a consistent setup experience without creating provider-specific abstractions.\n\nAs Shopify CEO Tobi Lütke [recently pointed out](https://x.com/tobi/status/1891137636720419191), while MCP has enormous potential as \"the USB-C of LLM tools,\" it's still incomplete. To follow his analogy, MCP currently defines the cable and wire protocol but is missing the plug.\n\nThe community is actively addressing these challenges. Two important discussions are happening in the MCP GitHub repository:\n\n- [Official Registry Specification](https://github.com/orgs/modelcontextprotocol/discussions/159): A standard for how registries should work\n- [`.well-known/mcp.json` Directory Specification](https://github.com/orgs/modelcontextprotocol/discussions/84): A standard for decentralized MCP server discovery\n\n## Insights from Ecosystem Leaders\n\nTo go deeper, we met with the companies pushing the MCP world forward: Andy Qin from [OpenTools](https://opentools.com/), Tadas Antanavicius from [PulseMCP](https://www.pulsemcp.com/), Steve Manuel from [MCP.run](https://www.mcp.run/), and Henry Mao from [Smithery](https://smithery.ai/).\n\nEach company is solving different parts of the MCP puzzle rather than competing directly:\n\n- OpenTools is focusing on search and curation to complement the new official registry\n- PulseMCP provided insights on how registry consolidation will reduce redundant work for registries\n- MCP.run is building secure, performant MCP hosting through WebAssembly and will support the new MCP registry APIs\n- Smithery brings practical implementation experience from Henry's previous success with [Jenni.ai](http://jenni.ai/) and will also support the new MCP APIs\n\n## Why We're Betting on MCP\n\nAfter evaluating the landscape of AI tool integration standards, we believe MCP offers compelling advantages:\n\n1. **Open Standard**: MCP is designed as an industry-wide protocol rather than a proprietary solution, allowing for implementation without vendor lock-in.\n2. **Industry Adoption**: Notable companies have implemented MCP in production, including Zed, Replit, Codeium/Windsurf, Sourcegraph, Cursor, and Block/Square.\n3. **Ecosystem Compatibility**: MCP's architecture allows other standards to implement bridge servers (as demonstrated by Composio's recent integration), providing flexibility as the ecosystem evolves.\n4. **Active Development**: The community is addressing current limitations through the proposed [official registry](https://github.com/orgs/modelcontextprotocol/discussions/159) and [`.well-known/mcp.json`](https://github.com/orgs/modelcontextprotocol/discussions/84) specifications.\n\n## Market Perspectives on MCP\n\nThe broader ecosystem shows promising signs of momentum:\n\nNik Pash, Head of Operations at Cline, [outlined what he calls \"the MCP domino effect\"](https://x.com/pashmerepat/status/1894074058490384643), describing how companies who ignored MCP \"are about to have a rude awakening\" as more businesses release official servers and users grasp the power of seamless AI integrations.\n\nPash also [highlighted the entrepreneurial opportunity](https://x.com/pashmerepat/status/1896646892211294372), noting that building and monetizing MCP servers represents an \"insane opportunity right now\" with \"no competition, wide open space, and VCs begging to throw money at solo founders who get there first\".\n\nWhile there are problems today, they’re in the process of being solved, and MCP is looking like it’s about to have it’s moment.\n\n## Our Proposal: Framework-Friendly MCP\n\nAs part of our research we built a proof-of-concept for how agent frameworks could interact with MCP registries.\n\n### 1. Registry Client\n\nFrameworks need a standardized client implementation to interact with tool registries so they can introspect metadata about the registry, its available servers, and query for MCP server schemas.\n\n```tsx\nimport { RegistryClient } from \"@mcp/registry\";\n\nconst registry = new RegistryClient({\n  url: \"https://example-tools.com/.well-known/mcp.json\",\n});\n\nconst directory = await registry.connect();\nconsole.log(\"Connected to registry:\", directory.name, directory.homepage);\n\nconst allServers = await registry.listServers();\nconsole.log(\"\\nAvailable servers:\", allServers);\n```\n\n### 2. Server Definition for Configuration and Validation\n\nEach MCP server should expose its configuration schema with a standard format:\n\n```tsx\n// look up a server definition from a registry\nconst stripeServer = await registry.getServerDefinition({ id: \"stripe\" });\n// use the servers schema to build a configuration UI\nconst userInput = await example.buildServerUI(stripeServer.schemas);\n// validate that the user input is correct for the server definition\nconst validConfig = stripeServer.parseConfig(userInput);\n```\n\nThis pattern enables frameworks to build dynamic UIs and validate configurations before attempting to connect to MCP servers.\n\nThese primitives could be used to build registry UIs, hosted configuration UIs, CLIs, framework abstractions, `npm link` or `verdaccio`-like local dev experiences, and more.\n\n### Framework-Level Configuration\n\nWe plan to build on top of the `mcp.json` specification to expose an API to connect to any registry that implements the spec.\n\nAfter connecting to a registry, users will have IDE autocompletions for the available servers and configurations via some [gql.tada-like magic](https://gql-tada.0no.co).\n\n```tsx\nimport { MCPConfiguration } from \"@mastra/mcp\";\n\nconst configuration = new MCPConfiguration({\n  registry: \"https://mcp.run/.well-known/mcp.json\",\n  servers: {\n    googleCalendar: {\n      // <- IDE autocomplete via TypeScript for the Google calendar MCP server\n    },\n    // <- autocomplete all available servers for the connected registry\n  },\n});\n```\n\nUsing the MCP configuration would then involve passing it into a Mastra agent\n\n```tsx\n// get connected MCP clients for the configuration\nconst toolsets = await configuration.getConnectedTools();\n\nconst response = await agent.stream(prompt, {\n  toolsets, // <- pass to your agents to make those tools available\n});\n```\n\nOur code for this proof-of-concept is available [in the Mastra monorepo on Github](https://github.com/mastra-ai/mastra/blob/main/explorations/mcp-registry-client/README.md).\n\n## Using MCP with Mastra Today\n\nWhile our proof-of-concept registry client is still in development, you can already connect Mastra to any MCP server through our `MastraMCPClient` API. This provides access to the MCP ecosystem without waiting for the registry infrastructure to mature.\n\n### Connecting to Stdio MCP Servers\n\nHere's how you can connect to an MCP server that uses [stdio transport](https://modelcontextprotocol.io/docs/concepts/transports#standard-input%2Foutput-stdio):\n\n```tsx\nimport { Agent } from \"@mastra/core/agent\";\nimport { MastraMCPClient } from \"@mastra/mcp\";\nimport { anthropic } from \"@ai-sdk/anthropic\";\n\n// Sequential Thinking server as an example:\n// https://smithery.ai/server/@smithery-ai/server-sequential-thinking\n// Initialize the MCP client\nconst sequentialThinkingClient = new MastraMCPClient({\n  name: \"sequential-thinking\",\n  server: {\n    command: \"npx\",\n    args: [\"-y\", \"@modelcontextprotocol/server-sequential-thinking\"],\n  },\n});\n\n// Create a Mastra Agent\nconst agent = new Agent({\n  name: \"Reasoning agent\",\n  instructions:\n    \"You solve problems by breaking them down into sequential steps. Use the sequential thinking tool to walk through your reasoning process step by step.\",\n  model: anthropic(\"claude-3-5-sonnet-latest\"),\n});\n\ntry {\n  // Connect to the MCP server\n  await sequentialThinkingClient.connect();\n\n  // Get available tools\n  const sequentialThinkingTools = await sequentialThinkingClient.tools();\n\n  // Use the agent with the Sequential Thinking tool\n  const response = await agent.stream(\n    \"Design a system for optimizing delivery routes for a small local delivery business with 5 drivers and approximately 100 deliveries per day.\",\n    {\n      toolsets: {\n        sequentialThinking: sequentialThinkingTools,\n      },\n    },\n  );\n\n  for await (const part of response.fullStream) {\n    switch (part.type) {\n      case \"error\":\n        console.error(part.error);\n        break;\n      case \"text-delta\":\n        process.stdout.write(part.textDelta);\n        break;\n      case \"tool-call\":\n        console.info(`\\n-> Tool call: ${part.toolName}\\n`);\n    }\n  }\n} finally {\n  // Always disconnect when done\n  await sequentialThinkingClient.disconnect();\n}\n```\n\n### Connecting to SSE MCP Servers\n\nFor MCP servers that use [Server-Sent Events (SSE)](https://modelcontextprotocol.io/docs/concepts/transports#server-sent-events-sse), you can connect by passing the URL of the server:\n\n```tsx\n// Initialize the MCP client using an SSE server\nconst sseClient = new MastraMCPClient({\n  name: \"sse-client\",\n  server: {\n    url: new URL(\"https://your-mcp-server.com/sse\"),\n    // Optional fetch request configuration\n    requestInit: {\n      headers: {\n        Authorization: \"Bearer your-token\",\n      },\n    },\n  },\n});\n\n// The rest of the usage is identical to the Stdio example\n```\n\n## The Future of MCP and Mastra\n\nWe're committed to making MCP integration seamless in Mastra. Our roadmap includes:\n\n1. Building a standardized installation and configuration flow for MCP servers using `.well-known/mcp.json`\n2. Adding support for new MCP features as the spec evolves\n3. Creating primitives that make it easy to discover, install, and configure MCP servers\n\nThe current state of tool integration for AI agents is reminiscent of the early days of package management. Just as `npm install` transformed JavaScript package management, MCP can transform how AI agents interact with the world. By standardizing on MCP, we can create an ecosystem where tools are easy to discover, configure, and use securely."
  },
  {
    "metadata": {
      "title": "Building a Code Assistant with Large Context Windows",
      "publishedAt": "2025-03-05",
      "summary": "How we used Mastra and Gemini Flash 2.0 to build a natural language interface for code repositories",
      "author": "Sam Bhagwat",
      "draft": false,
      "categories": ["examples"]
    },
    "slug": "repo-base",
    "content": "## The limits of traditional RAG\n\nWe recently built a demo using Mastra called [Repo Base](https://repo-base.vercel.app/), a natural language interface for understanding code repositories.\n\n![Repo Base demo](/images/repo-base.gif)\n\nWhen building it, we initially took the standard RAG approach - chunking code files, generating embeddings, and storing vectors. This quickly revealed several technical limitations:\n\n1. Processing large codebases consumed significant compute resources\n2. Chunking destroyed important semantic relationships between code sections\n3. Vector storage and retrieval added unnecessary architectural complexity\n4. Traditional RAG approaches weren't optimized for code's hierarchical structure\n\n## Leveraging Large Context Windows\n\nThe breakthrough came with Gemini Flash 2.0's expanded context window. Instead of complex chunking and embedding pipelines, we could feed entire files directly to the model. Here's how we set it up:\n\n```typescript\nimport { google } from \"@ai-sdk/google\";\nimport { Agent } from \"@mastra/core/agent\";\n\nimport { memory } from \"../memory\";\nimport { instructions } from \"./instructions\";\nimport { getFilePaths } from \"../tools/getFilePaths\";\nimport { getFileContent } from \"../tools/getFileContent\";\nimport { getRepositoryIssues } from \"../tools/getRepositoryIssues\";\nimport { getRepositoryCommits } from \"../tools/getRepositoryCommits\";\nimport { getRepositoryPullRequests } from \"../tools/getRepositoryPullRequests\";\n\nexport const agent = new Agent({\n  name: \"agent\",\n  instructions,\n  memory,\n  model: google(\"gemini-2.0-flash-001\"),\n  tools: {\n    getFilePaths,\n    getFileContent,\n    getRepositoryIssues,\n    getRepositoryCommits,\n    getRepositoryPullRequests,\n  },\n});\n```\n\n## Simplifying with Mastra's Memory System\n\nRather than building custom database schemas for message history and context management, we leveraged Mastra's memory system:\n\n```typescript\nimport { Memory } from \"@mastra/memory\";\nimport { PostgresStore } from \"@mastra/pg\";\n\nexport const memory = new Memory({\n  storage: new PostgresStore({ connectionString: process.env.DB_URL! }),\n  options: { lastMessages: 10 },\n});\n```\n\nThis handled:\n\n- Conversation persistence\n- Context window management\n- Semantic search across previous interactions\n- File tree relationship tracking\n\n## Out-the-box UI\n\nRather than building a custom chat interface from scratch, we leveraged [assistant-ui](https://www.assistant-ui.com/) which provides pre-built components for chat interactions and thread management. This saved significant development time compared to implementing our own UI components for message history, typing indicators, and thread state management.\n\n## Implementation Details\n\nThe system works by:\n\n1. Maintaining the complete repository structure in memory\n2. Using file-tree context to understand import/export relationships\n3. Leveraging large context windows to analyze entire files\n4. Applying semantic search for finding relevant code examples\n\n## Key Technical Learnings\n\n1. **Context > Chunking**: For code understanding, preserving complete file context often beats sophisticated chunking strategies.\n\n2. **Memory Management**: Mastra's built-in memory system eliminated the need for custom persistence layers while providing better semantic search capabilities.\n\n3. **Model Selection**: Large context window models (like Gemini Flash) significantly simplified the architecture by reducing the need for complex RAG pipelines.\n\n4. **File Tree Context**: Maintaining repository structure awareness was crucial for accurate code understanding.\n\nThe end result is a more maintainable system that better preserves the semantic relationships in code. You can find the code for Repo Base [here](https://github.com/mastra-ai/repo-base).\n\nWe're excited to see how others use these patterns to build similar tools with Mastra."
  },
  {
    "metadata": {
      "title": "Mastra Changelog 2025-03-04",
      "publishedAt": "2025-03-04",
      "summary": "Introducing Mastra Cloud and Mastra Voice (as well as a few other things)",
      "author": "Shane Thomas",
      "draft": false,
      "categories": ["changelogs"]
    },
    "slug": "changelog-2025-03-04",
    "content": "This week we launched [Mastra Cloud](https://mastra.ai/cloud-beta) Beta: a serverless agent environment with atomic deployments. Mastra Cloud lets you access your agents and workflows from anywhere and monitor their performance.\n\nWe also added comprehensive voice capabilities to Agents, introduced tool-call based working memory, added CommonJS support, and made vector store improvements.\n\n## Mastra Cloud\n\n<div\n  style={{\n    position: \"relative\",\n    overflow: \"hidden\",\n    width: \"100%\",\n    paddingTop: \"56.25%\",\n  }}\n>\n  <iframe\n    style={{\n      position: \"absolute\",\n      top: 0,\n      left: 0,\n      bottom: 0,\n      right: 0,\n      width: \"100%\",\n      height: \"100%\",\n    }}\n    src=\"https://www.youtube.com/embed/fbmCQAd6at8?si=Q7XGFV4-H488LzV3&rel=0\"\n    title=\"Mastra Cloud Demo\"\n    frameBorder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n    allowFullScreen\n  />\n</div>\n\nWe are excited to introduce Mastra Cloud&mdash;a platform for instantly deploying your Mastra application and monitoring its performance.\n\nYou’ll also be able to chat directly with your agents, see which models and tools they reference, and view detailed traces. We’ve found this to be particularly helpful in debugging allowing you to see every step of every interaction.\n\nMastra Cloud is currently in beta. You can [request access here](https://mastra.ai/cloud-beta).\n\n## Mastra Voice\n\nWith Mastra Voice, you can make your agents **`speak`** and **`listen`**. Whether you want to use a single provider for both speech-to-text and text-to-speech, or combine multiple providers for different operations, the implementation remains simple.\n\nFor reference, this is how to build a helpful voice assistant that asks you how you’re doing:\n\n```tsx\nimport { Agent } from \"@mastra/core/agent\";\nimport { OpenAIVoice } from \"@mastra/voice-openai\";\n\nconst agent = new Agent({\n  name: \"Agent\",\n  instructions: `You are a helpful assistant with voice capabilities.`,\n  model: openai(\"gpt-4o\"),\n  voice: new OpenAIVoice(),\n});\n\nconst audioStream = await agent.speak(\"How are you today?\");\n```\n\nA list of supported providers and implementation details can be found [here](https://mastra.ai/blog/giving-agents-a-voice).\n\n## Memory enhancements\n\nPreviously, Mastra only supported one way of handling working memory updates — through text stream tags where the LLM would include XML-like tags in its response.\n\nThis worked well for text streaming but had limitations when using `toDataStream` (since parsing and masking XML tags from a data stream is complex.)\n\nThe new update adds an alternative approach using tool calls instead of text tags:\n\n```tsx\nconst memory = new Memory({\n  options: {\n    workingMemory: {\n      enabled: true,\n      use: \"tool-call\", // Use the new tool-based approach\n    },\n  },\n});\n```\n\n## CommonJS support\n\nThe codebase was previously ESM-only, which caused compatibility issues for users working with CommonJS projects. We listened to user feedback and added dual module support across all Mastra packages by:\n\n- Adding CommonJS builds alongside existing ESM output\n- Updating build configurations to generate both .cjs and .js formats with proper type definitions\n- Ensuring all packages maintain full TypeScript support for both module systems\n\n## Vector store improvements\n\nThe vector store APIs were modernized across all database implementations (PostgreSQL, Chroma, Astra) to use a more intuitive object-based parameter approach:\n\n```tsx\nawait vectorDB.query({\n  indexName: \"my_index\",\n  queryVector: [0.1, 0.2],\n  topK: 10,\n});\n```\n\nThis change improves type safety and developer experience while maintaining backward compatibility. The update also added support for multiple index types in PostgreSQL (HNSW, IVFFlat) and enhanced document storage capabilities in Chroma, allowing full-text documents to be stored alongside vectors."
  },
  {
    "metadata": {
      "title": "From Whiteboard to Excalidraw: Building a Multi-Agent Workflow",
      "publishedAt": "2025-02-28",
      "author": "Shane Thomas",
      "summary": "How we built a whiteboard image to Excalidraw converter using Mastra's multi-agent workflows, and what we learned about breaking down complex AI tasks into deterministic steps with validation loops.",
      "draft": false,
      "categories": ["examples"]
    },
    "slug": "whiteboard-to-excalidraw-converter",
    "content": "![whiteboard2excalidraw](/images/excalidraw/Image2Excalidraw.gif)\n\nDuring YC, we've had many fellow batch companies and a few other AI startups visit our apartment for whiteboarding sessions. These collab sessions often produce valuable diagrams and ideas that deserve to live beyond the temporary medium of a physical whiteboard.\n\n![Collage of Mastra whiteboarding sessions](/images/excalidraw/WhiteboardingCollageCropped.png)\n\n\nWe wanted to make these whiteboard sketches more accessible and reusable, so we built a tool that converts whiteboard images into editable Excalidraw diagrams. This post explores how we approached this challenge using Mastra's multi-agent workflows and what we learned along the way.\n\nHere's the [deployed version](https://image2excalidraw.netlify.app/), the [Mastra code](https://github.com/mastra-ai/excalidraw-mastra-app) and the [frontend app code](https://github.com/smthomas/whiteboard-excalidraw-converter).\n\n## The One-Shot Approach: Why It Failed\n\nOur first instinct was to solve this with a single agent and a comprehensive prompt. After all, models can \"see\" and understand images, so why not just ask them to convert directly to Excalidraw JSON?\n\n```typescript\nconst oneShot = new Agent({\n  name: \"Whiteboard Converter\",\n  instructions: `Convert this whiteboard image into Excalidraw JSON...`,\n  model: anthropic(\"claude-3-7-sonnet-20250219\"),\n});\n\n// This approach quickly hit limitations\n```\n\nThis approach worked for very simple whiteboard images but quickly hit limitations:\n\n1. **Output token limits**: Even with large context windows, we still faced output token constraints when generating complex JSON structures\n2. **Accuracy issues**: The agent would miss elements or relationships in more complex diagrams\n3. **Validation challenges**: Without intermediate steps, it was difficult to verify and correct the output\n\nWe needed a more structured approach.\n\n## Breaking Down the Problem: A Multi-Step Workflow\n\n![Mastra multi-step Workflow](/images/excalidraw/ExcalidrawConverterWorkflow.png)\n\nInstead of trying to solve everything at once, we decided to break the problem into discrete steps using Mastra's workflow functionality:\n\n```typescript\nexport const excalidrawConverterWorkflow = new Workflow({\n  name: \"excalidraw-converter\",\n  triggerSchema: z.object({\n    filename: z.string(),\n    file: z.string(),\n  }),\n});\n\nexcalidrawConverterWorkflow\n  .step(imageToCsvStep)\n  .then(validateCsvStep)\n  .then(csvToExcalidrawStep)\n  .then(validateExcalidrawStep)\n  .commit();\n```\n\nThis workflow follows a clear progression:\n\n1. **Image to CSV**: Convert the whiteboard image to a dense CSV representation\n2. **Validate CSV**: Check and improve the CSV output\n3. **CSV to Excalidraw**: Transform the validated CSV into Excalidraw JSON\n4. **Validate Excalidraw**: Ensure the JSON is valid and fix any issues\n\nLet's look at each step in more detail.\n\n## Step 1: Image to CSV Conversion\n\nThe first step uses a specialized agent to analyze the image and extract all visual elements into a structured CSV format:\n\n```typescript\nconst imageToCsvStep = new Step({\n  id: \"imageToCsv\",\n  outputSchema: z.object({\n    filename: z.string(),\n    csv: z.string(),\n  }),\n  execute: async ({ context }) => {\n    const triggerData = context?.getStepResult<{\n      filename: string;\n      file: string;\n    }>(\"trigger\");\n\n    if (!triggerData?.filename || !triggerData?.file) {\n      throw new Error(\"Missing required image data in context\");\n    }\n\n    const imageToCsv = mastra.getAgent(\"imageToCsvAgent\");\n    const response = await imageToCsv.generate(\n      [\n        {\n          role: \"user\",\n          content: [\n            {\n              type: \"image\",\n              image: triggerData.file,\n            },\n            {\n              type: \"text\",\n              text: `View this image of a whiteboard diagram and convert it into CSV format. Include all text, lines, arrows, and shapes. Think through all the elements of the image.`,\n            },\n          ],\n        },\n      ],\n      { maxSteps: 10 },\n    );\n\n    return {\n      filename: `${triggerData.filename.split(\".\")[0]}.excalidraw`,\n      csv: response.text,\n    };\n  },\n});\n```\n\nWe chose CSV as an intermediate format because:\n\n1. It's extremely dense, allowing us to represent many elements within token limits\n2. It's structured enough to capture all the necessary properties of visual elements\n3. It's easy to parse and transform in subsequent steps\n\n## Step 2: CSV Validation\n\nThe validation step was a critical addition that significantly improved our results:\n\n```typescript\nconst validateCsvStep = new Step({\n  id: \"validateCsv\",\n  // ... schema definitions ...\n  execute: async ({ context }) => {\n    // ... get data from previous step ...\n\n    const imageToCsv = mastra.getAgent(\"imageToCsvAgent\");\n    const response = await imageToCsv.generate(\n      [\n        // ... show the original image again ...\n        {\n          role: \"assistant\",\n          content: [\n            {\n              type: \"text\",\n              text: csvData.csv,\n            },\n          ],\n        },\n        {\n          role: \"user\",\n          content: [\n            {\n              type: \"text\",\n              text: `Validate your last response containing the CSV code to add missing elements (text, lines, etc.) to the CSV. You should add new items to the original CSV results. The previous step missed some elements. Find them and add them. Return the CSV text.`,\n            },\n          ],\n        },\n      ],\n      {\n        maxSteps: 10,\n      },\n    );\n\n    return {\n      filename: csvData.filename,\n      csv: response.text,\n    };\n  },\n});\n```\n\nThis validation step is essentially asking the same agent to review its own work by:\n\n1. Showing it the original image again\n2. Presenting its previous CSV output\n3. Explicitly asking it to find and add missing elements\n\nThis self-review process significantly improved the completeness of our element extraction.\n\n## Step 3: CSV to Excalidraw Conversion\n\nThe third step transforms the validated CSV into Excalidraw JSON:\n\n```typescript\nconst csvToExcalidrawStep = new Step({\n  id: \"csvToExcalidraw\",\n  // ... schema definitions ...\n  execute: async ({ context }) => {\n    const csvData = context?.getStepResult<{\n      filename: string;\n      csv: string;\n    }>(\"validateCsv\");\n\n    // Parse CSV into rows\n    const rows = csvData.csv\n      .split(\"\\n\")\n      .map((line) => line.trim())\n      .filter((line) => line.length > 0);\n\n    // ... detailed CSV parsing logic ...\n\n    // Create Excalidraw JSON\n    const excalidrawJson = {\n      type: \"excalidraw\",\n      version: 2,\n      source: \"https://excalidraw.com\",\n      elements,\n      appState: {\n        gridSize: 20,\n        gridStep: 5,\n        gridModeEnabled: false,\n        viewBackgroundColor: \"#ffffff\",\n      },\n      files: {},\n    };\n\n    return {\n      filename: csvData.filename,\n      excalidrawJson,\n    };\n  },\n});\n```\n\nThis step is primarily deterministic, parsing the CSV and mapping it to the Excalidraw JSON structure. We handle special cases for different element types and ensure all required properties are properly formatted.\n\n## Step 4: Excalidraw Validation Loop\n\nThe final validation step was perhaps the most crucial in our workflow:\n\n```typescript\nconst validateExcalidrawStep = new Step({\n  id: \"validateExcalidraw\",\n  // ... schema definitions ...\n  execute: async ({ context }) => {\n    // ... get data from previous step ...\n\n    // Validate the JSON\n    const validator = mastra.getAgent(\"excalidrawValidatorAgent\");\n    const messages: CoreMessage[] = [\n      {\n        role: \"user\",\n        content: [\n          {\n            type: \"text\",\n            text: `Validate the following Excalidraw JSON. If it is not valid, fix it and just return the valid JSON.`,\n          },\n          {\n            type: \"text\",\n            text: JSON.stringify(excalidrawData.excalidrawJson),\n          },\n        ],\n      },\n    ];\n\n    let attempts = 0;\n    const maxAttempts = 3;\n    let lastError: Error | null = null;\n\n    while (attempts < maxAttempts) {\n      attempts++;\n\n      const validationResponse = await validator.generate(messages, {\n        maxSteps: 10,\n      });\n\n      // Try to parse the response\n      try {\n        // ... clean and parse the JSON ...\n        return {\n          filename: excalidrawData.filename,\n          contents: parsedJson,\n        };\n      } catch (e) {\n        // If parsing fails, add the error to messages and try again\n        messages.push({\n          role: \"assistant\",\n          content: [{ type: \"text\", text: validationResponse.text }],\n        });\n\n        messages.push({\n          role: \"user\",\n          content: [\n            {\n              type: \"text\",\n              text: `The previous Excalidraw JSON did not validate. Please fix it and return the valid JSON without any string quotes or new lines. Here is the error: ${e}`,\n            },\n          ],\n        });\n      }\n    }\n\n    // If we've exhausted all attempts, throw an error\n    throw new Error(\n      `Failed to validate Excalidraw JSON after ${maxAttempts} attempts. Last error: ${lastError?.message}`,\n    );\n  },\n});\n```\n\nThis step implements a validation loop that:\n\n1. Attempts to parse the Excalidraw JSON\n2. If parsing fails, it feeds the error back to the agent\n3. The agent tries to fix the JSON based on the error\n4. This cycle repeats up to 3 times or until valid JSON is produced\n\nThis feedback loop dramatically improved the success rate of our converter, especially for complex diagrams.\n\n## The Specialized Agents\n\nThe workflow relies on two specialized agents with carefully crafted instructions:\n\n### Image to CSV Agent\n\n```typescript\nexport const imageToCsvAgent = new Agent({\n  name: \"Image to CSV Converter\",\n  instructions: `You are an expert at analyzing images and converting them into structured CSV data. Your task is to identify visual elements and their relationships in images and represent them in a CSV format that can be used to recreate the diagram.\n\nWhen you receive an image, carefully analyze its contents and create a CSV representation that captures:\n\n1. Elements:\n   - Type of each element (rectangle, arrow, text, line, ellipse, diamond, freedraw, etc.)\n   - Position (x, y coordinates)\n   - Size (width, height)\n   - Style properties (colors, stroke width, fill style)\n   - Text content (if text element)\n   - Unique identifier for each element\n   - Angle and rotation\n   - Points for lines and arrows\n   - Binding information for connectors\n   - Group IDs for grouped elements\n\n2. Relationships:\n   - Connections between elements (arrows, lines)\n   - Parent-child relationships\n   - Element groupings\n   - Binding points and arrowheads\n\n3. Layout and Style:\n   - Spatial arrangement\n   - Alignment\n   - Spacing\n   - Roughness and opacity\n   - Frame information\n   - Element-specific properties (roundness, etc.)\n\nYour output must be a CSV string with the following columns:\nid,type,x,y,width,height,text,strokeColor,backgroundColor,fillStyle,strokeWidth,strokeStyle,roughness,opacity,angle,points,startBinding,endBinding,arrowheads,fontSize,fontFamily,groupIds,frameId,roundness,seed,version,isDeleted,boundElements\n\nExample CSV format:\nid,type,x,y,width,height,text,strokeColor,backgroundColor,fillStyle,strokeWidth,strokeStyle,roughness,opacity,angle,points,startBinding,endBinding,arrowheads,fontSize,fontFamily,groupIds,frameId,roundness,seed,version,isDeleted,boundElements\nrect1,rectangle,83,10,147,122,,#e03131,transparent,solid,2,solid,1,100,0,,,,,,,,,,null,75180,1,false,\"[{\"\"type\"\":\"\"text\"\",\"\"id\"\":\"\"text1\"\"},{\"\"id\"\":\"\"arrow1\"\",\"\"type\"\":\"\"arrow\"\"}]\"\ntext1,text,108,45,96,50,\"Rectangle\\nExample\",#e03131,transparent,solid,2,solid,1,100,0,,,,,20,5,[],,,null,14450,1,false,\n\n// ... There are hundreds more lines of detailed instructions covering element relationships, \n// specific element types, formatting rules, binding mechanics, and error handling scenarios ...\n  `,\n  model: anthropic(\"claude-3-7-sonnet-20250219\"),\n});\n```\n\nThe full instructions for this agent are over 200 lines long, providing extremely detailed guidance on how to identify and represent every possible element type and relationship in a whiteboard diagram. This level of detail proved essential for accurate conversion.\n\n### Excalidraw Validator Agent\n\n```typescript\nexport const excalidrawValidatorAgent = new Agent({\n  name: \"Excalidraw Validator\",\n  instructions: `You are an expert at validating and fixing Excalidraw JSON for Excalidraw diagrams.\n\nYour response MUST be valid JSON in the excalidraw JSON format.\n\nThe format must follow this exact schema:\n\n{\n  \"type\": \"excalidraw\",\n  \"version\": 2,\n  \"source\": \"https://excalidraw.com\",\n  \"elements\": [\n    // Elements can be one of several types: rectangle, arrow, text, etc.\n    // Each element must include these common properties:\n    {\n      \"type\": string,              // \"rectangle\", \"arrow\", \"text\", \"line\", etc.\n      \"version\": number,           // Version number of the element      \n      \"id\": string,               // Unique element identifier\n      \"fillStyle\": string,        // \"hachure\", \"solid\", etc.\n      \"strokeWidth\": number,      // Width of the stroke\n      \"strokeStyle\": string,      // \"solid\", \"dashed\", etc.\n      \"roughness\": number,        // 0-2 indicating how rough the drawing should be\n      \"opacity\": number,          // 0-100\n      \"angle\": number,            // Rotation angle in degrees\n      \"x\": number,                // X coordinate\n      \"y\": number,                // Y coordinate\n      \"strokeColor\": string,      // Color in hex format\n      \"backgroundColor\": string,  // Background color in hex format\n      // ... Shortened for readability ...\n    }\n  ]\n  // ... additional JSON removed for readability\n}\n\nYou can update the JSON to be valid and ensure it matches the expected excalidraw schema.`,\n  model: anthropic(\"claude-3-7-sonnet-20250219\"),\n});\n```\n\nThis validator agent is crucial for the final step in our workflow, where it ensures the generated Excalidraw JSON is valid and properly formatted. It's specifically designed to understand the Excalidraw schema and fix any issues that might prevent the JSON from being properly rendered.\n\n## Key Lessons Learned\n\nBuilding this converter taught us several valuable lessons about developing complex AI applications:\n\n### 1. Break Complex Tasks into Deterministic Steps\n\nOur initial one-shot approach failed because it tried to do too much at once. Breaking the process into discrete steps with clear inputs and outputs made the problem tractable and improved results.\n\n### 2. Validation Loops Are Essential\n\nThe validation steps were not an afterthought—they were critical to the success of the converter. Having agents review and improve their own work significantly enhanced accuracy.\n\n### 3. Dense Intermediate Formats Help with Token Limits\n\nUsing CSV as an intermediate format allowed us to represent complex visual scenes efficiently within token constraints. This approach can be applied to many other multi-step AI processes.\n\n### 4. Explicit Instructions Beat Implicit Understanding\n\nEven with advanced models like Claude 3.7, extremely detailed instructions produced better results than relying on the model's implicit understanding. Our agent prompts were comprehensive, specifying exactly what to look for and how to format the output.\n\n### 5. Consider a Full Feedback Loop\n\nIf we were to improve this further, we would implement a complete feedback loop that compares the final Excalidraw rendering with the original image and makes adjustments. This could potentially use a reasoning model like o3, though at the time of development, it didn't support image inputs.\n\n## Conclusion\n\nBuilding AI applications that work reliably often requires more than just a single prompt or agent. By combining deterministic workflows with specialized agents and validation loops, we can create systems that handle complex tasks with higher reliability.\n\nThis whiteboard converter is just one example of how Mastra's multi-agent workflows can be applied to real-world problems. We hope it inspires you to think about how you might break down your own complex AI challenges into manageable, validated steps."
  },
  {
    "metadata": {
      "title": "Giving Agents a Voice",
      "publishedAt": "2025-02-27",
      "author": "Yujohn Nattrass",
      "summary": "Mastra Voice lets developers add speech-to-text and text-to-speech capabilities to AI agents using a straightforward API",
      "draft": false,
      "categories": ["announcements"]
    },
    "slug": "giving-agents-a-voice",
    "content": "Today we're releasing Mastra Voice: features that let you have real-time voice conversations with AI agents.\n\nWith Mastra Voice, you can make your agents `speak` and `listen` through a clean, flexible API. Whether you want to use a single provider for both speech-to-text and text-to-speech, or combine multiple providers for different operations, the implementation remains simple.\n\nHere&apos;s an example of building a helpful voice assistant that asks you how you're doing:\n\n```jsx\nimport { Agent } from \"@mastra/core/agent\";\nimport { OpenAIVoice } from \"@mastra/voice-openai\";\n\nconst agent = new Agent({\n  name: \"Agent\",\n  instructions: `You are a helpful assistant with voice capabilities.`,\n  model: openai(\"gpt-4o\"),\n  voice: new OpenAIVoice(),\n});\n\nconst audioStream = await agent.speak(\"How are you today!\");\n```\n\nMastra Voice works with Node.js streams, making it simple to save speech output to files and even transcribe audio input from various sources.\n\n## **Why voice matters**\n\nHumans don’t just communicate in writing. Therefore, we believe that agents shouldn’t either.\n\nIn fact, voice interfaces significantly expand what you can build with AI agents. While text-based interactions work well for many cases, they can also create unnecessary friction. Imagine having to always transcribe your thoughts to text and read a response. What if you need hands-free communication, to meet accessibility requirements, or just prefer the natural back-and-forth of human speech?\n\nSometimes voice communication is simply more efficient.\n\nHere’s an example of a helpful voice assistant waiting for the user to speak first:\n\n```jsx\nimport { Agent } from \"@mastra/core/agent\";\nimport { OpenAIVoice } from \"@mastra/voice-openai\";\n\nconst agent = new Agent({\n  name: 'Agent',\n  instructions: `You are a helpful assistant with voice capabilities.`,\n  model: openai('gpt-4o'),\n  voice: new OpenAIVoice();\n});\n\nconst audioStream = fs.createReadStream('/path/to.mp3')\n\nconst text = await agent.listen(audioStream)\n\n// Hey! How are ya!\n```\n\n## Supported Providers\n\nThe table shows which voice providers we currently support and what they can do. Each provider offers different combinations of speech-to-text and text-to-speech functionality.\n\n![Voice providers](/voice-providers.png)\n\n## Mix and Match Voice Providers\n\nThe `CompositeVoice` class is particularly useful when you want to leverage different providers' strengths for different operations. For example, you might prefer OpenAI's speech recognition accuracy but PlayAI's voice quality or cost structure for generating responses.\n\n```tsx\n// OpenAI Voice provider\nimport { OpenAIVoice } from \"@mastra/voice-openai\";\nconst openaiVoice = new OpenAIVoice({\n  listeningModel: {\n    name: \"whisper-1\",\n    apiKey: \"your-openai-api-key\",\n  },\n});\n\n// PlayAI Voice provider\nimport { PlayAIVoice } from \"@mastra/voice-playai\";\nconst playAIVoice = new PlayAIVoice({\n  speechModel: {\n    name: \"PlayDialog\",\n    apiKey: process.env.PLAYAI_API_KEY,\n    userId: process.env.PLAYAI_USER_ID,\n  },\n  speaker: \"Angelo\", // Default voice\n});\n\n// Use CompositeVoice to mix and match providers\nimport { CompositeVoice } from \"@mastra/core/voice\";\nconst customVoice = new CompositeVoice({\n  listenProvider: openaiVoice, // Use OpenAI for speech recognition\n  speakProvider: playAIVoice, // Use PlayAI for speech synthesis\n});\n```\n\nEach provider implementation in Mastra follows the same interface, making it straightforward to swap between them or add new providers as they become available. This gives you flexibility while maintaining a consistent developer experience across your application.\n\n## An example\n\n<Video src=\"/voice-demo.mp4\" />\n\nIn this demo, you can see a voice-enabled agent in action. It shows how people can have natural conversations with AI without typing, creating a more fluid experience.\n\nWe're actively developing Mastra! If you encounter any issues or have suggestions for improvements, please open an issue on our GitHub repository or contribute directly with a pull request.\n\nGet started with Mastra Voice today by installing the latest version of our packages:\n\n```bash\nnpm install @mastra/core @mastra/voice-openai\n```\n\nThe full documentation is available at [mastra.ai/docs/reference/voice/mastra-voice](https://mastra.ai/docs/reference/voice/mastra-voice) with additional examples and configuration options."
  },
  {
    "metadata": {
      "title": "Benchmarking pgvector RAG performance across different dataset sizes",
      "publishedAt": "2025-02-26",
      "author": "Nik Aiyer",
      "summary": "How we used LLMs to do API design at Mastra",
      "draft": false,
      "categories": ["foundations"]
    },
    "slug": "pgvector-perf",
    "content": "It started with a simple customer question:\n\n\"Why did you choose IVFFlat indexing for your PG vector library?\"\n\nSeems straightforward enough... except we realized we didn't have a data-backed answer.\n\nWe had implemented IVFFlat with fixed parameters (100 lists) as our default, but could we actually defend this choice with hard numbers?\nThis sent us down a rabbit hole of benchmarking and testing that I want to share, because the results are interesting.\n\nWhen customers ask technical \"why\" questions, they're usually really asking: \"Is this the optimal solution for my use case?\" Our customer had growing datasets and wanted confidence that our indexing strategy would scale with them.\nOur implementation looked like this:\n\n- IVFFlat indexes created immediately upon table creation\n- Fixed parameters (100 lists) regardless of dataset size\n- No index rebuilding as data changes or grows\n\n## Let's explore this\n\n- Our current fixed IVFFlat implementation\n- An adaptive IVFFlat approach that scales lists with dataset size\n- Flat (no index) as a baseline\n- HNSW with a maximum of 8 connections and a build time complexity of 32\n\nWe tested across:\n\n- Different dataset sizes (10K, 100K, 500K, and 1M vectors)\n- Various K values (10, 25, 50, 100 nearest neighbors)\n- Different dimensions (64, 384, 1024)\n- Different vector distributions (random, clustered, skewed, and mixed)\n\nFor each configuration, we ran 30 queries to get reliable data on both recall and latency.\n\n### Recall Performance: Better Than Expected\n\nOne of our initial concerns was that recall might degrade significantly with our fixed approach as datasets grew.\nThe data showed otherwise:\n\nCurrent:\n![pgvectorrecall](/images/pgvector-research/avgrecalloriginal.webp)\n\nAdaptive:\n![pgvectorrecall](/images/pgvector-research/avgrecallafter.webp)\n\nFlat:\n![pgvectorrecall](/images/pgvector-research/avgrecallflat.webp)\n\nHNSW:\n![pgvectorrecall](/images/pgvector-research/avgrecallhnsw.webp)\n\n_Note: Combined results shown for all dimensions (64, 384, 1024)_\n\nBoth fixed and adaptive approaches maintained excellent recall (typically 100%) for datasets larger than 1,000 vectors\n\nEven with our fixed lists, recall stayed strong as data grew.\n\n### Latency Performance: Room for Improvement\n\nHere's where things got interesting:\n\nCurrent:\n![pgvectorlatency](/images/pgvector-research/p95latencyoriginal.webp)\n\nAdaptive:\n![pgvectorlatency](/images/pgvector-research/p95latencyafter.webp)\n\nFlat:\n![pgvectorlatency](/images/pgvector-research/p95latencyflat.webp)\n\nHNSW:\n![pgvectorlatency](/images/pgvector-research/p95latencyhnsw.webp)\n\n_Note: Results shown for dimension 64_\n\n- The fixed approach showed much more variable latency, especially for P95 measurements\n- For large datasets, the adaptive approach delivered significant improvements:\n- With 1M vectors (64 dimensions), P95 latencies were 125-161ms for adaptive vs 141-219ms for fixed\n- With 500K vectors, median latencies were 60-65ms for adaptive vs 66-70ms for fixed\n\nThese might seem like small differences, but in production, they add up to a much better user experience.\n\nThe fixed approach created some major cluster imbalances:\n\n- With 100K vectors: 1000 vectors/list in fixed vs 158 vectors/list in adaptive\n- With 1M vectors: Up to 10,000 vectors in some clusters while others remained sparse\n\nThis uneven distribution explained the latency variability we were seeing.\n\nAfter seeing all these results, we decided to make some improvements.\n\n## Better index management now\n\n- Separate table creation from index building\n- User-Controlled Rebuilding: Index can be reconstructed whenever data changes occur\n- Intelligent List Sizing: Lists are dynamically calculated based on your dataset size\n\n```ts\nimport { PgVector } from \"@mastra/pg\";\n\nconst vector = new PgVector();\n\n// Rebuild the index on index creation\nawait vector.createIndex({\n  indexName: \"embeddings\",\n  dimension: 1536,\n  metric: \"cosine\",\n  indexConfig: {},\n  buildIndex: true,\n});\n\n// Rebuild via buildIndex\nawait vector.buildIndex({\n  indexName: \"embeddings\",\n  metric: \"cosine\",\n  indexConfig: {},\n});\n```\n\nThe nice thing about Mastra, is that you don't have to worry about most of this. We handle it for you.\n\n## When To Rebuild Your Index?\n\nThe one thing you still need to think about is: when should you rebuild your index?\n\nSome thoughts:\n\n- After inserting >20% new data\n- When query performance degrades noticeably\n- When recall rates drop (test with known queries)\n- Start with sufficient data before creating the initial index\n- Schedule rebuilds during low-traffic periods\n- Consider rebuilding after significant changes in data distribution"
  },
  {
    "metadata": {
      "title": "Mastra is moving into beta",
      "publishedAt": "2025-02-20",
      "summary": "Mastra moving into beta",
      "author": "Sam Bhagwat",
      "draft": false,
      "categories": ["announcements"]
    },
    "slug": "beta-launch",
    "content": "Excited to announce that Mastra, the Typescript agent framework, is moving into beta.\n\nWe’re [a lot of the former Gatsby team](https://x.com/calcsam/status/1882903164858548402), and we’re building Mastra (https\\://mastra.ai), an open-source JavaScript agent framework on top of Vercel’s AI SDK.\n\nThe backstory here: Abhi Aiyer, Shane Thomas and I were working on an AI-powered CRM but it felt like we were having to roll all the AI bits (agentic workflows, evals, RAG) ourselves.\n\nWe also noticed our friends getting stuck debugging prompts, figuring out why their agents called (or didn’t call) tools, and writing lots of custom memory retrieval logic.\n\nAt some point we just looked at each other and were like, why aren't we trying to make this part easier. Here’s a [demo video](https://www.youtube.com/watch?v=8o_Ejbcw5s8)\n\nOne thing we heard from folks is that seeing input/output of every step, of every run of every workflow, is very useful. So we took XState and built a workflow graph primitive on top with OTel tracing. We wrote the APIs to make control flow explicit: `.step()` for branching, `.then()` for chaining, and `.after()` for merging. We also added .`.suspend()/.resume()` for human-in-the-loop.\n\nWe abstracted the main RAG verbs like `.chunk()`, `embed()`, `.upsert(),` `.query` , and `rerank()` across document types and vector DBs. We shipped an eval runner with evals like completeness and relevance, plus the ability to write your own.\n\nThen we read the MemGPT paper and implemented agent memory on top of AI SDK with a `lastMessages` key, `topK` retrieval, and a `messageRange` for surrounding context (think `grep -C`).\n\nBut we still weren’t sure whether our agents were behaving as expected, so we built a local dev playground that lets you curl agents/workflows, chat with agents, view evals and traces across runs, and iterate on prompts with an assistant. The playground uses a local storage layer powered by libsql (thanks Turso team!) and runs on localhost with `npm run dev` (no Docker).\n\nMastra agents originally ran inside a Next.js app. But we noticed that AI teams’ development was increasingly decoupled from the rest of their organization, so we built Mastra so that you can also run it as a standalone endpoint or service.\n\nSome things people have been building so far: one user automates support for an iOS app he owns with tens of thousands of paying users. Another bundled Mastra inside an Electron app that ingests aerospace PDFs and outputs CAD diagrams. Another is building WhatsApp bots that let you chat with objects like your house.\n\nWe (for now) have adopted an Elastic v2 license. The agent space is pretty new, and we wanted to let users do whatever they want with Mastra but prevent, eg, AWS from grabbing it.\n\nWe believe that any developer should be able to build and productize a human-level agent or assistant. It should be as easy to build an agent as it is to build a website.\n\n## Beta release notes\n\nTo recap, agents are a layer on top of LLM calls that maintain state, making decisions, and using tools to accomplish tasks. Think of them as stateful workers that can reason about problems and take actions.\n\n[Mastra Agents](https://mastra.ai/docs/agents/00-overview) have access to tools, workflows, and synced data, enabling them to perform complex tasks and interact with external systems. Agents can invoke your custom functions, utilize third-party APIs through integrations, and access knowledge bases you have built.\n\nHere are some things we added in the beta:\n\n### `generate()` and `stream()` APIs\n\nWe introduced `generate()` and `stream()` to simplify LLM calls:\n\n- `generate()` returns a single, synchronous completion along with metadata (tokens, usage, etc.).\n\n- `stream()` provides partial outputs in real time for chat-like interfaces.\n\nBoth methods work with Mastra’s agent architecture and OTel logging. This keeps your prompt, memory, and tool usage all in one place.\n\n### Advanced agent memory\n\nWe have added a number of different backends, as well as memory compression:\n\n- Hierarchical Memory Storage: Organize context in layers (recent, mid-term, long-term).\n\n- Long-Term Compression: Summarize older context while preserving key details.\n\n- Vector Search Memory: Embed historical data and retrieve relevant context on demand.\n\nThis setup prevents memory bloat, speeds up retrieval, and keeps agent context focused.\n\n### Tool registry support via MCP\n\nWe added a support for tools under MCP (Model Context Protocol):\n\n- Unified Registry: Declare all tools in one place instead of wiring them manually.\n\n- Access Control: Restrict which tools each agent can use to maintain safety and permissions.\n\n- Visual Listing: `mastra dev` shows each tool’s methods and parameters for quick reference.\n\nSee the [Tools guide](https://mastra.ai/docs/guides/03-stock-agent) for details on how to configure MCP in your code.\n\n### Workflows: orchestrate complex tasks\n\nMost AI applications need more than a single call to a language model. You may want to run multiple steps, conditionally skip certain paths, or even pause execution altogether until you receive user input. Sometimes your agent tool calling is not accurate enough.\n\n[Workflows](https://mastra.ai/docs/workflows/00-overview) let you not only control the general flow of task-execution but also add checkpoints, moments when computation is suspended (so a human can provide feedback or guidance to the agent) before the workflow is resumed and ultimately completed.\n\nWe’ve built [several workflow patterns](https://mastra.ai/blog/building-workflows) in Mastra that you can add to projects and customize.\n\n#### Better workflow control flow APIs\n\nAI engineering is building production-ready ETL pipelines and should be described with dataflow nouns and verbs. Users [say things like](https://www.reddit.com/r/LangChain/comments/1fjmr4t/comment/m93huxf/) “it kind of feels like how I’d have done if I rolled my own”\n\nThey look like \\`workflow\\.step()\\` for step creation, \\`workflow\\.then()\\` for chaining, \\`workflow\\.after()\\` for branching and coalescing branches.\n\n#### Suspend/resume\n\nWe also introduced a [suspend/resume](https://mastra.ai/docs/workflows/suspend-and-resume) mechanism that allows you to pause a workflow partway through, gather additional data or human feedback, and then continue exactly where you left off. \nThis is particularly valuable for tasks that rely on asynchronous user interaction or third-party API responses that may arrive minutes (or even hours) later.\n\n### RAG (Retrieval Augmented Generation) \n\nRetrieval Augmented Generation (RAG) is essential for grounding AI responses in factual data. Here are some things we added in alpha:\n\n#### Standardized APIs to process and embed documents\n\nWe noticed that every team was writing their own document ingestion code. So we introduced clear patterns and example code for parsing, chunking, and embedding text from various sources (PDFs, HTML, Markdown, etc.). You can define chunk sizes and embedding configurations in your `mastra.config.ts`, which makes it straightforward to integrate with vector stores like libsql, Pinecone, or pgvector for retrieval-augmented workflows.\n\n#### Chunking and embedding strategies for optimal retrieval\n\nWe’ve introduced multiple chunking strategies—including semantic chunking (where we break up text by semantic boundaries rather than just raw token counts) and sliding window approaches. Plus, Mastra will help you keep track of overlaps, so important context isn’t lost in mid-sentence breaks.\n\n#### Reranking\n\nFinally, we added a reranking layer that sits between your vector search results and the final output. Mastra will take the top-N results and run them through a reranking algorithm to reorder them by likely relevance.\n\nWith reranking, you can drastically improve answer correctness without having to guess at hyperparameters in your vector search. There’s a built-in reranking function you can toggle on in \\`mastra.config.ts\\`. Or you can provide your own custom logic.\n\n### Mastra dev: from playground to prod\n\n[Mastra dev](https://mastra.ai/docs/local-dev/mastra-dev) is your local playground for experimenting with agents, tools, and workflows in one place. Spin it up with a single command to interact with your agents in real time. You can observe each step of the agent’s decision-making, review prompts and responses, and debug function calls on the fly.\n\nThis setup lets you quickly iterate on prompts, workflows, and integration logic.\n\nMastra dev includes:\n\n- an agents playground so you can chat with your agents\n\n- `/generate` and `/stream` endpoints for each of your agents so you can test them via curl or call them over the network\n\n- visual workflow diagrams for each workflow\n\n- endpoints for each of your workflows so you can test them via curl or call them over the network\n\n- a registry of all of your tools\n\n[Here's](https://youtu.be/spGlcTEjuXY?si=h3mhs2A-n4TBrZh7) Shane demoing Mastra Dev.\n\n### Built-in evals and prompt CMS\n\nEvals are automated tests that evaluate LLM outputs using model-graded, rule-based, and statistical methods. Each eval returns a normalized score between 0-1 that can be logged and compared. They can also be customized with your own prompts and scoring functions.\n\nWe built a @mastra/evals [package](https://mastra.ai/blog/introducing-mastra-evals) for systematic agent evaluation, including metrics for answer relevancy, completeness, and tone consistency. We also integrated that with @mastra/core for standardized evaluation hooks.\n\nWe added them to the \\`mastra dev\\` console too…. \\[see the gif I tweeted]\n\n### Mastra Create\n\nBuild projects in a standalone way using `npm create mastra`.\n\nThis will create a new project scaffold with directories and (if you want) examples. We've found this to be a cleaner DX for new users.\n\nWhile you can still initialize within an existing project (`mastra init`), `mastra create` generates a ready-to-use project scaffold—complete with recommended directories, configuration, and boilerplate.\n\n## Getting started & sharing feedback\n\nWe're very excited to see what you'll build. Please `npm create mastra@latest` and [tell](https://discord.gg/BTYqqHKUrf) [us](https://x.com/mastra_ai) everything you love or hate.\n\n## Next up\n\nOur [blog](https://mastra.ai/blog) and [Twitter](https://x.com/mastra_ai) accounts are good places to find the latest on Mastra. We're also constantly updating our [docs](https://mastra.ai/docs) and adding new [working code examples](https://mastra.ai/examples) to help anyone get started with Mastra.\n\nLastly, consider joining [our workshop series](https://lu.ma/mastra). Every week, we'll be tackling a new piece of the puzzle: agents, evals, RAG. We'll build something in under an hour."
  },
  {
    "metadata": {
      "title": "Solving the hardest problem in computer science with LLMs",
      "publishedAt": "2025-02-14",
      "author": "Sam Bhagwat",
      "summary": "How we used LLMs to do API design at Mastra",
      "draft": false,
      "categories": ["foundations"]
    },
    "slug": "solving-the-hardest-problem-in-cs",
    "content": "We all have been using LLM codegen for a bunch of things, but I wanted to share a niche but incredibly high leverage example for us at Mastra: naming things.\n\nWe all know that there are only two hard problems in computer science, cache invalidation and naming things.\n\nHot take: there may only be one now. Naming things is solved.\n\n---\n\nSome backstory: Mastra is an open-source Typescript agent framework. When you’re building a framework, you need to do a lot of API design, specifically the nouns and verbs your users use.\n\nClass and function names and parameters need to be immediately intuitive for your users.\n\nShould a variable be a function parameter or part of an options object? If you have a multi-tiered configuration, can you set defaults on a class but override them on a particular invocation? Do you want to be descriptive (`messageHistory`) or evocative ( `semanticRecall`)?\n\nLeft to their own devices even great engineers tend to ship overly verbose API names (eg `injectVectorHistorySearch`), create overly large options dictionaries, and overload methods in unintuitive ways.\n\nYour framework starts feeling heavyweight.\n\nDoes this matter? **Yes, it absolutely matters.**\n\nI was the cofounder of Gatsby, and we had way too many APIs. Meanwhile, Vercel and Next.js figured out how to ship minimal, evocative powerful APIs. This was a top 3 reason Next.js won.\n\nThe struggle for me and Kyle Mathews (my Gatsby cofounder) was to articulate our taste around APIs in a way that was legible to others, as well as meaningfully review APIs others proposed.\n\nAnd that was incredibly difficult....until AI.\n\nNow, I can describe the parameters of an API problem to Claude -- on web or in Windsurf locally -- and ask it for help. It can access all the underlying code. It will propose five or six plausible approaches. Usually one or two of them stand out, and then you can chat and iterate and improve them.\n\nEven better, I can do this collaboratively on a screenshare with an engineer, so they can themselves learn the process and begin to develop better taste for APIs in conjunction with Claude.\n\nAs an example, our workflow graph API has three main methods, `.step()` for branching, `.then()` for chaining, and `.after()` for merging.\n\n```typescript showLineNumbers copy\nmyWorkflow\n  .step(stepOne)\n  .then(stepTwo)\n  .after(stepOne)\n  .step(stepThree)\n  .then(stepFour)\n  .after([stepTwo, stepFour])\n  .step(stepFive)\n  .commit();\n```\n\n_An example Mastra workflow_. See our [workflow blog post](./building-workflows).\n\nEvery time we show it to a developer building with AIs, they immediately nod along. The control flow is apparent and intuitive."
  },
  {
    "metadata": {
      "title": "Using Vercel's AI SDK with Mastra",
      "publishedAt": "2025-02-13",
      "author": "Shane Thomas",
      "summary": "How Mastra enhances Vercel's AI SDK with agent primitives, RAG pipelines, and evaluation capabilities",
      "draft": false,
      "categories": ["foundations"]
    },
    "slug": "using-ai-sdk-with-mastra",
    "content": "One of the most popular TypeScript AI libraries is [Vercel's AI SDK](https://sdk.vercel.ai/). The AI SDK offers model routing (a unified interface on top of OpenAI, Anthropic, etc), structured output, and tool calling. It's a great foundation for building chatbots and prototyping proof of concepts.\n\nWe built Mastra as a framework on top of the AI SDK to help teams build their proof-of-concepts into production-ready apps.\n\n![AI SDK diagram](/mastra-ai-sdk.png)\n\n### Mastra agents with Vercel's AI SDK\n\nWhile the AI SDK provides the basic infrastructure for tool calling, the basic Mastra agent is built on top of the AI SDK structured output, model routing, and tool calling.\n\n```typescript\nimport { openai } from \"@ai-sdk/openai\";\nimport { Agent } from \"@mastra/core/agent\";\n\nconst agent = new Agent({\n  name: \"WeatherAgent\",\n  instructions: \"Instructions for the agent...\",\n  model: openai(\"gpt-4-turbo\"), // Model comes directly from AI SDK\n});\n\n// Use the agent\nconst result = await agent.generate(\"What is the weather like?\");\n```\n\nBut there are lots more things you get with Mastra, too:\n\n## 1. Agent Memory\n\nTo work effectively, AI agents need the right context from previous interactions. But the right context is often hard to define programmatically! This is where agent memory comes in.\n\nImplementing a memory system for agents usually involves:\n\n- Persisting memory in backend storage\n- Storing and retrieving conversation history\n- Finding relevant context from past interactions using semantic search\n\nSo we built a Memory API for agents in Mastra. This includes:\n\n- Conversation context management\n- Semantic search over past interactions\n- Storage backend abstraction\n- Thread sharing between agents\n\n![Memory retrieval](/memory-retrieval-dark.png)\n\n### Memory Configuration\n\nHere's a reasonable set of parameters for message retrieval and search that you might give an agent.\n\nIt includes both a number of recent messages, as well as older messages that are semantically similar, as well as the context surrounding those messages.\n\n```typescript copy showLineNumbers\nawait agent.stream(\"Message text\", {\n  memoryOptions: {\n    lastMessages: 10,\n    semanticRecall: {\n      topK: 3,\n      messageRange: 5,\n    },\n  },\n});\n```\n\n### Threads and Resources\n\nWhen making requests, you can specify some global parameters for the agent. You can specify conversation threads and resources (usually scoped to users or projects), to group context.\n\n- `resourceId`: Identifier for the user/entity making the request\n- `threadId`: Identifier for the conversation thread\n\n```typescript copy showLineNumbers\nawait agent.stream(\"Message text\", {\n  resourceId: \"user_123\",\n  threadId: \"thread_123\",\n});\n```\n\n### 2. Workflow Graphs\n\n![Diagram showing workflow with parallel steps](/parallel-chains.png)\n\nMastra provides a workflow graph system for building more deterministic AI pipelines.\n\n- Graph-based workflow engine built on XState\n- Simple control flow syntax for branching (`.step()`), chaining (`.then()`), merging (`.after()`), conditions (`when: { ...}`), and interrupts (`.suspend()`, `.resume()`)\n- Built-in Otel logging\n\nMore on this in our [workflow blog post](./building-workflows).\n\n### 3. Agent Development Environment\n\nMastra Dev provides a powerful local development environment for building and testing AI applications.\n\n\n![Mastra Dev Interface](/images/mastra-dev.png)\n\nSome things this includes: you can chat with your agents, visualize their state and memory, debug tool calls and execution paths, and test different prompts and configurations.\n\nYou can curl agents/workflows, chat with agents, view evals and traces across runs, and iterate on prompts with an assistant. The playground uses a local storage layer powered by libsql (thanks Turso team!) and runs on localhost with `npm run dev` (no Docker).\n\nHere's [docs](https://mastra.ai/docs/local-dev/mastra-dev) and a [prompt demo](https://x.com/calcsam/status/1889856384549982419)\n\n### 4. RAG Pipeline Primitives\n\nWe abstracted the main RAG verbs like `.chunk()`, `embed()`, `.upsert(),’ `.query()`, and `rerank()` across document types (text, HTML, Markdown, JSON) and vector DBs (Pincone, Pgvector, Qdrant).\n\nWe abstracted a metadata querying layer with a MongoDB/`sift`-like querying syntax on top. (`$not`, `$and`, `$or`, `$in`, etc)\n\nHere's [our RAG documentation](https://mastra.ai/docs/rag/overview).\n\n### 5. Evals\n\nWe shipped an eval runner with 15 evals around accuracy and reliability, context, and output quality. We noticed a lot of teams seemed hesistant to even start with evals due to lack of knowledge, so we wanted to ship evals that we ready to use, or be modified, as well as the ability to write your own.\n\nHere's [our eval documentation](https://mastra.ai/docs/evals/00-overview).\n\n## A Personal Note\n\nTechnology choice is always a human decision, too. We spent most of the last decade building Gatsby.\n\nAlong the way, we gained a lot of respect for what Guillermo and the Vercel team have built. We've spent quite a bit of time with the AI SDK team as well. Their stuff is solid, and we're excited to build on top."
  },
  {
    "metadata": {
      "title": "Mastra Changelog 2025-02-10",
      "summary": "Major updates including AI SDK integration, optimized bundle size through subpath imports, and enhanced storage layer with tracing capabilities",
      "author": "Sam Bhagwat",
      "publishedAt": "2025-02-10T18:05:00-08:00",
      "draft": false,
      "categories": ["changelogs"]
    },
    "slug": "changelog-2025-02-10",
    "content": "This week we focused on streamlining Mastra's core architecture through better integration with AI SDK, optimized bundle sizes, and improved developer tooling.\n\nWe've made significant changes to model routing, introduced subpath imports for better tree shaking, and added tracing capabilities to our storage layer.\n\n## Users should be using `ai-sdk` for model routing\n\nWe removed Mastra's own model routing implementation (which was built on top of the Vercel AI SDK) and instead recommend using `ai-sdk` directly for model routing. This makes it clear that the happy path for Mastra is using AI SDK and Mastra together.\n\nHere's how to use AI SDK with Mastra:\n\n```typescript\nimport { openai } from \"@ai-sdk/openai\";\nimport { Agent } from \"@mastra/core/agent\";\n\nconst agent = new Agent({\n  name: \"WeatherAgent\",\n  instructions: \"Instructions for the agent...\",\n  model: openai(\"gpt-4-turbo\"), // Model comes directly from AI SDK\n});\n\n// Use the agent\nconst result = await agent.generate(\"What is the weather like?\");\n```\n\nEventually we will also support alternate model routing providers, like Langchain, but that is not part of this release.\n\nIn practice, what this means is that functions like `embed` and `embedMany` should be imported directly from `ai-sdk`, and agents now take models from `ai-sdk` directly, rather than `@mastra/core`.\n\nWe removed the `LLM` object on the `Mastra` class.\n\n## New short-term working memory\n\nThe short-term working memory system uses XML-like tags to track and update contextual information during conversations. When enabled, agents can maintain persistent information through a stream-based approach where memory updates are automatically managed through tagged data in the conversation stream, with the option to mask these updates from end users.\n\n## New Bundling System with Subpath Imports\n\nMastra has added a bundling system using Rollup with tree shaking enabled to eliminate unused code across packages.\n\nIn order to take advantage of this, we recommend using the recommended import syntax:\n\n```typescript\nimport { Agent } from \"@mastra/core/agent\";\n```\n\nThis is a change from the previous pattern of `import { Agent } from '@mastra/core';`.\n\nAlong with the recent model routing changes, the core bundle size is down to 270kb from around 1MB.\n\n## DefaultStorage primitive with LibSQL\n\nMastra now uses LibSQL as the default storage primitive. Here's how to use it:\n\n```typescript\nimport { DefaultStorage } from \"@mastra/core/storage\";\n\nconst storage = new DefaultStorage({\n  config: {\n    url: \":memory:\",\n  },\n});\n\n// Initialize tables\nawait storage.init();\n```\n\nThe storage primitive handles:\n\n- Thread and message storage\n- Workflow snapshots\n- Evaluation results\n\n## Mastra storage includes tracing (soon)\n\nMastra's LibSQL storage layer includes tracing, and we're working on adding a UI in `mastra dev` for it. We should merge this in the next couple of days: [https://github.com/mastra-ai/mastra/pull/1820](https://github.com/mastra-ai/mastra/pull/1820)"
  },
  {
    "metadata": {
      "title": "Using Mastra's Agent Memory API",
      "summary": "A technical guide to implementing agent memory in Mastra, covering storage, vector search, and context management",
      "author": "Tyler Barnes",
      "publishedAt": "2025-02-04",
      "draft": false,
      "categories": ["foundations"]
    },
    "slug": "agent-memory-guide",
    "content": "To work effectively, AI agents need the right context from previous interactions. But the right context is often hard to define programmatically! This is where agent memory comes in.\n\nImplementing a memory system for agents usually involves:\n\n- Persisting memory in backend storage\n- Storing and retrieving conversation history\n- Finding relevant context from past interactions using semantic search\n\nSo we built a Memory API for agents in Mastra. This includes:\n\n- Conversation context management\n- Semantic search over past interactions\n- Storage backend abstraction\n- Thread sharing between agents\n\nThis post will walk you through implementing these capabilities in your application, from local development to production deployment.\n\n## Implementation Guide\n\nLet's build an agent with memory capabilities. Mastra's Memory API comes with sensible defaults that work out of the box, while still being configurable when needed.\n\nPrerequisites:\n\n```bash npm2yarn copy\nnpm install @mastra/core @mastra/memory\n```\n\n### Setting Up Memory\n\nThe default configuration initializes a LibSQL database for memory retrieval, storage, and semantic search and uses the `fastembed-js` library to download a local model for on-device embedding.\n\nBecause of this, you can initialize a Memory instance without any configuration:\n\n```tsx\nimport { Memory } from \"@mastra/memory\";\n\nconst memory = new Memory();\n```\n\nNow let's create an agent and attach memory to it:\n\n```tsx\nimport { Agent } from \"@mastra/core\";\nimport { Memory } from \"@mastra/memory\";\n\nconst memory = new Memory();\n\nconst agent = new Agent({\n  memory,\n  // Additional agent configuration options\n});\n```\n\nThat's it! You've built an agent with a memory system that includes:\n\n- Persistent storage via LibSQL\n- Built-in vector search and embedding capabilities via `fastembed-js`\n- Automatic context management\n\nFor custom storage backends, vector databases, or embedding models, check out the [Memory Configuration Guide](https://mastra.ai/docs/agents/01-agent-memory#basic-configuration).\n\n### Configure Memory Parameters\n\nHere's a reasonable set of parameters for message retrieval and search that you might give an agent. It includes both a number of recent messages, as well as older messages that are semantically similar, as well as the context surrounding those messages.\n\n```tsx\nconst memory = new Memory({\n  options: {\n    lastMessages: 100, // Number of recent messages to include\n    semanticRecall: {\n      topK: 2, // Number of similar messages to retrieve\n      messageRange: 2, // Number of messages before/after each result\n    },\n  },\n});\n```\n\n### Advanced Memory Usage\n\n---\n\n#### Using Resource and Threads\n\nNote that when making requests, you can specify some global parameters for the agent, if you want to attach it onto resources or threads that are available to other agents:\n\n- `resourceId`: Identifier for the user/entity making the request\n- `threadId`: Identifier for the conversation thread\n\n```tsx\nawait agent.stream(\"Message text\", {\n  resourceId: \"user_123\",\n  threadId: \"thread_123\",\n});\n```\n\n#### Overriding Memory Parameters\n\nWhile an agent has a default memory configuration, you can override those parameters for an individual request:\n\n```tsx\nawait agent.stream(\"Message text\", {\n  memoryOptions: {\n    lastMessages: 10,\n    semanticRecall: {\n      topK: 3,\n      messageRange: 5,\n    },\n  },\n});\n```\n\n## Summary\n\nThe Memory API implements these core functions:\n\n1. **Message Storage**: Stores messages and metadata in SQL tables, indexed by thread and resource IDs\n2. **Vector Search**: Converts messages to embeddings for similarity search\n3. **Context Management**: Retrieves recent messages and semantically similar historical context\n4. **Thread Sharing**: Enables multiple agents to access shared conversation threads\n\n## Documentation Links\n\n- [Memory API Documentation](https://mastra.ai/docs/agents/01-agent-memory)\n- [Code Examples](https://mastra.ai/examples/memory)\n- [API Reference](https://mastra.ai/docs/api/memory)\n- [Storage Configuration](https://mastra.ai/docs/agents/01-agent-memory#storage-options)\n- [Vector Search Configuration](https://mastra.ai/docs/rag/vector-databases)"
  },
  {
    "metadata": {
      "title": "Mastra Changelog 2025-01-31",
      "publishedAt": "2025-01-31",
      "summary": "New Memory API, speech providers, vector store integrations, and core improvements",
      "author": "Sam Bhagwat",
      "draft": false,
      "categories": ["changelogs"]
    },
    "slug": "changelog-2025-01-31",
    "content": "Welcome to Mastra Changelog #4. This release introduces persistent storage with the Memory API, replaces Docker/Postgres with LibSQL, and splits vector stores into modular packages. The core package is now 50% smaller with improved tree-shaking.\n\nAnd....stick around to the end for the \"one more thing\" drop.\n\n## Memory API\n\nThe new Memory API adds persistent storage and vector search capabilities:\n\n- `MastraStorage` and `MastraVector` classes for unified memory management\n- Support for both PostgreSQL and Redis as backend storage options\n- Token window management with automatic date-based filtering\n- Memory management helpers in `stream` and `generate`:\n  - `lastMessages`: Control number of recent messages included\n  - `semanticSearch`: Configure semantic search with:\n    - `topK`: Number of similar messages to retrieve\n    - `messageRange`: Messages before/after each result\n\n## LibSQL replacing `@mastra/engine`\n\nWe've replaced `@mastra/engine` with LibSQL as our storage layer, including:\n\n- Persistent storage for agent messages and conversation history\n- Tool call tracking and persistence\n- Workflow state persistence for suspended workflows\n\nThe storage layer powers the `mastra dev` server while eliminating the need for a Docker/Postgres setup locally.\n\n## Speech Provider Modules\n\nWe've replaced `@mastra/tts` with a modular speech provider system:\n\n- Azure Speech (`@mastra/speech-azure`)\n- DeepGram (`@mastra/speech-deepgram`)\n- ElevenLabs (`@mastra/speech-elevenlabs`)\n- Play.ai (`@mastra/speech-playai`)\n\nThe play.ai integration is new.\n\n## Vector Store Integrations\n\nWe've moved our vector store implementations into separate packages for better modularity:\n\n- Cloudflare Vectorize (`@mastra/vector-cloudflare`)\n- LibSQL (`@mastra/vector-libsql`)\n- Astra DB (`@mastra/vector-astra`)\n- Chroma (`@mastra/vector-chroma`)\n- Pinecone (`@mastra/vector-pinecone`)\n- Qdrant (`@mastra/vector-qdrant`)\n- Upstash (`@mastra/vector-upstash`)\n\nNote that the Vectorize and Libsql implementations are new ones! Also, the `pg` filter implementation has been updated, and we've split embed into separate `embed` and `embedMany` functions for more consistent return types.\n\nThe vector store implementations each (still) include:\n\n- Standard methods: `upsert`, `query`, `createIndex`, `listIndexes`\n- Configurable similarity metrics (cosine, euclidean, dotproduct)\n- Metadata filtering support\n- Automatic UUID generation for vector IDs\n\n## Unified Filter API\n\nThe new MongoDB-style filter API is based on `sift`, providing a standard filtering interface across vector stores:\n\n- Query operators: `$eq`, `$ne`, `$gt`, `$gte`, `$lt`, `$lte`\n- Logical operators: `$and`, `$or`, `$not`, `$nor`\n- Array operators: `$in`, `$nin`, `$all`, `$elemMatch`\n- Element operators: `$exists`\n- Regex operators: `$regex`, `$options`\n\nThe API enables hybrid RAG search by combining vector similarity with metadata filtering:\n\n```typescript\nconst results = await vectorStore.query({\n  vector,\n  filter: {\n    date: { $gt: \"2024-01-01\" },\n    tags: { $in: [\"documentation\", \"api\"] },\n    $or: [{ status: \"published\" }, { author: \"admin\" }],\n  },\n});\n```\n\nEach vector store translates these filters to its native query language, maintaining consistent behavior across implementations.\n\n## Core architecture improvements\n\nCore architecture improvements:\n\n- Split core package into separate entry files for better tree-shaking\n- Updated Agent and LLM configuration with granular temperature controls\n- Implemented unified bundling architecture across:\n  - Mastra deployers with optimized asset handling\n  - Vector store modules with type-safe interfaces\n  - Logging system with structured output\n\nThe overall `@mastra/core` package is now 50% smaller and the actual code imported will be even smaller than that (due to better tree-shaking).\n\n## Documentation Updates and bug fixes\n\n- Added intermediate RAG examples with code samples\n- Updated vector store integration guides with configuration examples\n- Added package keyword tags for better discoverability\n- Updated package READMEs with usage patterns\n- Added comprehensive RAG testing suite\n- Fixed workflow condition display in graph visualization\n- Resolved dev watcher file change detection issues\n- Fixed 500 error in memory persistence operations\n- Fixed thread sidebar state management in playground agent chat\n- Fixed logger configuration conflicts in deployment\n- Fixed CLI create command component flag parsing\n\n## ....and one more thing\n\nWe built Mastra Cloud as a serverless platform for AI apps\n\nThe first app we deployed on it was this [pretty cool beat generator](https://x.com/smthomas3/status/1885468281441837416)\n\nStay tuned for more :)"
  },
  {
    "metadata": {
      "title": "Baby steps towards AI Ops",
      "publishedAt": "2025-01-30",
      "author": "Abhi Aiyer",
      "summary": "How do we apply the lessons learned from the last decade of DevOps to manage, deploy, and operate AI agents at scale?",
      "draft": false,
      "categories": ["foundations"]
    },
    "slug": "ai-ops",
    "content": "For the last decade as an engineer, I’ve been deep in the weeds of distributed systems and platform engineering, building clusters on AWS, CGP, and Azure to reliably run highly variable PaaS workloads.\n\nNow, as I’m building Mastra, one thing is clear to me. As AI agents become increasingly central to modern software systems, we're facing a new challenge.\n\n**How do we apply the last decade of DevOps to manage, deploy, and operate AI agents at scale?**\n\nJust as DevOps transformed how we build and run traditional software, we’re learning how to reliably run AI agents in production.\n\n## More autonomy, more problems\n\nTraditional DevOps practices were built around (mostly) deterministic systems - code that behaves the same way given the same inputs.\n\nBut AI agents introduce new layers of complexity: they learn, adapt, and make decisions autonomously. This fundamental difference requires us to rethink our operational practices.\n\nConsider a simple deployment pipeline. In a traditional DevOps setup, we can test specific code paths and can be reasonably confident about how our application will behave.\n\nWith AI agents, we need to validate not just code paths but decision-making patterns, interaction models, and learning behaviors.\n\n## The areas of AI Ops\n\nOkay, so what should we be thinking about?\n\n### Monitoring agent decision-making\n\nTraditional metrics like CPU usage and response time are still important, but AI Ops introduces new dimensions of monitoring.\n\nLet’s say you have two decision paths, and the AI returns a good response in 90% down one path, but 40% down another path.\n\nYou need to understand not just success rates along both paths, but when an LLM system starts going more down one path than usual.\n\nIn other words, we need to track decision quality, interaction patterns, and learning trajectories. Should we add \"Continued Learning\" to \"CI/CD\"?\n\nThis means building new types of observability systems that can understand and validate agent behaviors, not just performance metrics.\n\n## Moving beyond blue-green deploys\n\nRolling out updates to AI agents requires more sophisticated deployment strategies than traditional blue-green deployments.\n\nTeams want to move quickly, but they are scared because non-deterministic systems need more testing than traditional software engineering.\n\nWe need patterns for safely introducing new agent behaviors, testing them in production, and rolling them back if they don't meet our criteria.\n\nThis might mean running multiple versions of agents in parallel and carefully routing traffic between them based on behavioral metrics.\n\nIt might even mean spinning up synthetic clusters that re-run production queries 5 or 10 or 30 times to see not just what happened in reality, but the whole distribution of likely scenarios.\n\nFeature flagging is about to go to a whole new level.\n\n## Enabling human-in-the-loop agentic governance\n\nGovernance today mainly focuses on access control and audit logs. AI agents require a whole new layer of governance.\n\nOur colleagues who have been working in the self-driving space have gotten used to an observing human-in-the-loop as a foundational piece of the puzzle, but we have not _yet_ developed the control primitives for this in most AI applications.\n\n## Baby steps towards AI Ops\n\nHow much do we know about what's required to do good AI Ops today? I'll be very honest, very little. But if you're looking for a starting point, here are some pieces I can think of:\n\n### Tracking stateful systems to reverse-engineer interpretability\n\nCode and configuration often doesn't capture the full state of an agent. RAG results depend on data pipelines. Memory matters.\n\nWe're starting to see new tools and practices for tracking the evolution of agent behaviors over time. What's known as LLM interpretability will take on more importance.\n\nThere is some software engineer right now, somewhere, who is working on a system to answer questions like \"why did the autonomous killer drone decide to detonate?\"\n\n### Testing\n\nEvals, unit tests, and workflow-level integration tests are a good start. But teams need more data.\n\nWe're starting to see teams build simulation environments where they can validate agent behaviors across a wide range of scenarios, using real and synthetic data.\n\n### Monitoring and Alerting\n\nOur monitoring systems need to understand normal vs. abnormal agent behaviors. This goes beyond simple thresholds - we need systems that can detect when agents are drifting from expected behavioral patterns.\n\n### Incident Response\n\nUnless your rollbacks are really good, when an AI agent starts behaving unexpectedly, traditional debugging tools may not be enough. Does your on-call team know how to do prompt engineering?\n\n## Looking forward\n\nAs we deploy more sophisticated AI agents, AI Ops will continue to evolve.\n\nEven in our YC batch we're already seeing the demand for specialized tools for agent monitoring, behavior testing, and deployment management. But the field is still in its early stages, and there's so much room for innovation!\n\nWe're excited to see what the future holds."
  },
  {
    "metadata": {
      "title": "Multi-Agent AI Travel Planning with Mastra",
      "publishedAt": "2025-01-28",
      "author": "Sam Bhagwat",
      "summary": "A demo using Mastra to show how multiple AI agents can work together to plan travel itineraries.",
      "draft": false,
      "categories": ["examples"]
    },
    "slug": "travel-ai",
    "content": "We recently built a demo showing how multiple AI agents can work together to plan travel itineraries.\n\nThe system takes user preferences and returns a complete travel package with flights, accommodations, and activities.\n\n## Implementation\n\nThe system uses two specialized agents:\n\n1. A primary travel agent that coordinates the planning process and interfaces with travel APIs\n2. An analyzer agent that validates and formats the data\n\nHere's how we implemented the primary travel agent:\n\n```typescript\nimport { anthropic } from \"@ai-sdk/anthropic\";\n\nexport const travelAgent = new Agent({\n  name: \"travelAgent\",\n  instructions: `You are an expert travel agent responsible for finding a flight, hotel, and three attractions for a user. You will be given a set of user preferences along with some tools and you will need to find the best options for them. Be as concise as possible with your response.`,\n  model: anthropic(\"claude-3-5-sonnet-20241022\"),\n  tools: {\n    searchFlights,\n    searchHotels,\n    searchAttractions,\n    searchAirbnbLocation,\n    searchAirbnb,\n  },\n});\n```\n\nThe agent receives structured input including:\n\n- Departure and arrival locations\n- Trip goals and interests\n- Flight preferences and priorities\n- Accommodation type and price range\n- Travel dates\n\nHere's the prompt that guides the agent's decision-making:\n\n```typescript\nconst message = `\n  You are a travel agent and have been given the following information about a customer's trip requirements.\n\n  - Find the best flight option for the customer (use departureLocation and arrivalLocation)\n  - Find the best accommodation option (use arrivalCityId)\n  - Find three activities based on their interests (use arrivalAttractionId)\n  - Find the best return flight option\n  - For Airbnb stays, search location then listings (use searchAirbnbLocation then searchAirbnb)\n\n  Notes:\n  - Include layover information when present\n  - Add images for hotels and accommodations\n  - flightPriority ranges 0-100 (0: prioritize price, 100: prioritize convenience)\n  - Use complete timestamps for departure/arrival times\n  - Return complete flight objects\n  - Only call relevant accommodation search based on user preference\n\n  Trip Requirements:\n  Departure: ${formObject.departureLocation}\n  Arrival: ${formObject.arrivalLocation}\n  Goals: ${formObject.tripGoals}\n  ...\n`;\n```\n\nThe analyzer agent then validates and formats the data:\n\n```typescript\nimport { anthropic } from \"@ai-sdk/anthropic\";\n\nexport const travelAnalyzer = new Agent({\n  name: \"travel-analyzer\",\n  instructions:\n    \"You are an expert travel agent responsible for finding a flight, hotel, and three attractions for a user. You will be given a set of user preferences along with some data to find the best options for them.\",\n  model: anthropic(\"claude-3-5-sonnet-20240620\"),\n});\n```\n\nThis agent also gets a fairly detailed prompt:\n\n```typescript\nconst messageToAnalyze = `\n  You are a travel agent analyzing research results.\n\n  Format the response according to the output schema for the travel planner.\n\n  For hotel ratings:\n  - Extract numeric rating from description/accessibilityLabel\n  - Rating format: \"X.X out of 5 stars\" or \"X out of 5 stars\"\n  - Use only first number (before \"out of\")\n  - Rating must be ≤ 5\n  - Include layover details in flight legs\n  - Replace <UNKNOWN> values with empty strings\n\n  ${JSON.stringify(data)}\n`;\n```\n\n## Type Safety\n\nThe system uses TypeScript and Zod schemas throughout:\n\n```typescript\nconst FlightSearchSchema = z.object({\n  budget: z.number(),\n  departure: z.string(),\n  destination: z.string(),\n  dates: z.object({\n    start: z.date(),\n    end: z.date(),\n  }),\n});\n```\n\nThis ensures data consistency between agents and external APIs.\n\nThe code is available on [GitHub](https://github.com/mastra-ai/mastra/tree/main/examples/travel-app), and you can try it at [mastra-eight.vercel.app](https://mastra-eight.vercel.app)."
  },
  {
    "metadata": {
      "title": "Every API needs a natural language endpoint",
      "publishedAt": "2025-01-25",
      "summary": "Building a world where every API has a chat endpoint",
      "author": "Shane Thomas",
      "draft": false,
      "categories": ["foundations"]
    },
    "slug": "api-endpoints",
    "content": "The barrier-to-entry to using developer APIs is significantly high. First you have to be a developer and understand the basics of HTTP requests and responses. Next, you need to spend time reading the API documentation, determining what type of API it is, and then learning how to actually interact with the data the API provides.\n\nLet’s take a step back. Ask yourself: what is the most common purpose behind using developer APIs at all?\n\nUsually it’s just to retrieve and/or update data from a third party system.\n\nSo let’s imagine you are building an ecommerce product that fulfills orders from different vendors. You need to use a vendor’s developer API to look up a product by name and add it to your existing order. Usually, you’d look up the REST endpoints needed and make multiple requests to the API.\n\nWhat if you could do this instead?\n\n```jsx\nconst res = await fetch(`https://imaginaryapi.mastra.com/api/chat`, {\n  method: 'POST',\n  body: {\n    message: `Find product ${product_title} and add it to order ${order_id}`,\n  }\n  headers: {\n    Authorization: `Bearer ${process.env.MY_API_KEY}`,\n  },\n});\n```\n\nIn other words, what if every API had a `/chat` endpoint where you could specify exactly what you need? In this case, it’s to ‘find product X and add it to order Y.’\n\n## What about reliability and latency?\n\nNow you might be thinking: LLMs are not 100% reliable, so why would I want a chat endpoint if I can’t rely on it to work 100% of the time?\n\nAnd what about latency? This request would be slower than simply calling the API directly!\n\nBoth points are true and, at a certain scale, you can’t rely on chat endpoints. But most applications aren’t built for massive levels of scale. They certainly don’t need 4 nines, i.e. 99.99%, of reliability.\n\nEven for the applications that do require guaranteed amounts of reliability, almost all of them start out as prototypes (or at least they should.) And there is no better way to rapidly prototype and iterate than to use natural language on a single API endpoint.\n\nOnce you scale and need such levels of reliability, you will likely either throw away the initial prototype and start over or use Cursor, pass in the API docs, and have it update them all. (Since you are already using natural language in your code, Cursor will do a reasonably good job of using the right endpoints and data structures.)\n\n## What about API versioning?\n\nThe chat endpoint for any individual API will need to be versioned just like any other API — the only difference is that it could change more frequently.\n\nIf the API developer changes the underlying model or system prompt, any applications built on top of that API could be impacted significantly, thus impacting reliability. Because of this, my recommendation would be for every API developer to have rolling versions, i.e. just put in the date of when you started (or last tested) your application. The API should automatically be able to use the correct underlying model and system prompt from that point in time.\n\nApplying it to our example, we get:\n\n```jsx\nconst res = await fetch(`https://imaginaryapi.mastra.com/api/chat`, {\n  method: 'POST',\n  body: {\n    message: `Find product ${product_title} and add it to order ${order_id}`,\n  }\n  headers: {\n    Authorization: `Bearer ${process.env.MY_API_KEY}`,\n    'Api-Version': '2025-01-16',\n  },\n});\n```\n\nNow if you are rebuilding parts of your application, you can always update the `Api-Version` to the current date and retest. Hopefully you get good, if not better, results.\n\n## The future is standardized chat API protocols\n\nNot every API is going to magically develop chat endpoints over the next 6 months but, over time, I believe many will. If we develop a standardized chat API protocol, it’ll be easier for more APIs to adopt chat endpoints and, therefore, easier for developers to integrate APIs into their prototypes and applications."
  },
  {
    "metadata": {
      "title": "Mastra Changelog 2025-01-24",
      "publishedAt": "2025-01-24",
      "summary": "New `mastra build` command, evals in dev playground, new model support, and an abstracted deployer package",
      "author": "Sam Bhagwat",
      "draft": false,
      "categories": ["changelogs"]
    },
    "slug": "changelog-2025-01-24",
    "content": "Welcome to Mastra Changelog #3. Let's dive in.\n\n## Major Updates\n\n### New `mastra build` command\n\nWe've released a new `mastra build` command.\n\nThis command lets you bundle a production version of your Mastra agents and workflows into a Hono server.\n\nThis is good for deploying Mastra on a cloud server or containerized environment (ie, EC2, Digital Ocean, etc).\n\n### Evals added to Dev Playground\n\nThe big news here is that we've added evals to the dev playground.\n\nThis means you can now test your agents and workflows locally and get feedback on their performance.\n\nMetrics can be accessed both during CI and in live environments through the playground.\n\nHere's a screenshot:\n\n![evals](https://mastra.ai/mastra-dev-evals.png)\n\n## More Dev Playground improvements\n\nThe local dev playground is now more powerful and easier to use:\n\n- Migrated to Hono server for better performance\n- Enhanced OpenAPI documentation\n- Added Swagger UI support\n- Automatic rebuild/refresh on file changes\n\nSome build and performance improvements here too:\n\n- Separated playground dependencies from CLI package\n- Improved build & deploy configurations\n- Enhanced tsconfig setup\n\nThese changes make the development experience more responsive and reliable.\n\n### More Evals\n\nBuilding on our recently released `@mastra/evals` framework, we've added new eval metrics:\n\n- Bias detection\n- Hallucination measurement\n- Contextual relevance\n- Summarization quality\n\nIn addition, we've reorganized metrics into NLP and LLM categories for better organization, and metrics now return a standardized format with `score` and `info` object containing detailed information.\n\n### New Model Support\n\nAdded support for:\n\n- Perplexity Sonar model\n- o1 model support\n- DeepSeek-v3 and DeepSeek-r1 model support\n- PlayAI TTS with streaming\n\n### New `@mastra/deployer` package\n\nWe've released a new `@mastra/deployer` package to streamline deployment workflows.\n\nThe package handles deploying Mastra into a serverless environment (ie, bundling and code splitting appropriately), managing secrets, and writing files.\n\nBecause we've abstracted it out into a package, other providers can extend it. If you're interested in adding support for another provider, take a look at the [@mastra/deployer](https://github.com/mastra-ai/mastra/tree/main/packages/deployer) package and the individual provider code: [Cloudflare](https://github.com/mastra-ai/mastra/tree/main/deployers/cloudflare), [Vercel](https://github.com/mastra-ai/mastra/tree/main/deployers/vercel), and [Netlify](https://github.com/mastra-ai/mastra/tree/main/deployers/netlify).\n\n## Mastra Memory (coming soon)\n\nWe're finalizing a number of advanced memory features:\n\n- Hierarchical memory storage\n- Long-term memory via memory compression\n- Vector search memory\n\nThe code is currently in a draft PR on a branch: [https://github.com/mastra-ai/mastra/pull/1525](https://github.com/mastra-ai/mastra/pull/1525)\n\nSneak peek:\n\n![memory](https://mastra.ai/mastra-hierarchical-memory.png)\n\n## Technical Improvements\n\n### RAG & Memory\n\n- Added transaction support to pgvector\n- Improved vector query search functionality\n- Enhanced memory state management\n\n### Bug Fixes\n\n- Fixed agent generate/stream with structured output in dev playground\n- Resolved workflow trigger handling issues\n- Addressed CLI build and deployment edge cases\n\n## Package Version Updates\n\n- `@mastra/core` (0.1.27-alpha.68)\n- `@mastra/cli` (0.1.57-alpha.106)\n- `@mastra/deployer` (0.0.1-alpha.6)\n- `@mastra/evals` (0.1.0-alpha.8)\n- `@mastra/engine` (0.0.5-alpha.62)\n- `@mastra/memory` (0.0.2-alpha.48)\n- `@mastra/mcp` (0.0.1-alpha.8)\n- `@mastra/rag` (0.0.2-alpha.54)\n- `@mastra/tts` (0.0.1-alpha.15)\n\n[Source: Mastra Changelog 2025-01-17](https://mastra.ai/blog/changelog-2025-01-17), [Mastra Changelog 2025-01-10](https://mastra.ai/blog/changelog-2025-01-10)"
  },
  {
    "metadata": {
      "title": "Introducing Evals in Mastra",
      "publishedAt": "2025-01-20",
      "summary": "A guide to Mastra's evaluation metrics suite, demonstrating how to measure and validate AI model outputs across dimensions including answer relevancy, completeness, content similarity, context usage, and tone consistency.",
      "author": "Nik Aiyer",
      "draft": false,
      "categories": ["announcements"]
    },
    "slug": "introducing-mastra-evals",
    "content": "## Using Evaluation Metrics in Mastra\n\nEvaluation metrics help measure and validate AI model outputs across different dimensions. Here's a comprehensive guide to Mastra's evaluation suite. All of these metrics output a score between 0 and 1.\n\nWe are starting with some NLP-based metrics, and will add additional LLM-as-judgemetrics soon.\n\n## Core Evaluation Metrics\n\n### Model Configuration\n\nFirst, set up the model configuration needed for several metrics:\n\n```typescript\nimport { ModelConfig } from \"@mastra/core\";\n\nconst model: ModelConfig = {\n  provider: \"OPEN_AI\",\n  model: \"gpt-4\",\n  apiKey: process.env.OPENAI_API_KEY,\n};\n```\n\n### Answer Relevancy\n\nAnswer relevancy evaluates if responses address queries appropriately:\n\n```typescript\nimport { AnswerRelevancyMetric } from \"@mastra/evals/llm\";\n\nconst metric = new AnswerRelevancyMetric(model, {\n  uncertaintyWeight: 0.3,\n  scale: 10,\n});\n\nconst result = await metric.measure({\n  input: \"What is the capital of France?\",\n  output: \"Paris is the capital of France.\",\n});\n```\n\nThis metric uses an LLM to judge how well responses address queries, scoring yes/no/unsure verdicts with uncertainty weighting. Score is calculated as (relevancyCount / totalVerdicts) \\* scale, where \"unsure\" counts as 0.3.\n\n### Completeness\n\nCompleteness measures how thoroughly a response covers the key elements from the input:\n\n```typescript\nimport { CompletenessMetric } from \"@mastra/evals/nlp\";\n\nconst metric = new CompletenessMetric();\n\nconst result = await metric.measure({\n  input: \"Explain the water cycle: evaporation, condensation, precipitation\",\n  output:\n    \"Water evaporates from surfaces, forms clouds through condensation, and returns as precipitation\",\n});\n```\n\nSpecifically, this metric extracts and compares key elements (nouns, verbs, topics) between input and output using NLP. Score represents the ratio of matched elements to total elements, with intelligent partial matching for longer words.\n\n### Content Similarity\n\nThis metric performs direct string comparison using a similarity library, with configurable case and whitespace sensitivity. Perfect matches score 1.0, with decreasing scores based on string differences.\n\n```typescript\nimport { ContentSimilarityMetric } from \"@mastra/evals/nlp\";\n\nconst metric = new ContentSimilarityMetric({\n  ignoreCase: true,\n  ignoreWhitespace: true,\n});\n\nconst result = await metric.measure({\n  input: \"The quick brown fox\",\n  output: \"the Quick Brown fox\",\n});\n```\n\n### Context Position\n\nContext position evaluates how well the model uses ordered context. Earlier positions are weighted more heavily (weight = 1/position).\n\nThe final score is the ratio of weighted relevant items to maximum possible weighted sum.\n\n```typescript\nimport { ContextPositionMetric } from \"@mastra/evals/llm\";\n\nconst metric = new ContextPositionMetric(model, {\n  scale: 10,\n});\n\nconst result = await metric.measure({\n  input: \"Summarize the events\",\n  output: \"First came A, then B, finally C\",\n  context: [\n    \"A occurred in the morning\",\n    \"B happened at noon\",\n    \"C took place in the evening\",\n  ],\n});\n```\n\n### Context Precision\n\nContext precision measures accurate use of provided context by calculating precision at each relevant position in the response. Score is normalized sum of precision values at relevant positions divided by number of relevant items.\n\n```typescript\nimport { ContextPrecisionMetric } from \"@mastra/evals/llm\";\n\nconst metric = new ContextPrecisionMetric(model, {\n  scale: 10,\n});\n\nconst result = await metric.measure({\n  input: \"What did the research find?\",\n  output: \"The study found significant improvements\",\n  context: [\n    \"Research showed 45% improvement\",\n    \"Results were statistically significant\",\n  ],\n});\n```\n\n### Difference\n\nDifference metric calculates detailed text differences:\n\n```typescript\nimport { TextualDifferenceMetric } from \"@mastra/evals/nlp\";\n\nconst metric = new TextualDifferenceMetric();\n\nconst result = await metric.measure({\n  input: \"Original text version\",\n  output: \"Modified text version\",\n});\n// Provides ratio, number of changes, and length differences\n```\n\n### Keyword Coverage\n\nExtracts and compares keywords between input and output using a keyword extraction library. Score is simply matched keywords divided by total keywords from input.\n\n```typescript\nimport { KeywordCoverageMetric } from \"@mastra/evals/nlp\";\n\nconst metric = new KeywordCoverageMetric();\n\nconst result = await metric.measure({\n  input: \"Explain photosynthesis: chlorophyll, sunlight, glucose\",\n  output: \"Plants use chlorophyll to convert sunlight into glucose\",\n});\n```\n\n### Prompt Alignment\n\nUses an LLM to check adherence to specific instructions with binary yes/no scoring. Final score is the ratio of followed instructions to total instructions, scaled to range.\n\n```typescript\nimport { PromptAlignmentMetric } from \"@mastra/evals/llm\";\n\nconst metric = new PromptAlignmentMetric(model, {\n  instructions: [\n    \"Use formal language\",\n    \"Include specific examples\",\n    \"Stay under 100 words\",\n  ],\n  scale: 10,\n});\n\nconst result = await metric.measure({\n  input: \"Describe quantum computing\",\n  output: \"Quantum computing uses quantum bits...\",\n});\n```\n\n### Tone Consistency\n\nAnalyzes sentiment consistency either between input/output or within sentences of a single text. Score is based on sentiment difference or variance, with 1.0 indicating perfect consistency.\n\n```typescript\nimport { ToneConsistencyMetric } from \"@mastra/evals/nlp\";\n\nconst metric = new ToneConsistencyMetric();\n\nconst result = await metric.measure({\n  input: \"Write a positive product review\",\n  output: \"This product exceeded my expectations!\",\n});\n// Measures sentiment stability and alignment\n```\n\n## Combining Metrics\n\nCombine metrics for more assessment:\n\n```typescript\nimport {\n  AnswerRelevancyMetric,\n  ContextPrecisionMetric,\n  PromptAlignmentMetric,\n  ToneConsistencyMetric,\n} from \"@mastra/evals/llm\";\n\nimport {\n  CompletenessMetric,\n  ContentSimilarityMetric,\n  TextualDifferenceMetric,\n  KeywordCoverageMetric,\n  ToneConsistencyMetric,\n} from \"@mastra/evals/nlp\";\n\nasync function evaluateResponse({\n  input,\n  output,\n  context,\n  instructions,\n}: {\n  input: string;\n  output: string;\n  context?: string[];\n  instructions?: string[];\n}) {\n  const model: ModelConfig = {\n    provider: \"openai\",\n    model: \"gpt-4\",\n    apiKey: process.env.OPENAI_API_KEY,\n  };\n\n  const metrics = [\n    new AnswerRelevancyMetric(model),\n    new CompletenessMetric(),\n    new ContentSimilarityMetric(),\n    new ContextPrecisionMetric(model),\n    new TextualDifferenceMetric(),\n    new KeywordCoverageMetric(),\n    new PromptAlignmentMetric(model, { instructions: instructions || [] }),\n    new ToneConsistencyMetric(),\n  ];\n\n  const results = await Promise.all(\n    metrics.map((metric) =>\n      metric.measure({\n        input,\n        output,\n        context,\n      }),\n    ),\n  );\n\n  return results;\n}\n```"
  },
  {
    "metadata": {
      "title": "Introducing TTS in Mastra",
      "publishedAt": "2025-01-20",
      "summary": "Use Mastra's TTS system with OpenAI's speech models to create text-to-speech applications, covering voice selection, audio generation, streaming capabilities",
      "author": "Sam Bhagwat",
      "draft": false,
      "categories": ["announcements"]
    },
    "slug": "tts-support",
    "content": "## Building Text-to-Speech Applications with Mastra\n\nWe recently shipped a TTS module that integrates with OpenAI and ElevenLabs speech models.\n\nLet's explore how to use it.\n\n## Basic Setup\n\nFirst, install the required package:\n\n```bash\nnpm install @mastra/tts\n```\n\nConfigure your environment:\n\n```bash\nOPENAI_API_KEY=your_api_key_here\n```\n\n## Basic TTS Usage\n\nInitialize the TTS client:\n\n```typescript\nimport { OpenAITTS } from \"@mastra/tts\";\n\nconst tts = new OpenAITTS({\n  model: {\n    name: \"tts-1\",\n  },\n});\n```\n\n## Voice Selection\n\nOpenAI provides several voices to choose from:\n\n```typescript\nconst voices = await tts.voices();\n// Available voices: alloy, echo, fable, onyx, nova, shimmer\n```\n\n## Generate Speech\n\nGenerate audio with your chosen voice:\n\n```typescript\nconst { audioResult } = await tts.generate({\n  text: \"Hello, world!\",\n  voice: \"nova\",\n  speed: 1.0,\n});\n```\n\n## Streaming Audio\n\nFor real-time audio streaming:\n\n```typescript\nconst { audioResult } = await tts.stream({\n  text: \"This is a streaming response\",\n  voice: \"alloy\",\n  speed: 1.2,\n});\n\n// audioResult is a PassThrough stream\n```\n\n## Error Handling and Telemetry\n\nThe TTS system includes built-in telemetry and error tracing, so you can use your favorite tracing tools to get visibility into your TTS usage.\n\n## Usage with Mastra\n\nIntegrate TTS with your Mastra application:\n\n```typescript\nimport { Mastra } from \"@mastra/core\";\nimport { OpenAITTS } from \"@mastra/tts\";\n\nconst tts = new OpenAITTS({\n  model: {\n    name: \"tts-1\",\n  },\n});\n\nconst mastra = new Mastra({\n  tts,\n});\n\n// Generate speech\nconst audio = await mastra.tts.generate({\n  text: \"Welcome to Mastra\",\n  voice: \"nova\",\n});\n```\n\nThe Mastra TTS system provides type-safe speech generation with telemetry and error handling. Start with basic generation and add streaming as needed.\n\n## Next Steps: Exposing TTS to Agents\n\nOne thing we're thinking about is how to expose TTS to agents.\n\nCurrently, our thought is to optionally let agents be configured with a TTS model, and then `agent.tts.generate()` and `agent.tts.stream()` would be available, as well as `/agents/$AGENT_ID/tts/generate` and `/agents/$AGENT_ID/tts/stream` endpoints.\n\nSome other questions:\n\n- How should we expose this functionality in the `mastra dev` UI?\n\nWe figured we would embed a sound clip in the chat UI for agents that have a TTS model configured.\n\n- How should we expose this functionality in agent memory?\n\nWe figured we would probably add a new `tts` field to items in agent memory, and then we could store the TTS model name there."
  },
  {
    "metadata": {
      "title": "Mastra Changelog 2025-01-17",
      "publishedAt": "2025-01-17",
      "summary": "Mastra's changelog for Jan 10th, 2025 through Jan 17th, 2025",
      "author": "Sam Bhagwat",
      "draft": false,
      "categories": ["changelogs"]
    },
    "slug": "changelog-2025-01-17",
    "content": "Welcome to Mastra Changelog #2. Let's dive in.\n\n## New Eval Framework\n\nWe added a new `@mastra/evals` package for systematic agent evaluation, including metrics for answer relevancy, completeness, and tone consistency. We also integrated that with @mastra/core for standardized evaluation hooks.\n\nWe just pushed this today and are still working on a blog post and docs, but you can see the [codebase](https://github.com/mastra-ai/mastra/tree/main/packages/evals) for now and install with `npm install @mastra/evals`.\n\n## Enhanced RAG Capabilities\n\nWe published a [blog post](https://mastra.ai/blog/build-rag-workflow) from Nik Aiyer on Mastra’s RAG capabilities.\n\nWe added a GraphRAG implementation for improved context retrieval, and introduced reranking capabilities with OpenAI and Cohere integration. We also added vector result inclusion options and filtering options across all vector store implementations.\n\n## MCP Support\n\nMastra now integrates with Anthropic’s Model Context Protocol (MCP), allowing automatic discovery and integration of third-party AI tools and services. When you connect to an MCP server, Mastra automatically detects available tools and makes them accessible through its typed interface, maintaining the same development experience as built-in tools.\n\nThis integration opens up access to hundreds of tools from decentralized tool registries like opentools.com and MCP.run. These tools include data analysis, image processing, filesystem access, app integrations, etc,\n\n## Text-to-Speech Integration\n\nText-to-speech capabilities are now available through the new `@mastra/tts` package, supporting both ElevenLabs and OpenAI providers. The integration includes both streaming and generation methods, along with features for voice selection and management.\n\n## NotebookLM & multi-agent orchestration\n\nWe've added support for multi-agent capabilities in Mastra with two new patterns.\n\nA [hierarchical multi-agent system](https://mastra.ai/examples/agents/hierarchical-multi-agent) allows agents to use other agents as tools, enabling sophisticated task delegation. Another workflow-based approach provides more structured control over agent interactions through explicit steps with deterministic execution.\n\nWe've demonstrated both patterns in our new NotebookLM clone example ([blog post](https://mastra.ai/blog/notebooklm-clone-with-agent-orchestration), [live demo](https://notebooklm-mastra.vercel.app/), [codebase](https://github.com/mastra-ai/notebooklm-mastra)), which uses an orchestrator agent to coordinate between specialized agents for content analysis, script generation, and audio production.\n\n## Bug Fixes, Testing Upgrades & Infrastructure\n\n### Testing Infrastructure\n\n- Migrated from Jest to Vitest across all packages\n- Added new test coverage for GraphRAG and reranking\n- Improved test reliability and speed\n- Added streaming tests for TTS functionality\n\n### Vector Store Improvements\n\n- Fixed filter handling in PostgreSQL vector store\n- Improved error handling in Qdrant integration\n- Enhanced vector inclusion logic across all stores\n- Updated type definitions for better TypeScript support\n\n### Memory & Engine Updates\n\n- Improved entity creation logic in PostgreSQL engine\n- Enhanced sync record handling\n- Updated memory thread management\n- Fixed workflow snapshot persistence issues\n\n## Packages Version Updates\n\n- `@mastra/core` (0.1.27-alpha.64)\n- `@mastra/cli` (0.1.57-alpha.95)\n- `@mastra/rag` (0.0.2-alpha.49)\n- `@mastra/engine` (0.0.5-alpha.58)\n- `@mastra/memory` (0.0.2-alpha.44)\n- `@mastra/evals` (0.0.1-alpha.0) [NEW]\n- `@mastra/tts` (0.0.1-alpha.7) [NEW]\n- `@mastra/mcp` (0.0.1-alpha.4)\n- `create-mastra` (0.1.0-alpha.24)"
  },
  {
    "metadata": {
      "title": "Setting up a RAG pipeline with Mastra",
      "publishedAt": "2025-01-16",
      "summary": "A guide to setting up a simple RAG pipeline with Mastra",
      "author": "Nik Aiyer",
      "categories": ["foundations"]
    },
    "slug": "build-rag-workflow",
    "content": "### Understanding RAG: The Theory\n\nLarge Language Models (LLMs) face a fundamental challenge: they can only work with information they were trained on. This creates limitations when dealing with new, private, or domain-specific information. Additionally, LLMs sometimes generate plausible but incorrect information – a problem known as hallucination.\n\nRetrieval-Augmented Generation (RAG) addresses these limitations by connecting LLMs to external knowledge sources. Think of RAG as giving an LLM the ability to \"look up\" information before responding, similar to how a human might consult reference materials to answer a question accurately.\n\n### How RAG Works\n\nThe RAG process mirrors how humans research and answer questions. When a query comes in, the system first searches its knowledge base for relevant information using vector embeddings – numerical representations that capture the meaning of text. This retrieved information is then formatted into a prompt for the LLM, which combines this specific knowledge with its general capabilities to generate an accurate, contextual response.\n\nHere's a visual representation of how RAG works:\n\n![RAG workflow diagram](/rag-image.png)\n\nMastra has built-in support for RAG, and allows developers to easily integrate a RAG workflow into their applications.\n\nNow, let's build a RAG workflow using Mastra, including reranking capabilities for improved accuracy.\n\n### Part 1: Document Ingestion and Chunking\n\nLet’s start by ingesting a document and chunking it. We want to split the document into bite-sized pieces for our search; this is called “chunking.”\n\nThe goal is to split at natural boundaries like topic transitions and new sections. This is called “semantic coherence.”\n\n```tsx\nimport { Mastra } from \"@mastra/core\";\nimport { MDocument, PgVector, Reranker } from \"@mastra/rag\";\nimport { embedMany } from \"ai\";\nimport { openai } from \"@ai/openai\";\n\nconst doc = MDocument.fromText(`Your text content here...`);\n\nconst chunks = await doc.chunk({\n  strategy: \"recursive\",\n  size: 512,\n  overlap: 50,\n  separator: \"\\n\",\n});\n```\n\n### Part 2: Embedding Generation\n\nAfter chunking, we'll need to embed our data – transform it into a vector, or an array of 1536 values between 0 and 1, representing the meaning of the text.\n\nWe do this with LLMs, because they make the embeddings much more accurate. OpenAI has an API for this.\n\n```tsx\nconst { embeddings } = await embedMany({\n  values: chunks,\n  model: openai.embedding(\"text-embedding-3-small\"),\n});\n```\n\n### Part 3: Vector Storage and Management\n\nWe need to use a vector DB which can store these vectors and do the math to search on them. We'll use pgvector, which comes out of the box with Postgres.\n\nOnce we pick a vector DB, we need to set up an index to store our document chunks, represented as vector embeddings.\n\n```tsx\nconst pgVector = new PgVector(process.env.POSTGRES_CONNECTION_STRING!);\n\n// add to the Mastra object to get logging\nexport const mastra = new Mastra({\n  vectors: { pgVector },\n});\n\nawait pgVector.createIndex(\"embeddings\", 1536);\nawait pgVector.upsert(\n  \"embeddings\",\n  embeddings,\n  chunks?.map((chunk: any) => ({ text: chunk.text })),\n);\n```\n\n### Part 4: Query Processing and Response Generation\n\nLet's set up the LLM we'll use for generating responses and define our query.\nWe'll then generate an embedding for the query using the same embedding API we used for the document chunks.\n\n```tsx\nconst query = \"insert query here\";\nconst { embedding } = await embed({\n  value: query,\n  model: openai.embedding(\"text-embedding-3-small\"),\n  maxRetries: 3,\n});\n```\n\nOkay, after that setup, we can now query the database!\n\nUnder the hood, Mastra is running an algorithm that compares our query string to all the chunks in the database and returning the most similar ones.\n\nThe actual algorithm is called “cosine similarity”.\nThe implementation is similar to geo queries searching latitude/longitude, except the search goes over 1536 dimensions instead of two.\nWe can use other algorithms as well.\n\nNow, we can construct a prompt using the results as context.\n\n```tsx\n// Perform vector similarity search\nconst results = await pgVector.query(\"embeddings\", embedding);\n\n// Extract and combine relevant chunks\nconst relevantChunks = results.map((result) => result?.metadata?.text);\nconst relevantContext = relevantChunks.join(\"\\n\\n\");\n\nconst prompt = `\n    Please answer the following question:\n    ${query}\n\n    Please base your answer only on this context ${relevantContext}. \n    If the context doesn't contain enough information to fully answer the question, please state that explicitly.\n`;\n```\n\nWe can now pass this prompt into an LLM to generate a response.\n\n```tsx\nconst { completion } = await generateText({\n  model: openai(\"gpt-4o-mini\"),\n  prompt: prompt,\n});\nconsole.log(completion);\n```\n\n### Part 5: Enhanced Retrieval with Reranking\n\nOptionally, after querying and getting our results, we can use a reranker. Reranking is basically a more computationally expensive way of searching the dataset.\n\nIt would take too long to run it over the entire database, but we can run it over our results to improve the ordering.\n\n```tsx\nconst reranker = new Reranker({\n  semanticProvider: openai(\"gpt-4o-mini\"),\n});\n\nconst rerankedResults = await reranker.rerank({\n  query: query,\n  vectorStoreResults: results,\n  topK: 3,\n});\n// Process reranked results\nconst rerankedChunks = rerankedResults.map(\n  ({ result }) => result?.metadata?.text,\n);\n\n// Combine reranked chunks into a context\nconst rerankedContext = rerankedChunks.join(\"\\n\\n\");\n\nconst rerankedPrompt = `\n    Please answer the following question:\n    ${query}\n\n    Please base your answer only on this context ${rerankedContext}. \n    If the context doesn't contain enough information to fully answer the question, please state that explicitly.\n`;\n```\n\nFinally, we can generate a response using the reranked prompt:\n\n```tsx\nconst rerankedCompletion = await llm.generate(rerankedPrompt);\nconsole.log(rerankedCompletion.text);\n```\n\n## Common Patterns and Best Practices\n\nWhen working with this RAG implementation, keep these principles in mind:\n\nThe chunk size and overlap settings in the document processing significantly impact retrieval quality. Using a recursive strategy with moderate overlap (like 50 tokens) often provides good results while maintaining context.\n\nThe choice of embedding model affects both the quality of retrieval and the cost of operation. While OpenAI's embeddings provide excellent results, there are open-source alternatives available for different needs.\n\nThe number of chunks retrieved (topK in the vector query tool) is a balance between providing enough context and staying within the LLM's context window. Start with 3-4 chunks and adjust based on need.\n\nConsider the tradeoff between response time and quality when deciding whether to use reranking. While it can improve accuracy, it adds an additional processing step to the workflow."
  },
  {
    "metadata": {
      "title": "Building a NotebookLM clone with an agent orchestrator",
      "publishedAt": "2025-01-16",
      "summary": "A technical guide showing how to build a NotebookLM clone that transforms documents into AI-generated podcasts using Mastra's agent orchestration, Pgvector vector storage, and LlamaIndex for content processing.",
      "author": "Sam Bhagwat",
      "categories": ["examples"]
    },
    "slug": "notebooklm-clone-with-agent-orchestration",
    "content": "We've recently been polishing up our multi-agent functionality, and decided to build a NotebookLM clone to show off some of the capabilities.\n\nHere's the [deployed version](https://notebooklm-mastra.vercel.app) and the [code](https://github.com/mastra-ai/notebooklm-mastra).\n\n## The initial deterministic workflow\n\nWe created a deterministic workflow using Mastra's workflow functionality to get the source material into the DB and embedded into pgvector for summarization.\n\nThe first step was to use LlamaIndex Cloud to parse the uploaded source material and turn it into a markdown file. Then, we used Mastra's `@mastra/rag` package to chunk the markdown file, and embed the chunks into our Postgres DB using `PgVectorStore`.\n\nNext, we needed to turn that into an audio podcast, with source material included.\n\n## The decision to use an agent orchestrator\n\nThere were two fundamental agent architecture approaches we could have taken.\n\nBecause we were essentially building an audio processing pipeline, we could have made another structured workflow with a defined set of steps.\n\nHowever, we found that giving a single \"orchestrator\" agent a number of tools, each of which called a separate agent, and which could be called in any order, worked well enough...assuming we wrote an INCREDIBLY DETAILED prompt.\n\n## The orchestrator agent and its tools\n\nBut we'll get to that in a bit. Here's the code for the orchestrator agent:\n\n```typescript copy\nimport { anthropic } from \"@ai-sdk/anthropic\";\n\nexport const orchestrator = new Agent({\n  name: \"orchestrator\",\n  instructions: orchestratorInstructions,\n  model: anthropic(\"claude-3-5-sonnet-20241022\"),\n  tools: {\n    validateSourcesAvailability,\n    querySourceSummaryAndChunks,\n    savePodcastDetails,\n    generatePodcastOutline,\n    generatePodcastScript,\n    submitForAudioProduction,\n  },\n});\n```\n\nLet's dive into some of the tools listed here. Quick summaries:\n\n- `validateSourcesAvailability` does a \"normal\" database query to check if the sources are available.\n- `querySourceSummaryAndChunks` uses the `embed` function from `@mastra/rag` to generate embeddings for a query, and then uses pgvector to find the closest embeddings in the database.\n- `generatePodcastOutline` uses a specialized agent that is good at extracting insights and turning them into an outline.\n- `generatePodcastScript` uses a specialized agent that can take that outline and turn it into a script.\n- `submitForAudioProduction` calls a specialized API, play.ai, that can turn the two-person script into a podcast. (We tried other audio APIs, like ElevenLabs, but found that Play.ai did the best job at making the interchange between the two hosts sound natural.)\n\nYou'll notice that many of these tools are specialized agents, and one is doing RAG. The hard work, though, was describing them in a way that the LLM could understand.\n\n## The orchestrator agent's instructions\n\nNow let's take a look at the instructions we gave to the orchestrator agent. The prompt is...a bit of a beast:\n\n```typescript copy\nexport const orchestratorInstructions = `\nYou are an orchestrator tasked with coordinating the generation of a podcast from written sources.\nYour role is to manage the entire process, from content research to final audio production, ensuring quality and coherence throughout.\n\nPhases\n1. Validate all required sources are available\n2. Query source summaries and chunks to get the available content\n3. Identify key insights and themes\n4. Generate an outline for the podcast targetting 15 to 30 minutes\n5. Use the outline to generate a script\n6. Review the script\n\nScript requirements\n- Maintain consistent voice and tone\n- The script should only contain the spoken words of the hosts\n- The script should NOT include non-verbal cues, directions, instructions, etc\n- The script should be formatted in the following way\n  - Prefix each speaker's turn with either 'Host 1:' or 'Host 2:'\n  - Example format:\n      Host 1: Hello there. Today we're talking about something very interesting.\n      Host 2: Very interesting doesn't even begin to describe how interesting this is, I'm particularly fascinated...\n\nTools\nYou have access to the following tools to help you with your task\n- 'validateSourcesAvailability': this tool helps you validate if the sources are available. it accepts a notebookId, which will retrieve all relevant sources and it will return an object with the following shape\n- 'querySourceSummaryAndChunks': This tool takes a query string, notebookId, similarity threshold, and limit as input, and returns an array of sources (containing sourceId, sourceTitle, sourceSummary, and sourceChunks) by comparing vector embeddings in a PostgreSQL database.\n- 'submitForAudioProduction': This tool accepts a podcast transcript and voice configuration options as input, submits it to the PlayDialog API for processing with alternating voices, and returns the URL the user will use to poll for completion of the audio production job.\n- 'savePodcastDetails': This tool is used to save podcast details like the audio_url and podcast_script for the notebook. Use it to always save the details you get. You don't have to pass all the details at the same time. Ensure you save the script before you submit for podcast generation.\n- 'generatePodcastOutline': This tool is used to generate a show outline for the podcast. This outline will be used to plan the scripting process. You need to pass instructions and a list of key insights and it will give you back the outline\n- 'generatePodcastScript': This tool is used to generate script for the podcast. Look at the result of this tool and make sure it follows the prescribed format and is long enough for the target time, you can use it again to regenerate the script until it meets the requirements.\n\nDO NOT STOP after the outline has been generated. Make sure to go all the way until you submit the script for audio production.\n`;\n```\n\nSo why did we write such a long prompt?\n\nThe length and detail proved to be crucial for reliable orchestration:\n\n- Without explicit phases, the agent tried to skip steps or execute them out of order\n- Without detailed script requirements, it sometimes generated unusable formats with stage directions or inconsistent speaker labels that would break the audio synthesis pipeline.\n- Without exhaustive tool definitions, the agent hallucinated capabilities (like trying to directly generate audio) or misused available tools (like submitting unformatted text to the audio production API).\n\nIt's an example of \"explicit is better than implicit\" in system design - while the prompt could be shorter, the additional verbosity dramatically improves reliability and reduces edge cases.\n\n## Our lessons\n\n1. **Explicit Instructions Beat Implicit Inference**: Even with a state-of-the-art model like Claude Sonnet, additional verbosity improved reliability and reduced edge cases.\n\n2. **Tool Definition is Critical**: The success of an orchestrator agent depended heavily on clear tool definitions.\n\n3. **Architectures are flexible if you're willing to put in the work**: While we could have used a deterministic workflow with fixed steps, the agent orchestrator approach worked well - but only when combined with very detailed instructions.\n\nAnd, finally, one meta-lesson: building even relatively simple AI applications, at high levels of quality,requires multiple AI primitives (agents, workflows, tools, and agents)."
  },
  {
    "metadata": {
      "title": "Building workflows with Mastra",
      "publishedAt": "2025-01-13",
      "summary": "A few patterns for building workflows",
      "author": "Abhi Aiyer",
      "categories": ["foundations"]
    },
    "slug": "building-workflows",
    "content": "At Mastra, we love agents. What we’ve found is that agents are even more powerful when you pair them with workflows when you need more structured decision-making.\n\nWe’re hardly the first to discover this — Anthropic had a [great blog post](https://www.anthropic.com/research/building-effective-agents) on this last month.\n\nEnter workflows: a way to explicitly structure and determine how agents and LLMs work together.\n\nWorkflows let you not only control the general flow of task-execution but also add _checkpoints_, moments when computation is _suspended_ (so a human can provide feedback or guidance to the agent) before the workflow is _resumed_ and ultimately completed.\n\n## Mastra workflow patterns\n\nWe’ve built several workflow patterns in Mastra that you can add to projects and customize.\n\nThink of these as tools for the implementing most common types of workflows:\n\n### Sequential chains\n\nSequential chains are useful for organizing steps that need to be executed one after another in a specific order. But they’re more than just ordered execution.\n\nThese chains can ensure that each step's output becomes the input for the next step—which is particularly helpful when you want to control data flows and/or manage dependencies.\n\n![Sequential chains workflow diagram](/sequential-chains.png)\n\nSequential chains, where steps follow a linear order, are the “hello world” of workflows.\n\nThey're valuable for:\n\n- **Simple document processing pipelines:** Extracting, validating, and storing text or data in a structured sequence.\n- **Multi-stage approval processes:** Ensuring documents or requests pass through each level of review in order.\n- **Data transformation and validation sequences:** Cleaning, transforming, and verifying data step by step.\n\nWe've built a [simple code example](https://mastra.ai/examples/workflows/sequential-steps) for building sequential chains in Mastra.\n\nThe relevant bit:\n\n```typescript copy\nmyWorkflow.step(stepOne).then(stepTwo).then(stepThree);\n```\n\nIn the example, `stepTwo` only executes after `stepOne` succeeds, and `stepThree` only executes after `stepTwo` succeeds.\n\nIf you want to dive deeper, [here are the docs](https://mastra.ai/docs/workflows/00-overview#sequential-execution).\n\n### Parallel chains\n\nParallel chains are ideal for tasks that can be executed simultaneously (rather than one after another.) These workflows enable multiple steps to run independently, reducing overall processing time. Unlike sequential chains, parallel chains don't rely on the output of one step to feed into the next. Instead, tasks run concurrently and can converge later if needed.\n\n![Parallel chains workflow diagram](/parallel-chains.png)\n\nFrom the user’s perspective, parallel execution reduces latency; from a system perspective, it improves resource utilization. Key applications include:\n\n- **Batch processing**: Handling multiple documents, datasets, or files at once.\n- **Concurrent data analysis**: Running independent computations on the same or different datasets simultaneously.\n- **Multitask workflows**: Managing distinct but related tasks, like generating reports while sending notifications.\n\nWe've built a [simple code example](https://mastra.ai/examples/workflows/parallel-steps) for building parallel chains in Mastra.\n\nThe relevant bit:\n\n```typescript copy\nmyWorkflow.step(stepOne).then(stepTwo).step(stepThree).then(stepFour);\n```\n\nThis creates a workflow where `stepOne` and `stepThree` kick off their sequential chains concurrently. Each chain also updates the global context with its results. The overall workflow engine manages dependencies and ensures that all parallel steps are completed.\n\nYou can read more about parallel workflows [here](https://mastra.ai/docs/workflows/00-overview#parallel-execution).\n\n### Subscribed chains\n\nSubscribed chains are much like parallel chains—except they’re only triggered by the completion of a specific step. In other words, they’re event-driven.\n\n![Subscribed chains workflow diagram](/subscribed-chains.png)\n\nSubscribed chains are powerful for managing workflows that require simultaneous but event-driven actions like:\n\n- **Complex notifications**: Sending different alerts to multiple channels using specific triggers.\n- **Audit trail generation**: Logging actions and changes in parallel to the main workflow.\n- **Asynchronous processing**: Handling background tasks like data enrichment without blocking the main process.\n- **Secondary validation**: Running additional checks alongside the primary workflow.\n\nWe've built a [simple code example](https://mastra.ai/examples/workflows/subscribed-steps) for building subscribed chains in Mastra.\n\nThe relevant bit:\n\n```typescript copy\nmyWorkflow\n  .step(stepOne)\n  .then(stepTwo)\n  .after(stepOne)\n  .step(stepThree)\n  .then(stepFour)\n  .commit();\n```\n\nRead more about subscriber chains [here](https://mastra.ai/docs/workflows/00-overview#branching-and-merging-paths)\n\n## Suspending and resuming workflows\n\nFor workflows that require human-in-the-loop, have external event dependencies, or involve operations that are too long to be kept in memory, Mastra has tools for pausing execution, persisting state, and resuming execution.\n\nHere are some examples of human-in-the-loop workflows that benefit from pausing and resuming execution:\n\n- **Approval workflows**: A workflow generates a document that require approval before moving to the next step. The process suspends until the manager approves or rejects the document.\n- **Customer service escalation**: An AI chatbot handles initial customer inquiries but escalates complex issues to a human agent. The human specifies a solution and delegates back to the AI agent to complete follow-up actions.\n\nUse `suspend()` to ‘pause’ any Mastra workflow. When called, `suspend()` triggers a cascade of state-management operations:\n\n![Workflow suspend diagram](/suspend.png)\n\nHere’s an example of how to use `suspend()` in Mastra:\n\n```tsx\nexecute: async ({ suspend, context }) => {\n  const needsApproval = await checkApprovalRequired();\n  if (needsApproval) {\n    await suspend(); // Workflow pauses here\n    return { status: \"awaiting_approval\" };\n  }\n};\n```\n\nOnce a Mastra workflow is suspended, it can be resumed using `resume()`. When called, `resume()` will continue executing the workflow from where it last left off.\n\n![Workflow resume diagram](/resume.png)\n\nThe `resume()` function can be called from within your application after you handle any stipulated human-in-the-loop interactions. You can pass that external context back into the workflow, trusting that it will pick up where it left off.\n\nHere’s an example of using `resume()` with Mastra:\n\n```jsx\nconst workflow = mastra.getWorkflow(\"myWorkflow\");\n\nconst { runId, start } = workflow.createRun();\n\nawait start();\n\nawait workflow.watch(runId, {\n  onTransition: async ({ activePaths, context }) => {\n    for (const path of activePaths) {\n      const ctx = context.stepResults?.[path.stepId]?.status;\n      if (ctx === \"suspended\") {\n        // Handle suspension logic here.\n        if (confirmed) {\n          await workflow.resume({\n            stepId: path.stepId,\n            runId,\n            context: {\n              confirm: true,\n            },\n          });\n        }\n      }\n    }\n  },\n});\n```\n\n## What’s next: orchestrator-workers\n\nIn the near future, we plan to introduce hierarchal workflow management through _orchestrator-workers_. Here’s a high-level sneak peek at what that looks like:\n\n![Orchestrator workflows diagram](/orchestrator-workflows.png)\n\nOrchestrator-workers will let you compose and manage multiple workflows.\n\nStay tuned!"
  },
  {
    "metadata": {
      "title": "Mastra Changelog 2025-01-10",
      "publishedAt": "2025-01-10",
      "summary": "Mastra's changelog for Dec 27th 2024 through Jan 10th, 2025",
      "author": "Sam Bhagwat",
      "draft": false,
      "categories": ["changelogs"]
    },
    "slug": "changelog-2025-01-10",
    "content": "Welcome to the Mastra Changelog.\n\nWe’ll be publishing these every week, but since this is our first one, we’re covering ground from the last two weeks.\n\nLet's dive in.\n\n## Suspend/resume in workflows\n\nWe’ve made it easier to build checkpoints, human-in-the-loop intervention, and/or monitoring into any of your workflows. You can simply call `suspend` inside any step (or between steps) in a workflow and `resume` it later.\n\nState is stored in the Postgres DB in the mastra engine.\n\nThe mental model here is to think about `suspend` as an exception you're throwing, where the workflow will handle catching that exception and persisting state.\n\n## `mastra create` for new projects\n\nYou can now build projects in a standalone way using `npm create mastra`.\n\nThis will create a new project scaffold with directories and (if you want) examples. We've found this to be a cleaner DX for new users.\n\nWhile you can still initialize within an existing project (`mastra init`), `create` generates a ready-to-use project scaffold—complete with recommended directories, configuration, and boilerplate.\n\n## `mastra dev` for an interactive local dev server\n\n<div\n  style={{\n    position: \"relative\",\n    overflow: \"hidden\",\n    width: \"100%\",\n    paddingTop: \"56.25%\",\n  }}\n>\n  <iframe\n    style={{\n      position: \"absolute\",\n      top: 0,\n      left: 0,\n      bottom: 0,\n      right: 0,\n      width: \"100%\",\n      height: \"100%\",\n      borderRadius: \"8px\",\n    }}\n    width=\"560\"\n    height=\"315\"\n    src=\"https://www.youtube.com/embed/spGlcTEjuXY?rel=0\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n    allowfullscreen\n  ></iframe>\n</div>\n\nHere's Shane Thomas demoing Mastra Dev, a local development server for Mastra projects that helps you see and interact with all the agents, workflows, and tools within your Mastra application.\n\nIt includes:\n\n- an agents playground so you can chat with your agents\n- `/text` and `/generate` endpoints for each of your agents so you can test them via `curl` or call them over the network\n- visual workflow diagrams for each workflow\n- endpoints for each of your workflows so you can test them via `curl` or call them over the network\n- a registry of all of your tools\n\n`mastra dev` makes it easier to build, understand, and debug your agents and workflows.\n\n## Mastra Examples: basic building blocks for anyone to get started\n\n![Mastra Examples](/2025-01-10-examples.gif)\n\nTo make it easy for anyone to start building projects with Mastra, we released [a collection of short examples](https://mastra.ai/examples) of all the basic AI engineering Mastra primitives: things like generating and streaming text, describing images, calling a workflow, chunking text, and even implementing basic RAG (retrieval-automated generation) systems.\n\nEach example links to a subdirectory of a Github repo, so you can `npm install` and run them locally.\n\n## API endpoint improvements\n\nPreviously, Mastra offered a single \"generate\" API for text responses with two flags for structured output and streaming.\n\nWe realized having four combinations of flags was confusing, so we've split `generate` into two separate APIs: `generate` and `stream`, each with a flag for structured output."
  },
  {
    "metadata": {
      "title": "A framework for the next million AI developers",
      "publishedAt": "2024-11-20",
      "summary": "The next generation of AI products will be built with Apis written in Typescript",
      "author": "Sam Bhagwat",
      "categories": ["announcements"]
    },
    "slug": "the-next-million-ai-developers",
    "content": "Today we're excited to announce [Mastra](https://mastra.ai/), a Typescript AI framework that Abhi Aiyer, Shane Thomas, and I are building.\n\n**Brief backstory:** last year, after spending the better part of a decade building Gatsby, we joined Netlify. And earlier this year, we began building an AI-powered CRM called Kepler.\n\nBuilding Kepler, we tried a few different frameworks for the AI bits. Each time, we ended up ripping out most of the code as we tried to make our pipelines more reliable. So we switched gears.\n\nWe wanted to create an AI framework that's fun to build toy projects with, but sturdy enough to take into production.\n\nWe wanted to build for product developers: frontend, fullstack, backend, the kinds of folks that had fun toying on their Gatsby blogs years and years ago. The kind of folks who will be (we think) the next million AI engineers.\n\nWe're launching today, and we're excited to share both [our project](https://github.com/mastra-ai/mastra) and our principles:\n\n**Typescript-first, Typescript-only.** Typescript is the language of product development. And these days \"APIs are all you need\" for AI development.\n\nThe Python tie-in of current AI frameworks is a legacy holdover. Sure, Pythonic frameworks have JS/TS ports, but they're an afterthought with a fraction of the features and docs/examples.\n\nA great framework should be mono-lingual to work with language-specific package managers, testing tools, and ORMs. And Typescript developers want a great tool that's built for them.\n\n**Workflow graph based.** LLM applications are non-deterministic pipelines. One emergent practice is defining discrete steps, logging inputs and outputs at each step of each run, and piping them into an observability tool.\n\nMastra includes a workflow graph that encapsulates loops, branching, waiting for human input, embedding other workflows, error handling, retries, and so on.\n\n**Domain-complete.** Most developers prefer to use a framework that has opinions about all the key jobs-to-be-done in a domain. Kubernetes for running infra, Terraform for provisioning it, Next.js for websites.\n\nMastra has [carefully curated](https://mastra.ai/blog/ai-engineering) top-level nouns for all the key concerns in AI engineering: LLMs, agents, workflows, RAG, observability, evals, integrations & syncs. But we don't want to reinvent the wheel, and in many cases choose to build on top of great libraries (AI SDK, autoevals).\n\n**Object-sparse, example-rich.** Many frameworks hand users flat lists of dozens of classes. But more API surface area is a liability, not an asset. Objects should be scoped to the correct top-level concern. Integrations should be scoped to an appropriate provider model. A rich library of code examples should cover use-cases.\n\nThis one comes from painful experience: \"too many, poorly organized APIs\" was one of Gatsby's main weaknesses -- and a mistake we see AI frameworks making.\n\n**Domain-specific grammar.** AI engineering is building production-ready ETL pipelines and should be described with dataflow nouns and verbs.\n\nWe prefer concrete nouns (\"workflow\", \"step\", \"connection\") to abstract nouns (\"graph\", \"node\", \"edge\") or adjectives (\"runnable\"). We prefer verbs (\"extract\") to agent nouns (\"TextExtractor\"). We prefer them because it's the proper way to write TypeScript. Frameworks are DSLs and need domain- and language-specific grammar.\n\n**Explicit, no indirection.** Many framework APIs help people get started by [hiding prompt engineering and retry logic in their APIs](https://hamel.dev/blog/posts/prompt/). These indirections speed up the \"aha!\" moment but are notoriously difficult to debug.\n\nIt's why many engineers (us included, when we were building Kepler) used frameworks for the first 90% of their project...then for the next 90% ripped and replaced them with vanilla model API calls.\n\n(And anyway, Claude on Cursor is a good enough prompt engineer to get started.)\n\n**Deployable to a serverless or public cloud.** It's 2024. You should be able to easily deploy agents and workflows to a serverless cloud and access them as API endpoints.\n\nWe're building examples and guides for you to deploy wherever you want. (We're also building a simple cloud-based agent/workflow runtime with a generous free tier.)\n\n### Let's build together\n\nWe're excited to see what you'll build. Please [npm/pnpm/yarn install mastra](https://mastra.ai/docs/) and tell us everything you love or hate.\n\nSee you on Github :)"
  },
  {
    "metadata": {
      "title": "AI engineering, two years later",
      "publishedAt": "2024-11-11",
      "summary": "What's changed in AI engineering over the last two years: models, agents, knowledge, tracing, evals",
      "author": "Sam Bhagwat",
      "draft": false,
      "categories": ["foundations", "case studies"]
    },
    "slug": "ai-engineering",
    "content": "It&apos;s been two years since ChatGPT went viral.\n\nBut despite being avid users of 4o, Claude, Copilot, Cursor, etc, when we started building AI features a few months ago, it took a while to get up to speed. Building AI applications today feels a bit like web development in 2012 \\- the primitives are powerful, but frameworks and best practices are still emerging.\n\n## Local Environment, Models & Prompts\n\nIf you&apos;re doing AI development, you should use an editor with AI chat built in. Even if you&apos;re married to your vim keybindings or you hate code complete, AI models write pretty good prompts, and they know the provider SDKs well.\n\nWant to get to “hello world” in five minutes? Download Cursor, back it with claude-3.5-sonnet-new, and ask the chat to write a script that uses OpenAI&apos;s Node.js SDK to classify support tickets (or whatever your use-case is). If you need sample data, ask the AI to generate some.\n\nClaude&apos;s models have been better than OpenAI&apos;s for the last few months, so you probably want an Anthropic API key. If cost is an issue, Google&apos;s Gemini model has a generous free tier. Also, if you bring your own API key, Cursor is free.\n\n## Sequences & Workflows\n\nThe first generation of AI features were single-prompt transformations (a transcript summary or an image classifier) and then simple agents (a chatbot with some functions to call).\n\nBut LLMs generate short responses and don&apos;t perform well on long context windows. So people started decomposing long/complex prompts into multiple prompts, then joining the answers together. They started using one model to judge the results of other models. They added error handling and retries to boost reliability.\n\nAs these sort of improvements stacked, AI applications started looking more like graph-based data workflows, and researchers [began calling this](https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/) “compound AI”. There are a few graph-based frameworks around (and we&apos;re building one\\!) so if you think your application will end up looking like this it&apos;s worth giving them a try.\n\n{/* **TKTK three-panel diagram** */}\n\n## Agents\n\nThe best definition of an AI agent is a system where the language model chooses a sequence of actions. Six to twelve months ago, agents weren&apos;t very good, but there have been fairly rapid advances in this space.\n\nAI agents, like self-driving cars, have different levels of autonomy. At low levels, agents are “decider” nodes in a pre-defined workflow graph. At higher levels, agents own the control flow – they can break down tasks into subtasks, execute them, and check their work.\n\nA basic agent – something you could write in a day, like an agent to get stock prices – might have have a single prompt, do some query parsing, make a function call hitting an external service, and lightly track internal state and conversation history (or offload that to a model, like OpenAI Assistants).\n\nThe best open-source agents today ([OpenHands](https://github.com/All-Hands-AI/OpenHands/), [Composio SWEKit](https://github.com/ComposioHQ/composio/tree/master/python/swe)) have been trained on tasks like code generation with public benchmarks. They delegate to other agents in a project-specific team structure – say, a code analyzer, a code writer, and a code reviewer who coordinates the other two.\n\nThey break down tasks into subtasks, browse the web, run sandboxed code, retry with modification on errors, and prompt the user for clarification. They keep a memory of event history, current task context, and the runtime environment and include it in LLM context windows. They store task status in a state machine. A controller runs a while loop until a task reaches a finished state.\n\n{/* **TKTK three-panel diagram** */}\n\n## Knowledge & RAG\n\nBuilding agents and workflows usually requires both general knowledge (from base models) and domain- or user-specific knowledge, from specific documents, web scrapers, or data from internal SaaS.\n\nYou get this from retrieval-augmented generation (RAG), basically an ETL pipeline with specific querying techniques. The ETL part is “chunking” documents and other content into smaller pieces, “embedding” each chunk (transforming it into a vector), and loading it into a vector DB.\n\nThen, the querying part, known as “retrieval.” You have to embed your query text into a vector, search the DB for similar vectors, and feed the results into an LLM call. (You can also take your top results and use a more computationally-intense search method to “rerank” them.)\n\n![Knowledge and RAG diagram](/knowledge-and-rag.png)\n\n\nOnce you build an initial version there are a dozen ways to tweak and optimize: overlap between chunks, query smaller chunks to get more precise answers, feed surrounding context to the LLM synthesizer, use domain-specific embedding models, combine multiple query algorithms, add filtering based on metadata, and so on.\n\n## Observability: Tracing & Evals\n\nThe three magic properties of AI applications are accuracy, latency, and cost. So once you get a basic application working, the next step is to add tracing and hook it up to an observability service, write tests, and hook it up to evals.\n\n### Tracing\n\nTracing is the gold standard for LLM observability. It gives you function-level insight into execution times, and inputs and outputs at each stage. Most frameworks come with tracing out of the box (we do), so this avoids you having to hand-instrument your code. There are a bunch of different providers, and some take tracing in non-standard formats. Ideally, prefer a provider that takes straight-OpenTelemetry logs.\n\nThe UIs for these will feel fairly familiar if you&apos;ve used a provider like Datadog or Honeycomb; they help you zoom into surprisingly long requests or unexpected LLM responses. Look for unexpected responses whenever you&apos;re calling an LLM.\n\n![Tracing diagram](/tracing.png)\n\n### Evals\n\nRepeat three times after me. Evals are just tests. Evals are just tests. Evals are just tests.\n\nOkay, evals _are_ tests of non-deterministic systems, so you need to write more. Evals can return fractional values, not just binary pass/fail. Unlike the rest of your CI suite, your evals may not all pass all the time.\n\nYou may write five evals for a single (input, output, expected) triple of a single LLM call, checking it for accuracy, relevance, factual consistency, length adherence. You might write evals checking semantic distance, search results to look for a particular string, or have a different LLM evaluate your first LLM&apos;s response. Short, well-tested workflows can amass hundreds of evals.\n\nAfter you write your evals, you&apos;ll probably go back and change your prompts, or your pipeline, or some part about your application, and then you&apos;ll be able to see the impact of those changes on the data set.\n\nFor your own and your team&apos;s sanity, you may want to use the same service for tracing and evals (Braintrust is good here).\n\n## Putting it all together\n\nThe reality of building AI projects in most companies is that it usually takes a day or two to get yourself to “wow”, a week or two to build a demo to show everyone else value, and a month or two (or more) getting to something you can ship.\n\nHappy building, and do check out Mastra if you&apos;re building in TypeScript."
  }
]
